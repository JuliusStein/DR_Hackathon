Title,Description,Solution,Files Changed,,Bug,Fix
Fix project deletion for acceptance tests,"There is a problem with acceptance deletion after test run completion.
Script tried to delete project before deployment removal, so that we have an error for this.","Added try-finally for acceptance tests resource deletion
Change fixture initialization for prediction acceptance tests to have proper resource utilization order
Fixed logging for tests
Bump timeout for nginx max file size test","diff --git a/tests/api_acceptance_tests/conftest.py b/tests/api_acceptance_tests/conftest.py
index 80ee96c1c32abd..774ad12628ba9c 100644
--- a/tests/api_acceptance_tests/conftest.py
+++ b/tests/api_acceptance_tests/conftest.py
@@ -482,7 +482,7 @@ def admin_user(create_user):
 
 @pytest.fixture(scope='session')
 def v2_client(user, create_datarobot_client):
-    print('creating v2_client')
+    print('Create v2_client')
     print(user)
     return create_datarobot_client(user_info=user)
 
@@ -513,7 +513,7 @@ def _create_project(project_type=""binary_csv""):
         dataset_path = os.path.join(fixture_data_path, pr_conf[""dataset_name""])
         project_target = pr_conf[""target""]
 
-        print(""Creating project"")
+        print(""Create project"")
         project = dr.Project.create(dataset_path, project_name=project_name)
         project.analyze_and_model(
             target=project_target,
@@ -551,15 +551,16 @@ def _create_prediction_environment():
         prediction_environment_list.append(prediction_environment)
         return prediction_environment
 
-    yield _create_prediction_environment
-
-    for environment in prediction_environment_list:
-        try:
-            print(f""Deleting environment {environment.id}"")
-            environment.delete()
-        # pylint: disable=broad-except
-        except Exception as e:
-            print(f""WARNING: failed to delete environment {environment.id}. \n{e}"")
+    try:
+        yield _create_prediction_environment
+    finally:
+        for environment in prediction_environment_list:
+            try:
+                print(f""Delete environment {environment.id}"")
+                environment.delete()
+            # pylint: disable=broad-except
+            except Exception as e:
+                print(f""WARNING: Failed to delete environment {environment.id}. \n{e}"")
 
 
 @pytest.fixture(scope='session')
@@ -584,7 +585,7 @@ def create_deployment(v2_client, create_prediction_environment, is_enterprise):
 
     def _create_deployment(model, nextgen_predictions=False):
         # pylint: disable=logging-dynamic-error-message
-        print(f""Creating deployment for {model.model_type}"" f""(Nextgen={nextgen_predictions})"")
+        print(f""Create deployment for {model.model_type}"" f""(Nextgen={nextgen_predictions})"")
         if nextgen_predictions:
             try:
                 prediction_environment_id = [
@@ -614,7 +615,7 @@ def _create_deployment(model, nextgen_predictions=False):
             deployment_id = response.json()[""id""]
             deployment = dr.Deployment.get(deployment_id)
 
-            print(f""Activate realtime predictions for deployment {deployment_id}"")
+            print(f""Activate realtime predictions for deployment"")
             payload = {
                 ""predictionsSettings"": {
                     ""realTime"": True,
@@ -653,25 +654,26 @@ def _create_deployment(model, nextgen_predictions=False):
         deployment_list.append(deployment)
         return deployment
 
-    yield _create_deployment
-
-    for deployment in deployment_list:
-        try:
-            print(f""Deleting deployment {deployment.id}"")
-            deployment.delete()
-        # pylint: disable=broad-except
-        except Exception as e:
-            # pylint: disable=logging-dynamic-error-message
-            print(f""WARNING: Failed to delete deployment {deployment.id}. \n{e}"")
-
-    if mlpkg_list:
-        for mlpkg in mlpkg_list:
+    try:
+        yield _create_deployment
+    finally:
+        for deployment in deployment_list:
             try:
-                print(f""Archive mlpkg {mlpkg}"")
-                v2_client.post(f""modelPackages/{mlpkg}/archive/"")
+                print(f""Delete deployment {deployment.id}"")
+                deployment.delete()
             # pylint: disable=broad-except
             except Exception as e:
-                print(f""WARNING: Failed to archive mlpkg {mlpkg}. \n{e}"")
+                # pylint: disable=logging-dynamic-error-message
+                print(f""WARNING: Failed to delete deployment {deployment.id}. \n{e}"")
+
+        if mlpkg_list:
+            for mlpkg in mlpkg_list:
+                try:
+                    print(f""Archive mlpkg {mlpkg}"")
+                    v2_client.post(f""modelPackages/{mlpkg}/archive/"")
+                # pylint: disable=broad-except
+                except Exception as e:
+                    print(f""WARNING: Failed to archive mlpkg {mlpkg}. \n{e}"")
 
 
 @pytest.fixture(scope='session')
@@ -735,7 +737,7 @@ def _create_deployment_from_mlpkg(project_name, nextgen_predictions=False):
         wait_for_async_resolution(v2_client, response.headers[""Location""])
         deployment_id = response.json()[""id""]
         if nextgen_predictions:
-            print(f""Activate realtime predictions for deployment {deployment_id}"")
+            print(f""Activate realtime predictions for deployment"")
             payload = {
                 ""predictionsSettings"": {
                     ""realTime"": True,
@@ -762,20 +764,21 @@ def _create_deployment_from_mlpkg(project_name, nextgen_predictions=False):
         deployment_list.append(deployment)
         return deployment
 
-    yield _create_deployment_from_mlpkg
-
-    for deployment in deployment_list:
-        try:
-            print(f""Deleting deployment {deployment.id}"")
-            deployment.delete()
-        # pylint: disable=broad-except
-        except Exception as e:
-            print(f""WARNING: Failed to delete deployment {deployment.id}. \n{e}"")
+    try:
+        yield _create_deployment_from_mlpkg
+    finally:
+        for deployment in deployment_list:
+            try:
+                print(f""Delete deployment {deployment.id}"")
+                deployment.delete()
+            # pylint: disable=broad-except
+            except Exception as e:
+                print(f""WARNING: Failed to delete deployment {deployment.id}. \n{e}"")
 
-    for mlpkg in mlpkg_list:
-        try:
-            print(f""Archive mlpkg {mlpkg}"")
-            v2_client.post(f""modelPackages/{mlpkg}/archive/"")
-        # pylint: disable=broad-except
-        except Exception as e:
-            print(f""WARNING: Failed to archive mlpkg {mlpkg}. \n{e}"")
+        for mlpkg in mlpkg_list:
+            try:
+                print(f""Archive mlpkg {mlpkg}"")
+                v2_client.post(f""modelPackages/{mlpkg}/archive/"")
+            # pylint: disable=broad-except
+            except Exception as e:
+                print(f""WARNING: Failed to archive mlpkg {mlpkg}. \n{e}"")
diff --git a/tests/api_acceptance_tests/predictions/conftest.py b/tests/api_acceptance_tests/predictions/conftest.py
index cb0e62c5b8bff2..3616b7651c50e7 100644
--- a/tests/api_acceptance_tests/predictions/conftest.py
+++ b/tests/api_acceptance_tests/predictions/conftest.py
@@ -45,11 +45,11 @@ def _train_model(project, explanations=False):
 
 @pytest.fixture(scope='session')
 def prediction_test_data(
+    create_project,
+    train_model,
     create_deployment,
     create_deployment_from_mlpkg,
     prediction_dataset_path,
-    create_project,
-    train_model,
     is_enterprise,
 ):
     if is_enterprise:
@@ -60,16 +60,19 @@ def prediction_test_data(
         project = create_project(project_type=""japanese"")
         model = train_model(project=project, explanations=True)
         deployment = create_deployment(model=model)
+        print(f""Project id: {project.id}"")
+        print(f""Model id: {model.id}"")
+    print(f""Deployment id: {deployment.id}"")
     pred_file_path = prediction_dataset_path(project_name=""japanese"")
     return deployment, pred_file_path
 
 
 @pytest.fixture(scope='session')
 def prediction_test_nextgen_data(
-    create_deployment,
-    create_deployment_from_mlpkg,
     create_project,
     train_model,
+    create_deployment,
+    create_deployment_from_mlpkg,
     prediction_dataset_path,
     is_enterprise,
     is_local_dev_env,
@@ -84,6 +87,9 @@ def prediction_test_nextgen_data(
             project = create_project(project_type=""japanese"")
             model = train_model(project=project)
             deployment = create_deployment(model=model, nextgen_predictions=True)
+            print(f""Project id: {project.id}"")
+            print(f""Model id: {model.id}"")
+        print(f""Deployment id: {deployment.id}"")
         pred_file_path = prediction_dataset_path(project_name=""japanese"")
         return deployment, pred_file_path
 
@@ -121,7 +127,7 @@ def realtime_prediction_environment(v2_client):
     pred_environment_list = []
 
     def _realtime_prediction_environment():
-        print(""Create realtime prediction environment"")
+        print(""Create serverless prediction environment"")
         prediction_environment = dr.PredictionEnvironment.create(
             name=""acceptance_test_pred_env"",
             platform=""datarobotServerless"",
@@ -130,12 +136,13 @@ def _realtime_prediction_environment():
         pred_environment_list.append(prediction_environment)
         return prediction_environment
 
-    yield _realtime_prediction_environment
-
-    for env in pred_environment_list:
-        try:
-            print(f""Deleting prediction environment {env.id}"")
-            env.delete()
-        # pylint: disable=broad-except
-        except Exception as e:
-            print(f""WARNING: Failed to delete prediction environment {env.id}. \n{e}"")
+    try:
+        yield _realtime_prediction_environment
+    finally:
+        for env in pred_environment_list:
+            try:
+                print(f""Delete prediction environment {env.id}"")
+                env.delete()
+            # pylint: disable=broad-except
+            except Exception as e:
+                print(f""WARNING: Failed to delete prediction environment {env.id}. \n{e}"")
diff --git a/tests/api_acceptance_tests/predictions/test_nginx_configuration.py b/tests/api_acceptance_tests/predictions/test_nginx_configuration.py
index 68da20f584fa47..d2879556ebbad9 100644
--- a/tests/api_acceptance_tests/predictions/test_nginx_configuration.py
+++ b/tests/api_acceptance_tests/predictions/test_nginx_configuration.py
@@ -213,7 +213,7 @@ def test_prediction_dataset_size_limit_50mb(
             url,
             files={'file': (""generated_dataset.csv"", f)},
             allow_redirects=False,
-            timeout=120,
+            timeout=200,
         )
         assert expected_response_code == response.status_code, (response.content, response.url)",,"Fix project deletion for acceptance tests

There is a problem with acceptance deletion after test run completion.
Script tried to delete project before deployment removal, so that we have an error for this.","Added try-finally for acceptance tests resource deletion
Change fixture initialization for prediction acceptance tests to have proper resource utilization order
Fixed logging for tests
Bump timeout for nginx max file size test

diff --git a/tests/api_acceptance_tests/conftest.py b/tests/api_acceptance_tests/conftest.py
index 80ee96c1c32abd..774ad12628ba9c 100644
--- a/tests/api_acceptance_tests/conftest.py
+++ b/tests/api_acceptance_tests/conftest.py
@@ -482,7 +482,7 @@ def admin_user(create_user):
 
 @pytest.fixture(scope='session')
 def v2_client(user, create_datarobot_client):
-    print('creating v2_client')
+    print('Create v2_client')
     print(user)
     return create_datarobot_client(user_info=user)
 
@@ -513,7 +513,7 @@ def _create_project(project_type=""binary_csv""):
         dataset_path = os.path.join(fixture_data_path, pr_conf[""dataset_name""])
         project_target = pr_conf[""target""]
 
-        print(""Creating project"")
+        print(""Create project"")
         project = dr.Project.create(dataset_path, project_name=project_name)
         project.analyze_and_model(
             target=project_target,
@@ -551,15 +551,16 @@ def _create_prediction_environment():
         prediction_environment_list.append(prediction_environment)
         return prediction_environment
 
-    yield _create_prediction_environment
-
-    for environment in prediction_environment_list:
-        try:
-            print(f""Deleting environment {environment.id}"")
-            environment.delete()
-        # pylint: disable=broad-except
-        except Exception as e:
-            print(f""WARNING: failed to delete environment {environment.id}. \n{e}"")
+    try:
+        yield _create_prediction_environment
+    finally:
+        for environment in prediction_environment_list:
+            try:
+                print(f""Delete environment {environment.id}"")
+                environment.delete()
+            # pylint: disable=broad-except
+            except Exception as e:
+                print(f""WARNING: Failed to delete environment {environment.id}. \n{e}"")
 
 
 @pytest.fixture(scope='session')
@@ -584,7 +585,7 @@ def create_deployment(v2_client, create_prediction_environment, is_enterprise):
 
     def _create_deployment(model, nextgen_predictions=False):
         # pylint: disable=logging-dynamic-error-message
-        print(f""Creating deployment for {model.model_type}"" f""(Nextgen={nextgen_predictions})"")
+        print(f""Create deployment for {model.model_type}"" f""(Nextgen={nextgen_predictions})"")
         if nextgen_predictions:
             try:
                 prediction_environment_id = [
@@ -614,7 +615,7 @@ def _create_deployment(model, nextgen_predictions=False):
             deployment_id = response.json()[""id""]
             deployment = dr.Deployment.get(deployment_id)
 
-            print(f""Activate realtime predictions for deployment {deployment_id}"")
+            print(f""Activate realtime predictions for deployment"")
             payload = {
                 ""predictionsSettings"": {
                     ""realTime"": True,
@@ -653,25 +654,26 @@ def _create_deployment(model, nextgen_predictions=False):
         deployment_list.append(deployment)
         return deployment
 
-    yield _create_deployment
-
-    for deployment in deployment_list:
-        try:
-            print(f""Deleting deployment {deployment.id}"")
-            deployment.delete()
-        # pylint: disable=broad-except
-        except Exception as e:
-            # pylint: disable=logging-dynamic-error-message
-            print(f""WARNING: Failed to delete deployment {deployment.id}. \n{e}"")
-
-    if mlpkg_list:
-        for mlpkg in mlpkg_list:
+    try:
+        yield _create_deployment
+    finally:
+        for deployment in deployment_list:
             try:
-                print(f""Archive mlpkg {mlpkg}"")
-                v2_client.post(f""modelPackages/{mlpkg}/archive/"")
+                print(f""Delete deployment {deployment.id}"")
+                deployment.delete()
             # pylint: disable=broad-except
             except Exception as e:
-                print(f""WARNING: Failed to archive mlpkg {mlpkg}. \n{e}"")
+                # pylint: disable=logging-dynamic-error-message
+                print(f""WARNING: Failed to delete deployment {deployment.id}. \n{e}"")
+
+        if mlpkg_list:
+            for mlpkg in mlpkg_list:
+                try:
+                    print(f""Archive mlpkg {mlpkg}"")
+                    v2_client.post(f""modelPackages/{mlpkg}/archive/"")
+                # pylint: disable=broad-except
+                except Exception as e:
+                    print(f""WARNING: Failed to archive mlpkg {mlpkg}. \n{e}"")
 
 
 @pytest.fixture(scope='session')
@@ -735,7 +737,7 @@ def _create_deployment_from_mlpkg(project_name, nextgen_predictions=False):
         wait_for_async_resolution(v2_client, response.headers[""Location""])
         deployment_id = response.json()[""id""]
         if nextgen_predictions:
-            print(f""Activate realtime predictions for deployment {deployment_id}"")
+            print(f""Activate realtime predictions for deployment"")
             payload = {
                 ""predictionsSettings"": {
                     ""realTime"": True,
@@ -762,20 +764,21 @@ def _create_deployment_from_mlpkg(project_name, nextgen_predictions=False):
         deployment_list.append(deployment)
         return deployment
 
-    yield _create_deployment_from_mlpkg
-
-    for deployment in deployment_list:
-        try:
-            print(f""Deleting deployment {deployment.id}"")
-            deployment.delete()
-        # pylint: disable=broad-except
-        except Exception as e:
-            print(f""WARNING: Failed to delete deployment {deployment.id}. \n{e}"")
+    try:
+        yield _create_deployment_from_mlpkg
+    finally:
+        for deployment in deployment_list:
+            try:
+                print(f""Delete deployment {deployment.id}"")
+                deployment.delete()
+            # pylint: disable=broad-except
+            except Exception as e:
+                print(f""WARNING: Failed to delete deployment {deployment.id}. \n{e}"")
 
-    for mlpkg in mlpkg_list:
-        try:
-            print(f""Archive mlpkg {mlpkg}"")
-            v2_client.post(f""modelPackages/{mlpkg}/archive/"")
-        # pylint: disable=broad-except
-        except Exception as e:
-            print(f""WARNING: Failed to archive mlpkg {mlpkg}. \n{e}"")
+        for mlpkg in mlpkg_list:
+            try:
+                print(f""Archive mlpkg {mlpkg}"")
+                v2_client.post(f""modelPackages/{mlpkg}/archive/"")
+            # pylint: disable=broad-except
+            except Exception as e:
+                print(f""WARNING: Failed to archive mlpkg {mlpkg}. \n{e}"")
diff --git a/tests/api_acceptance_tests/predictions/conftest.py b/tests/api_acceptance_tests/predictions/conftest.py
index cb0e62c5b8bff2..3616b7651c50e7 100644
--- a/tests/api_acceptance_tests/predictions/conftest.py
+++ b/tests/api_acceptance_tests/predictions/conftest.py
@@ -45,11 +45,11 @@ def _train_model(project, explanations=False):
 
 @pytest.fixture(scope='session')
 def prediction_test_data(
+    create_project,
+    train_model,
     create_deployment,
     create_deployment_from_mlpkg,
     prediction_dataset_path,
-    create_project,
-    train_model,
     is_enterprise,
 ):
     if is_enterprise:
@@ -60,16 +60,19 @@ def prediction_test_data(
         project = create_project(project_type=""japanese"")
         model = train_model(project=project, explanations=True)
         deployment = create_deployment(model=model)
+        print(f""Project id: {project.id}"")
+        print(f""Model id: {model.id}"")
+    print(f""Deployment id: {deployment.id}"")
     pred_file_path = prediction_dataset_path(project_name=""japanese"")
     return deployment, pred_file_path
 
 
 @pytest.fixture(scope='session')
 def prediction_test_nextgen_data(
-    create_deployment,
-    create_deployment_from_mlpkg,
     create_project,
     train_model,
+    create_deployment,
+    create_deployment_from_mlpkg,
     prediction_dataset_path,
     is_enterprise,
     is_local_dev_env,
@@ -84,6 +87,9 @@ def prediction_test_nextgen_data(
             project = create_project(project_type=""japanese"")
             model = train_model(project=project)
             deployment = create_deployment(model=model, nextgen_predictions=True)
+            print(f""Project id: {project.id}"")
+            print(f""Model id: {model.id}"")
+        print(f""Deployment id: {deployment.id}"")
         pred_file_path = prediction_dataset_path(project_name=""japanese"")
         return deployment, pred_file_path
 
@@ -121,7 +127,7 @@ def realtime_prediction_environment(v2_client):
     pred_environment_list = []
 
     def _realtime_prediction_environment():
-        print(""Create realtime prediction environment"")
+        print(""Create serverless prediction environment"")
         prediction_environment = dr.PredictionEnvironment.create(
             name=""acceptance_test_pred_env"",
             platform=""datarobotServerless"",
@@ -130,12 +136,13 @@ def _realtime_prediction_environment():
         pred_environment_list.append(prediction_environment)
         return prediction_environment
 
-    yield _realtime_prediction_environment
-
-    for env in pred_environment_list:
-        try:
-            print(f""Deleting prediction environment {env.id}"")
-            env.delete()
-        # pylint: disable=broad-except
-        except Exception as e:
-            print(f""WARNING: Failed to delete prediction environment {env.id}. \n{e}"")
+    try:
+        yield _realtime_prediction_environment
+    finally:
+        for env in pred_environment_list:
+            try:
+                print(f""Delete prediction environment {env.id}"")
+                env.delete()
+            # pylint: disable=broad-except
+            except Exception as e:
+                print(f""WARNING: Failed to delete prediction environment {env.id}. \n{e}"")
diff --git a/tests/api_acceptance_tests/predictions/test_nginx_configuration.py b/tests/api_acceptance_tests/predictions/test_nginx_configuration.py
index 68da20f584fa47..d2879556ebbad9 100644
--- a/tests/api_acceptance_tests/predictions/test_nginx_configuration.py
+++ b/tests/api_acceptance_tests/predictions/test_nginx_configuration.py
@@ -213,7 +213,7 @@ def test_prediction_dataset_size_limit_50mb(
             url,
             files={'file': (""generated_dataset.csv"", f)},
             allow_redirects=False,
-            timeout=120,
+            timeout=200,
         )
         assert expected_response_code == response.status_code, (response.content, response.url)"
Fix prediction acceptance tests for staging run,There is an error with mlpkg import with acceptance tests on staging,"Fix defect with deployment creation for acceptance tests. Made a mistake with the way we do this based on DR type (on-prem or saas)
","diff --git a/tests/api_acceptance_tests/predictions/conftest.py b/tests/api_acceptance_tests/predictions/conftest.py
index 5de8493a6b5dd0..cb0e62c5b8bff2 100644
--- a/tests/api_acceptance_tests/predictions/conftest.py
+++ b/tests/api_acceptance_tests/predictions/conftest.py
@@ -53,13 +53,13 @@ def prediction_test_data(
     is_enterprise,
 ):
     if is_enterprise:
-        project = create_project(project_type=""japanese"")
-        model = train_model(project=project, explanations=True)
-        deployment = create_deployment(model=model)
-    else:
         deployment = create_deployment_from_mlpkg(
             project_name=""japanese"",
         )
+    else:
+        project = create_project(project_type=""japanese"")
+        model = train_model(project=project, explanations=True)
+        deployment = create_deployment(model=model)
     pred_file_path = prediction_dataset_path(project_name=""japanese"")
     return deployment, pred_file_path
 
@@ -76,14 +76,14 @@ def prediction_test_nextgen_data(
 ):
     if not is_local_dev_env:
         if is_enterprise:
-            project = create_project(project_type=""japanese"")
-            model = train_model(project=project)
-            deployment = create_deployment(model=model, nextgen_predictions=True)
-        else:
             deployment = create_deployment_from_mlpkg(
                 project_name=""japanese"",
                 nextgen_predictions=True,
             )
+        else:
+            project = create_project(project_type=""japanese"")
+            model = train_model(project=project)
+            deployment = create_deployment(model=model, nextgen_predictions=True)
         pred_file_path = prediction_dataset_path(project_name=""japanese"")
         return deployment, pred_file_path",,"Fix prediction acceptance tests for staging run

There is an error with mlpkg import with acceptance tests on staging","Fix defect with deployment creation for acceptance tests. Made a mistake with the way we do this based on DR type (on-prem or saas)


diff --git a/tests/api_acceptance_tests/predictions/conftest.py b/tests/api_acceptance_tests/predictions/conftest.py
index 5de8493a6b5dd0..cb0e62c5b8bff2 100644
--- a/tests/api_acceptance_tests/predictions/conftest.py
+++ b/tests/api_acceptance_tests/predictions/conftest.py
@@ -53,13 +53,13 @@ def prediction_test_data(
     is_enterprise,
 ):
     if is_enterprise:
-        project = create_project(project_type=""japanese"")
-        model = train_model(project=project, explanations=True)
-        deployment = create_deployment(model=model)
-    else:
         deployment = create_deployment_from_mlpkg(
             project_name=""japanese"",
         )
+    else:
+        project = create_project(project_type=""japanese"")
+        model = train_model(project=project, explanations=True)
+        deployment = create_deployment(model=model)
     pred_file_path = prediction_dataset_path(project_name=""japanese"")
     return deployment, pred_file_path
 
@@ -76,14 +76,14 @@ def prediction_test_nextgen_data(
 ):
     if not is_local_dev_env:
         if is_enterprise:
-            project = create_project(project_type=""japanese"")
-            model = train_model(project=project)
-            deployment = create_deployment(model=model, nextgen_predictions=True)
-        else:
             deployment = create_deployment_from_mlpkg(
                 project_name=""japanese"",
                 nextgen_predictions=True,
             )
+        else:
+            project = create_project(project_type=""japanese"")
+            model = train_model(project=project)
+            deployment = create_deployment(model=model, nextgen_predictions=True)
         pred_file_path = prediction_dataset_path(project_name=""japanese"")
         return deployment, pred_file_path"
External Prediction URL doesn't Wrap in Console UI,"It looks like for External Deployments (in this case at least Sagemake deployments), if the External Predictions URL is sufficiently long, it is not truncated or wrapped, but instead continues on to the next column in the prediction details UI",Wrap long text on console overview,"diff --git a/client/js/deployments/console/console-deployment/console-deployment-overview/console-deployment-overview-details.less b/client/js/deployments/console/console-deployment/console-deployment-overview/console-deployment-overview-details.less
index 4fdf2de2310b33..ab2d809b12fb47 100644
--- a/client/js/deployments/console/console-deployment/console-deployment-overview/console-deployment-overview-details.less
+++ b/client/js/deployments/console/console-deployment/console-deployment-overview/console-deployment-overview-details.less
@@ -20,6 +20,7 @@
   .console-deployment-overview-details-row-description {
     width: 70%;
     padding-bottom: @spacing-03;
+    overflow-wrap: break-word;
   }
 
   .created-by-info {",,"External Prediction URL doesn't Wrap in Console UI

It looks like for External Deployments (in this case at least Sagemake deployments), if the External Predictions URL is sufficiently long, it is not truncated or wrapped, but instead continues on to the next column in the prediction details UI","Wrap long text on console overview

diff --git a/client/js/deployments/console/console-deployment/console-deployment-overview/console-deployment-overview-details.less b/client/js/deployments/console/console-deployment/console-deployment-overview/console-deployment-overview-details.less
index 4fdf2de2310b33..ab2d809b12fb47 100644
--- a/client/js/deployments/console/console-deployment/console-deployment-overview/console-deployment-overview-details.less
+++ b/client/js/deployments/console/console-deployment/console-deployment-overview/console-deployment-overview-details.less
@@ -20,6 +20,7 @@
   .console-deployment-overview-details-row-description {
     width: 70%;
     padding-bottom: @spacing-03;
+    overflow-wrap: break-word;
   }
 
   .created-by-info {"
Invalid model-metadata.yaml upload results in 500 from server,"This one should reproduce - just create a custom model version with it, from peaking at kibana it looks like it’s caused by accidentally passing a string default value when a numeric runtime parameter is specified




name: ""DataAnalyst Jr Analyst""
type: inference
targetType: ""textgeneration""
runtimeParameterDefinitions:
- fieldName: py_sandbox_deployment
  type: string
  defaultValue: ""663a31e5527f6f492c3b7b5d""
  description: Deployment for executing sandboxed python code

- fieldName: py_generator_deployment
  type: string
  defaultValue: ""66421d193cadab5a55a0a677""
  description: Deployment for generating python code based on user instruction(s)

- fieldName: messages_feature_name
  type: string
  defaultValue: ""messagesJSON""
  description: Name of the feature in the prediction data containing the message history data (JSON encoded)

- fieldName: data_feature_name
  type: string
  defaultValue: ""dataset_id""
  description: Name of the feature in the prediction data containing an (optional) AI catalog dataset ID against which analysis will be performed

- fieldName: output_type_feature_name
  type: string
  defaultValue: ""output_type""
  description: Name of the feature in the prediction data containing output validation to be performed (1 == enforce base64 png output)

- fieldName: returncode_feature_name
  type: string
  defaultValue: ""returncode""
  description: Name of the output variable indicating if the analysis was successful (0 == successful)

- fieldName: target_feature_name
  type: string
  defaultValue: ""completion""
  description: Name of the target variable for the deployment, this will contain the result of the analysis

- fieldName: trace_feature_name
  type: string
  defaultValue: ""trace""
  description: Name of the output variable containing the trace for the analysis orchestration

- fieldName: max_retries
  type: numeric
  defaultValue: ""2""
  description: Number of times to reattempt code generation if code fails to execute","Convert a numeric string value into float
Unit tests","diff --git a/raptor_lake/custom_model/runtime_parameters.py b/raptor_lake/custom_model/runtime_parameters.py
index 78788073f92a82..a7aee031fba620 100644
--- a/raptor_lake/custom_model/runtime_parameters.py
+++ b/raptor_lake/custom_model/runtime_parameters.py
@@ -81,9 +81,10 @@ def get_numeric_string_param_value(launch_config, param_definition):
     str_value, numeric_value = '', 0  # these are default values
 
     if param_definition.type == RuntimeVariableType.NUMERIC:
-        # if type is numeric, string value should be empty
-        # if current_value is None, default to 0
-        numeric_value = current_value or 0
+        # Notes:
+        # 1. If type is numeric, string value should be empty
+        # 2. A numeric value might be provided as a string. We need to convert it to float.
+        numeric_value = 0 if current_value is None else float(current_value)
     elif param_definition.type == RuntimeVariableType.CREDENTIAL:
         # Specifying this separator causes the output to be minified.
         str_value = (
diff --git a/tests/backend/unit/raptor_lake/custom_model/test_custom_model_runtime_parameters.py b/tests/backend/unit/raptor_lake/custom_model/test_custom_model_runtime_parameters.py
index ef9983154c6423..c4072f57893679 100644
--- a/tests/backend/unit/raptor_lake/custom_model/test_custom_model_runtime_parameters.py
+++ b/tests/backend/unit/raptor_lake/custom_model/test_custom_model_runtime_parameters.py
@@ -93,100 +93,154 @@ def key_value_service():
         yield kvs(persistent=None)
 
 
-def test_export_runtime_params_to_key_values_empty(key_value_service, custom_model_version):
-    user = Mock()
-    entity_id = ObjectId()
+class TestExportRuntimeParameters:
+    def test_to_key_values_empty(self, key_value_service, custom_model_version):
+        user = Mock()
+        entity_id = ObjectId()
 
-    export_runtime_parameters_to_key_values(
-        custom_model_version, entity_id, KeyValueEntityType.MODEL_PACKAGE, user
-    )
-    key_value_service.create.assert_not_called()
+        export_runtime_parameters_to_key_values(
+            custom_model_version, entity_id, KeyValueEntityType.MODEL_PACKAGE, user
+        )
+        key_value_service.create.assert_not_called()
 
+    def test_to_key_values_success(self, key_value_service, custom_model_version):
+        """"""
+        Export runtime variables from a prepared LaunchConfiguration to key-values.
+        :param key_value_service: mocked KeyValueService; we check called args
+        :param custom_model_version: mostly empty CustomTaskVersion
+        """"""
+        user = Mock()
+        user.uid = ObjectId()
+        user.username = ""unittest@datarobot.com""
+        entity_id = ObjectId()
+        default_deployment_id = ObjectId()
+        override_deployment_id = ObjectId()
+        override_credential_value = {'credential_id': ObjectId(), 'user_id': user.uid}
+
+        custom_model_version.launch_configuration = LaunchConfiguration(
+            schema=[
+                CredentialRuntimeVariableDefinition(
+                    field_name='MyCredentialVar', type=RuntimeVariableType.CREDENTIAL
+                ),
+                StringRuntimeVariableDefinition(
+                    field_name='MyVar2', type=RuntimeVariableType.STRING
+                ),
+                StringRuntimeVariableDefinition(
+                    field_name='MyVar3', type=RuntimeVariableType.STRING, default_value='Hola'
+                ),
+                DeploymentIdRuntimeVariableDefinition(
+                    field_name='DeploymentId1',
+                    type=RuntimeVariableType.DEPLOYMENT,
+                    default_value=default_deployment_id,
+                ),
+                BooleanRuntimeVariableDefinition(
+                    field_name=""MyBoolVar1"", type=RuntimeVariableType.BOOLEAN, default_value=True
+                ),
+                NumericRuntimeVariableDefinition(
+                    field_name=""MyNumericVar"", type=RuntimeVariableType.NUMERIC, default_value=50
+                ),
+                NumericRuntimeVariableDefinition(
+                    field_name=""MyNumericVar2"", type=RuntimeVariableType.NUMERIC
+                ),
+            ],
+            overrides=[
+                CredentialRuntimeVariableValue(
+                    field_name='MyCredentialVar',
+                    type=RuntimeVariableType.CREDENTIAL,
+                    value=override_credential_value,
+                ),
+                StringRuntimeVariableValue(
+                    field_name='MyVar3', type=RuntimeVariableType.STRING, value='Hello'
+                ),
+                DeploymentIdRuntimeVariableValue(
+                    field_name='DeploymentId1',
+                    type=RuntimeVariableType.DEPLOYMENT,
+                    value=override_deployment_id,
+                ),
+                BooleanRuntimeVariableValue(
+                    field_name='MyBoolVar1', type=RuntimeVariableType.BOOLEAN, value=False
+                ),
+                NumericRuntimeVariableValue(
+                    field_name='MyNumericVar', type=RuntimeVariableType.NUMERIC, value=55
+                ),
+            ],
+        )
 
-def test_export_runtime_params_to_key_values(key_value_service, custom_model_version):
-    """"""
-    Export runtime variables from a prepared LaunchConfiguration to key-values.
-    :param key_value_service: mocked KeyValueService; we check called args
-    :param custom_model_version: mostly empty CustomTaskVersion
-    """"""
-    user = Mock()
-    user.uid = ObjectId()
-    user.username = ""unittest@datarobot.com""
-    entity_id = ObjectId()
-    default_deployment_id = ObjectId()
-    override_deployment_id = ObjectId()
-    override_credential_value = {'credential_id': ObjectId(), 'user_id': user.uid}
-
-    custom_model_version.launch_configuration = LaunchConfiguration(
-        schema=[
-            CredentialRuntimeVariableDefinition(
-                field_name='MyCredentialVar', type=RuntimeVariableType.CREDENTIAL
-            ),
-            StringRuntimeVariableDefinition(field_name='MyVar2', type=RuntimeVariableType.STRING),
-            StringRuntimeVariableDefinition(
-                field_name='MyVar3', type=RuntimeVariableType.STRING, default_value='Hola'
-            ),
-            DeploymentIdRuntimeVariableDefinition(
-                field_name='DeploymentId1',
-                type=RuntimeVariableType.DEPLOYMENT,
-                default_value=default_deployment_id,
-            ),
-            BooleanRuntimeVariableDefinition(
-                field_name=""MyBoolVar1"", type=RuntimeVariableType.BOOLEAN, default_value=True
-            ),
-            NumericRuntimeVariableDefinition(
-                field_name=""MyNumericVar"", type=RuntimeVariableType.NUMERIC, default_value=50
-            ),
-            NumericRuntimeVariableDefinition(
-                field_name=""MyNumericVar2"", type=RuntimeVariableType.NUMERIC
-            ),
-        ],
-        overrides=[
-            CredentialRuntimeVariableValue(
-                field_name='MyCredentialVar',
-                type=RuntimeVariableType.CREDENTIAL,
-                value=override_credential_value,
-            ),
-            StringRuntimeVariableValue(
-                field_name='MyVar3', type=RuntimeVariableType.STRING, value='Hello'
-            ),
-            DeploymentIdRuntimeVariableValue(
-                field_name='DeploymentId1',
-                type=RuntimeVariableType.DEPLOYMENT,
-                value=override_deployment_id,
-            ),
-            BooleanRuntimeVariableValue(
-                field_name='MyBoolVar1', type=RuntimeVariableType.BOOLEAN, value=False
-            ),
-            NumericRuntimeVariableValue(
-                field_name='MyNumericVar', type=RuntimeVariableType.NUMERIC, value=55
-            ),
-        ],
-    )
+        credential_value = json.dumps(
+            override_credential_value,
+            cls=GenericJSONEncoder,
+            separators=(',', ':'),
+        )
+        expected = [
+            ('MyCredentialVar', credential_value, 0),
+            ('MyVar2', '', 0),
+            ('MyVar3', 'Hello', 0),
+            ('DeploymentId1', str(override_deployment_id), 0),
+            ('MyBoolVar1', 'False', 0),
+            ('MyNumericVar', '', 55),
+            ('MyNumericVar2', '', 0),
+        ]
 
-    credential_value = json.dumps(
-        override_credential_value,
-        cls=GenericJSONEncoder,
-        separators=(',', ':'),
-    )
-    expected = [
-        ('MyCredentialVar', credential_value, 0),
-        ('MyVar2', '', 0),
-        ('MyVar3', 'Hello', 0),
-        ('DeploymentId1', str(override_deployment_id), 0),
-        ('MyBoolVar1', 'False', 0),
-        ('MyNumericVar', '', 55),
-        ('MyNumericVar2', '', 0),
-    ]
-
-    export_runtime_parameters_to_key_values(
-        custom_model_version, entity_id, KeyValueEntityType.MODEL_PACKAGE, user
+        export_runtime_parameters_to_key_values(
+            custom_model_version, entity_id, KeyValueEntityType.MODEL_PACKAGE, user
+        )
+        actual_key_values = [
+            (args[0].name, args[0].value, args[0].numeric_value)
+            for args, _ in key_value_service.create.call_args_list
+        ]
+        assert actual_key_values == expected
+
+    @pytest.mark.parametrize(
+        'value, expected_result',
+        [
+            (2, 2.0),
+            (2.0, 2.0),
+            ('2', 2.0),
+            (None, 50.0),  # default value
+            ('', ValueError),
+            ('non-numeric-str', ValueError),
+        ],
+        ids=['int', 'float', 'str', 'none', 'empty-str', 'non-numeric-str'],
     )
-    actual_key_values = [
-        (args[0].name, args[0].value, args[0].numeric_value)
-        for args, _ in key_value_service.create.call_args_list
-    ]
-    assert actual_key_values == expected
+    def test_numeric_values(self, key_value_service, custom_model_version, value, expected_result):
+        field_name = 'MyNumericVar'
+        if expected_result == ValueError:
+            expected_err = (
+                f""Set error for field: value. Setting value: '{value}'. ""
+                ""Error: value can't be converted to float""
+            )
+            with pytest.raises(ValueError, match=expected_err):
+                NumericRuntimeVariableValue(
+                    field_name=field_name, type=RuntimeVariableType.NUMERIC, value=value
+                )
+        else:
+            user = Mock()
+            user.uid = ObjectId()
+            user.username = ""unittest@datarobot.com""
+            entity_id = ObjectId()
+
+            custom_model_version.launch_configuration = LaunchConfiguration(
+                schema=[
+                    NumericRuntimeVariableDefinition(
+                        field_name=field_name,
+                        type=RuntimeVariableType.NUMERIC,
+                        default_value=50,
+                    ),
+                ],
+                overrides=[
+                    NumericRuntimeVariableValue(
+                        field_name=field_name, type=RuntimeVariableType.NUMERIC, value=value
+                    ),
+                ],
+            )
+            export_runtime_parameters_to_key_values(
+                custom_model_version, entity_id, KeyValueEntityType.MODEL_PACKAGE, user
+            )
+            key_value_service.create.assert_called_once()
+            key_value_arg = key_value_service.create.call_args.args[0]
+            assert key_value_arg.name == field_name
+            assert key_value_arg.value == ''
+            assert key_value_arg.numeric_value == expected_result
 
 
 class TestRuntimeParametersParser:",,"Invalid model-metadata.yaml upload results in 500 from server

This one should reproduce - just create a custom model version with it, from peaking at kibana it looks like it’s caused by accidentally passing a string default value when a numeric runtime parameter is specified




name: ""DataAnalyst Jr Analyst""
type: inference
targetType: ""textgeneration""
runtimeParameterDefinitions:
- fieldName: py_sandbox_deployment
  type: string
  defaultValue: ""663a31e5527f6f492c3b7b5d""
  description: Deployment for executing sandboxed python code

- fieldName: py_generator_deployment
  type: string
  defaultValue: ""66421d193cadab5a55a0a677""
  description: Deployment for generating python code based on user instruction(s)

- fieldName: messages_feature_name
  type: string
  defaultValue: ""messagesJSON""
  description: Name of the feature in the prediction data containing the message history data (JSON encoded)

- fieldName: data_feature_name
  type: string
  defaultValue: ""dataset_id""
  description: Name of the feature in the prediction data containing an (optional) AI catalog dataset ID against which analysis will be performed

- fieldName: output_type_feature_name
  type: string
  defaultValue: ""output_type""
  description: Name of the feature in the prediction data containing output validation to be performed (1 == enforce base64 png output)

- fieldName: returncode_feature_name
  type: string
  defaultValue: ""returncode""
  description: Name of the output variable indicating if the analysis was successful (0 == successful)

- fieldName: target_feature_name
  type: string
  defaultValue: ""completion""
  description: Name of the target variable for the deployment, this will contain the result of the analysis

- fieldName: trace_feature_name
  type: string
  defaultValue: ""trace""
  description: Name of the output variable containing the trace for the analysis orchestration

- fieldName: max_retries
  type: numeric
  defaultValue: ""2""
  description: Number of times to reattempt code generation if code fails to execute","Convert a numeric string value into float
Unit tests

diff --git a/raptor_lake/custom_model/runtime_parameters.py b/raptor_lake/custom_model/runtime_parameters.py
index 78788073f92a82..a7aee031fba620 100644
--- a/raptor_lake/custom_model/runtime_parameters.py
+++ b/raptor_lake/custom_model/runtime_parameters.py
@@ -81,9 +81,10 @@ def get_numeric_string_param_value(launch_config, param_definition):
     str_value, numeric_value = '', 0  # these are default values
 
     if param_definition.type == RuntimeVariableType.NUMERIC:
-        # if type is numeric, string value should be empty
-        # if current_value is None, default to 0
-        numeric_value = current_value or 0
+        # Notes:
+        # 1. If type is numeric, string value should be empty
+        # 2. A numeric value might be provided as a string. We need to convert it to float.
+        numeric_value = 0 if current_value is None else float(current_value)
     elif param_definition.type == RuntimeVariableType.CREDENTIAL:
         # Specifying this separator causes the output to be minified.
         str_value = (
diff --git a/tests/backend/unit/raptor_lake/custom_model/test_custom_model_runtime_parameters.py b/tests/backend/unit/raptor_lake/custom_model/test_custom_model_runtime_parameters.py
index ef9983154c6423..c4072f57893679 100644
--- a/tests/backend/unit/raptor_lake/custom_model/test_custom_model_runtime_parameters.py
+++ b/tests/backend/unit/raptor_lake/custom_model/test_custom_model_runtime_parameters.py
@@ -93,100 +93,154 @@ def key_value_service():
         yield kvs(persistent=None)
 
 
-def test_export_runtime_params_to_key_values_empty(key_value_service, custom_model_version):
-    user = Mock()
-    entity_id = ObjectId()
+class TestExportRuntimeParameters:
+    def test_to_key_values_empty(self, key_value_service, custom_model_version):
+        user = Mock()
+        entity_id = ObjectId()
 
-    export_runtime_parameters_to_key_values(
-        custom_model_version, entity_id, KeyValueEntityType.MODEL_PACKAGE, user
-    )
-    key_value_service.create.assert_not_called()
+        export_runtime_parameters_to_key_values(
+            custom_model_version, entity_id, KeyValueEntityType.MODEL_PACKAGE, user
+        )
+        key_value_service.create.assert_not_called()
 
+    def test_to_key_values_success(self, key_value_service, custom_model_version):
+        """"""
+        Export runtime variables from a prepared LaunchConfiguration to key-values.
+        :param key_value_service: mocked KeyValueService; we check called args
+        :param custom_model_version: mostly empty CustomTaskVersion
+        """"""
+        user = Mock()
+        user.uid = ObjectId()
+        user.username = ""unittest@datarobot.com""
+        entity_id = ObjectId()
+        default_deployment_id = ObjectId()
+        override_deployment_id = ObjectId()
+        override_credential_value = {'credential_id': ObjectId(), 'user_id': user.uid}
+
+        custom_model_version.launch_configuration = LaunchConfiguration(
+            schema=[
+                CredentialRuntimeVariableDefinition(
+                    field_name='MyCredentialVar', type=RuntimeVariableType.CREDENTIAL
+                ),
+                StringRuntimeVariableDefinition(
+                    field_name='MyVar2', type=RuntimeVariableType.STRING
+                ),
+                StringRuntimeVariableDefinition(
+                    field_name='MyVar3', type=RuntimeVariableType.STRING, default_value='Hola'
+                ),
+                DeploymentIdRuntimeVariableDefinition(
+                    field_name='DeploymentId1',
+                    type=RuntimeVariableType.DEPLOYMENT,
+                    default_value=default_deployment_id,
+                ),
+                BooleanRuntimeVariableDefinition(
+                    field_name=""MyBoolVar1"", type=RuntimeVariableType.BOOLEAN, default_value=True
+                ),
+                NumericRuntimeVariableDefinition(
+                    field_name=""MyNumericVar"", type=RuntimeVariableType.NUMERIC, default_value=50
+                ),
+                NumericRuntimeVariableDefinition(
+                    field_name=""MyNumericVar2"", type=RuntimeVariableType.NUMERIC
+                ),
+            ],
+            overrides=[
+                CredentialRuntimeVariableValue(
+                    field_name='MyCredentialVar',
+                    type=RuntimeVariableType.CREDENTIAL,
+                    value=override_credential_value,
+                ),
+                StringRuntimeVariableValue(
+                    field_name='MyVar3', type=RuntimeVariableType.STRING, value='Hello'
+                ),
+                DeploymentIdRuntimeVariableValue(
+                    field_name='DeploymentId1',
+                    type=RuntimeVariableType.DEPLOYMENT,
+                    value=override_deployment_id,
+                ),
+                BooleanRuntimeVariableValue(
+                    field_name='MyBoolVar1', type=RuntimeVariableType.BOOLEAN, value=False
+                ),
+                NumericRuntimeVariableValue(
+                    field_name='MyNumericVar', type=RuntimeVariableType.NUMERIC, value=55
+                ),
+            ],
+        )
 
-def test_export_runtime_params_to_key_values(key_value_service, custom_model_version):
-    """"""
-    Export runtime variables from a prepared LaunchConfiguration to key-values.
-    :param key_value_service: mocked KeyValueService; we check called args
-    :param custom_model_version: mostly empty CustomTaskVersion
-    """"""
-    user = Mock()
-    user.uid = ObjectId()
-    user.username = ""unittest@datarobot.com""
-    entity_id = ObjectId()
-    default_deployment_id = ObjectId()
-    override_deployment_id = ObjectId()
-    override_credential_value = {'credential_id': ObjectId(), 'user_id': user.uid}
-
-    custom_model_version.launch_configuration = LaunchConfiguration(
-        schema=[
-            CredentialRuntimeVariableDefinition(
-                field_name='MyCredentialVar', type=RuntimeVariableType.CREDENTIAL
-            ),
-            StringRuntimeVariableDefinition(field_name='MyVar2', type=RuntimeVariableType.STRING),
-            StringRuntimeVariableDefinition(
-                field_name='MyVar3', type=RuntimeVariableType.STRING, default_value='Hola'
-            ),
-            DeploymentIdRuntimeVariableDefinition(
-                field_name='DeploymentId1',
-                type=RuntimeVariableType.DEPLOYMENT,
-                default_value=default_deployment_id,
-            ),
-            BooleanRuntimeVariableDefinition(
-                field_name=""MyBoolVar1"", type=RuntimeVariableType.BOOLEAN, default_value=True
-            ),
-            NumericRuntimeVariableDefinition(
-                field_name=""MyNumericVar"", type=RuntimeVariableType.NUMERIC, default_value=50
-            ),
-            NumericRuntimeVariableDefinition(
-                field_name=""MyNumericVar2"", type=RuntimeVariableType.NUMERIC
-            ),
-        ],
-        overrides=[
-            CredentialRuntimeVariableValue(
-                field_name='MyCredentialVar',
-                type=RuntimeVariableType.CREDENTIAL,
-                value=override_credential_value,
-            ),
-            StringRuntimeVariableValue(
-                field_name='MyVar3', type=RuntimeVariableType.STRING, value='Hello'
-            ),
-            DeploymentIdRuntimeVariableValue(
-                field_name='DeploymentId1',
-                type=RuntimeVariableType.DEPLOYMENT,
-                value=override_deployment_id,
-            ),
-            BooleanRuntimeVariableValue(
-                field_name='MyBoolVar1', type=RuntimeVariableType.BOOLEAN, value=False
-            ),
-            NumericRuntimeVariableValue(
-                field_name='MyNumericVar', type=RuntimeVariableType.NUMERIC, value=55
-            ),
-        ],
-    )
+        credential_value = json.dumps(
+            override_credential_value,
+            cls=GenericJSONEncoder,
+            separators=(',', ':'),
+        )
+        expected = [
+            ('MyCredentialVar', credential_value, 0),
+            ('MyVar2', '', 0),
+            ('MyVar3', 'Hello', 0),
+            ('DeploymentId1', str(override_deployment_id), 0),
+            ('MyBoolVar1', 'False', 0),
+            ('MyNumericVar', '', 55),
+            ('MyNumericVar2', '', 0),
+        ]
 
-    credential_value = json.dumps(
-        override_credential_value,
-        cls=GenericJSONEncoder,
-        separators=(',', ':'),
-    )
-    expected = [
-        ('MyCredentialVar', credential_value, 0),
-        ('MyVar2', '', 0),
-        ('MyVar3', 'Hello', 0),
-        ('DeploymentId1', str(override_deployment_id), 0),
-        ('MyBoolVar1', 'False', 0),
-        ('MyNumericVar', '', 55),
-        ('MyNumericVar2', '', 0),
-    ]
-
-    export_runtime_parameters_to_key_values(
-        custom_model_version, entity_id, KeyValueEntityType.MODEL_PACKAGE, user
+        export_runtime_parameters_to_key_values(
+            custom_model_version, entity_id, KeyValueEntityType.MODEL_PACKAGE, user
+        )
+        actual_key_values = [
+            (args[0].name, args[0].value, args[0].numeric_value)
+            for args, _ in key_value_service.create.call_args_list
+        ]
+        assert actual_key_values == expected
+
+    @pytest.mark.parametrize(
+        'value, expected_result',
+        [
+            (2, 2.0),
+            (2.0, 2.0),
+            ('2', 2.0),
+            (None, 50.0),  # default value
+            ('', ValueError),
+            ('non-numeric-str', ValueError),
+        ],
+        ids=['int', 'float', 'str', 'none', 'empty-str', 'non-numeric-str'],
     )
-    actual_key_values = [
-        (args[0].name, args[0].value, args[0].numeric_value)
-        for args, _ in key_value_service.create.call_args_list
-    ]
-    assert actual_key_values == expected
+    def test_numeric_values(self, key_value_service, custom_model_version, value, expected_result):
+        field_name = 'MyNumericVar'
+        if expected_result == ValueError:
+            expected_err = (
+                f""Set error for field: value. Setting value: '{value}'. ""
+                ""Error: value can't be converted to float""
+            )
+            with pytest.raises(ValueError, match=expected_err):
+                NumericRuntimeVariableValue(
+                    field_name=field_name, type=RuntimeVariableType.NUMERIC, value=value
+                )
+        else:
+            user = Mock()
+            user.uid = ObjectId()
+            user.username = ""unittest@datarobot.com""
+            entity_id = ObjectId()
+
+            custom_model_version.launch_configuration = LaunchConfiguration(
+                schema=[
+                    NumericRuntimeVariableDefinition(
+                        field_name=field_name,
+                        type=RuntimeVariableType.NUMERIC,
+                        default_value=50,
+                    ),
+                ],
+                overrides=[
+                    NumericRuntimeVariableValue(
+                        field_name=field_name, type=RuntimeVariableType.NUMERIC, value=value
+                    ),
+                ],
+            )
+            export_runtime_parameters_to_key_values(
+                custom_model_version, entity_id, KeyValueEntityType.MODEL_PACKAGE, user
+            )
+            key_value_service.create.assert_called_once()
+            key_value_arg = key_value_service.create.call_args.args[0]
+            assert key_value_arg.name == field_name
+            assert key_value_arg.value == ''
+            assert key_value_arg.numeric_value == expected_result
 
 
 class TestRuntimeParametersParser:"
[ui] workbench cannot scroll the connection page to show the save button,"Reproducible on US prod and staging

In workbench -> add data -> “+ connect” to add a new connection. 

Choose databricks and fill in the top three fields (can be anything)

Choose a credential

Click “show additional parameters”

 

Expected: after filling in the catalog field, user should be able to scroll and click the save button

Actual: cannot scroll up anymore ","Refactor

Simplified the logic for expanding additional options by removing unnecessary imports and scroll-related code.
Style

Improved the layout of the connection configuration section by adjusting height and overflow properties.","diff --git a/client/js/workbench/usecase-add-data/data-connection/components/additional-options/additional-options.js b/client/js/workbench/usecase-add-data/data-connection/components/additional-options/additional-options.js
index 3724d912f6fb56..aa902d372fb348 100644
--- a/client/js/workbench/usecase-add-data/data-connection/components/additional-options/additional-options.js
+++ b/client/js/workbench/usecase-add-data/data-connection/components/additional-options/additional-options.js
@@ -1,6 +1,5 @@
-import React, { useMemo, useRef, useState } from 'react';
+import React, { useMemo, useState } from 'react';
 import PropTypes from 'prop-types';
-import debounce from 'lodash/debounce';
 import sortBy from 'lodash/sortBy';
 import { _t } from 'gettext';
 
@@ -19,7 +18,6 @@ export const AdditionalOptions = ({
   optionalState,
   isEditingDisabled,
 }) => {
-  const collapsiblePanelAnchorRef = useRef(null);
   const [expanded, setExpanded] = useState(false);
   const { fieldConfig, fields } = optionalState;
   const { addField, removeField, setFieldValue, replaceField } = storeActions;
@@ -28,17 +26,8 @@ export const AdditionalOptions = ({
     [fieldConfig]
   );
 
-  const onScrollDebounced = debounce(() => {
-    collapsiblePanelAnchorRef.current.scrollIntoView({
-      behavior: 'smooth',
-    });
-  }, 300);
-
   const onChange = () => {
     setExpanded(!expanded);
-    if (!expanded) {
-      onScrollDebounced();
-    }
   };
 
   return (
@@ -51,8 +40,6 @@ export const AdditionalOptions = ({
             accentType={ACCENT_TYPES.COMMAND}
             testId=""show-additional-options""
           >
-            <span ref={collapsiblePanelAnchorRef} />
-
             {_t('Show additional parameters')}
           </Button>
         }
diff --git a/client/js/workbench/usecase-add-data/data-connection/components/connection-configuration/connection-configuration.less b/client/js/workbench/usecase-add-data/data-connection/components/connection-configuration/connection-configuration.less
index 0208777d2852f1..1414442eea81d8 100644
--- a/client/js/workbench/usecase-add-data/data-connection/components/connection-configuration/connection-configuration.less
+++ b/client/js/workbench/usecase-add-data/data-connection/components/connection-configuration/connection-configuration.less
@@ -1,7 +1,4 @@
 .connection-configuration-wrapper {
-  height: 100%;
-  overflow: hidden;
-
   .connection-configuration-section {
     display: flex;
     flex-wrap: wrap;",,"[ui] workbench cannot scroll the connection page to show the save button

Reproducible on US prod and staging

In workbench -> add data -> “+ connect” to add a new connection. 

Choose databricks and fill in the top three fields (can be anything)

Choose a credential

Click “show additional parameters”

 

Expected: after filling in the catalog field, user should be able to scroll and click the save button

Actual: cannot scroll up anymore ","Refactor

Simplified the logic for expanding additional options by removing unnecessary imports and scroll-related code.
Style

Improved the layout of the connection configuration section by adjusting height and overflow properties.

diff --git a/client/js/workbench/usecase-add-data/data-connection/components/additional-options/additional-options.js b/client/js/workbench/usecase-add-data/data-connection/components/additional-options/additional-options.js
index 3724d912f6fb56..aa902d372fb348 100644
--- a/client/js/workbench/usecase-add-data/data-connection/components/additional-options/additional-options.js
+++ b/client/js/workbench/usecase-add-data/data-connection/components/additional-options/additional-options.js
@@ -1,6 +1,5 @@
-import React, { useMemo, useRef, useState } from 'react';
+import React, { useMemo, useState } from 'react';
 import PropTypes from 'prop-types';
-import debounce from 'lodash/debounce';
 import sortBy from 'lodash/sortBy';
 import { _t } from 'gettext';
 
@@ -19,7 +18,6 @@ export const AdditionalOptions = ({
   optionalState,
   isEditingDisabled,
 }) => {
-  const collapsiblePanelAnchorRef = useRef(null);
   const [expanded, setExpanded] = useState(false);
   const { fieldConfig, fields } = optionalState;
   const { addField, removeField, setFieldValue, replaceField } = storeActions;
@@ -28,17 +26,8 @@ export const AdditionalOptions = ({
     [fieldConfig]
   );
 
-  const onScrollDebounced = debounce(() => {
-    collapsiblePanelAnchorRef.current.scrollIntoView({
-      behavior: 'smooth',
-    });
-  }, 300);
-
   const onChange = () => {
     setExpanded(!expanded);
-    if (!expanded) {
-      onScrollDebounced();
-    }
   };
 
   return (
@@ -51,8 +40,6 @@ export const AdditionalOptions = ({
             accentType={ACCENT_TYPES.COMMAND}
             testId=""show-additional-options""
           >
-            <span ref={collapsiblePanelAnchorRef} />
-
             {_t('Show additional parameters')}
           </Button>
         }
diff --git a/client/js/workbench/usecase-add-data/data-connection/components/connection-configuration/connection-configuration.less b/client/js/workbench/usecase-add-data/data-connection/components/connection-configuration/connection-configuration.less
index 0208777d2852f1..1414442eea81d8 100644
--- a/client/js/workbench/usecase-add-data/data-connection/components/connection-configuration/connection-configuration.less
+++ b/client/js/workbench/usecase-add-data/data-connection/components/connection-configuration/connection-configuration.less
@@ -1,7 +1,4 @@
 .connection-configuration-wrapper {
-  height: 100%;
-  overflow: hidden;
-
   .connection-configuration-section {
     display: flex;
     flex-wrap: wrap;"
Model guard checks,"If someone accidently configures the input_column_name for the model guard as promptText and the actual input is also promptText, in the response phase, moderation renames the resultText as promptText and there are 2 promptText columns - not acceptable.  Verify this before making request

In the response dataframe ensure that target_name is present, if not raise the error

Handle all other exceptions","Handle all kinds of exceptions
Move column renaming code inside model guard
More test cases","diff --git a/datarobot_dome/guard_executor.py b/datarobot_dome/guard_executor.py
index d76f999..dae9409 100644
--- a/datarobot_dome/guard_executor.py
+++ b/datarobot_dome/guard_executor.py
@@ -70,17 +70,18 @@ def _map_column_names(self, guard, copy_df, stage, is_guard_executed):
                 f""Expecting column {from_column_name} in DF, but was missing. ""
                 f""Stage: {stage} guard executed: {is_guard_executed}""
             )
+        if to_column_name in copy_df.columns:
+            raise ValueError(
+                f""Column {to_column_name} already exists in DF, can't rename it""
+                f""Stage: {stage} guard executed: {is_guard_executed}""
+            )
         copy_df.rename(columns={from_column_name: to_column_name}, inplace=True)
 
     def run_guard(self, guard, copy_df, stage):
-        if guard.type == GuardType.MODEL:
-            self._map_column_names(guard, copy_df, stage, False)
         start_time = time.time()
         executor = getattr(self, self.guard_executor_map[guard.type])
         df = executor(guard, copy_df, stage)
         end_time = time.time()
-        if guard.type == GuardType.MODEL:
-            self._map_column_names(guard, df, stage, True)
 
         latency = end_time - start_time
         if isinstance(guard, OOTBGuard) and guard.ootb_type == OOTBType.TOKEN_COUNT:
@@ -142,29 +143,46 @@ def run_model_guard(self, guard, copy_df, stage):
         metric_column = guard.model_info.target_name
 
         try:
+            self._map_column_names(guard, copy_df, stage, False)
+
             result_df, response_headers = predict(
                 deployment,
                 copy_df[[guard.model_info.input_column_name]],
                 timeout=DEFAULT_GUARD_PREDICTION_TIMEOUT_IN_SEC,
             )
-        except requests.exceptions.Timeout:
-            self._logger.error(f'Timed out waiting for guard ""{guard.name}"" to predict')
+            if metric_column not in result_df.columns:
+                # This is caught anyways in the exception handling code and masked
+                raise ValueError(
+                    f""Missing output column {metric_column} in the model guard response""
+                    f""Columns obtained: {result_df.columns}""
+                )
+            # Ensure that index of result and copy dfs are same, so that concat will work
+            # correctly
+            result_df.index = copy_df.index
+            copy_df = pd.concat([copy_df, result_df[metric_column]], axis=""columns"")
+
+            copy_df, _ = self._intervene(guard, copy_df, stage, metric_column)
+            # eg. toxicity_toxic_PREDICTION should be renamed to ""Prompts_toxicity_toxic_PREDICTION""
+            # and ""Response_toxicity_toxic_PREDICTION"", if toxicity is configured for both
+            # prompts and responses
+            copy_df.rename(
+                columns={metric_column: Guard.get_stage_str(stage) + ""_"" + metric_column},
+                inplace=True,
+            )
+        except Exception as ex:
+            if isinstance(ex, requests.exceptions.Timeout):
+                self._logger.error(f'Timed out waiting for guard ""{guard.name}"" to predict')
+            else:
+                self._logger.error(f'Predictions failed for model guard ""{guard.name}"": {ex}')
+                self._logger.error(traceback.format_exc())
+            # No intervention
             copy_df = self._dont_intervene(guard, copy_df, stage)
-            return copy_df
-
-        # Ensure that index of result and copy dfs are same, so that concat will work
-        # correctly
-        result_df.index = copy_df.index
-        copy_df = pd.concat([copy_df, result_df[metric_column]], axis=""columns"")
-
-        copy_df, num_intervened = self._intervene(guard, copy_df, stage, metric_column)
-        # eg. toxicity_toxic_PREDICTION should be renamed to ""Prompts_toxicity_toxic_PREDICTION""
-        # and ""Response_toxicity_toxic_PREDICTION"", if toxicity is configured for both
-        # prompts and responses
-        copy_df.rename(
-            columns={metric_column: Guard.get_stage_str(stage) + ""_"" + metric_column},
-            inplace=True,
-        )
+            if isinstance(ex, ValueError):
+                # If it was an error in mapping the column names in the first place, we can't
+                # map them back, so return immediately
+                return copy_df
+
+        self._map_column_names(guard, copy_df, stage, True)
         return copy_df
 
     def _dont_intervene(self, guard, copy_df, stage):
diff --git a/tests/unit/test_guard_executor.py b/tests/unit/test_guard_executor.py
index 9c49500..06673ee 100644
--- a/tests/unit/test_guard_executor.py
+++ b/tests/unit/test_guard_executor.py
@@ -11,6 +11,8 @@
 from datarobot_dome.constants import GuardAction
 from datarobot_dome.constants import GuardOperatorType
 from datarobot_dome.constants import GuardStage
+from datarobot_dome.guard import Guard
+from datarobot_dome.guard_executor import DEFAULT_GUARD_PREDICTION_TIMEOUT_IN_SEC
 from datarobot_dome.guard_executor import AsyncGuardExecutor
 from datarobot_dome.guard_executor import GuardExecutor
 from tests.helpers import get_pipeline_with_guards
@@ -34,6 +36,8 @@ def test_block_intervention(self, pipeline_and_guard, copy_df, toxicity_guard_de
         pipeline, model_guard = pipeline_and_guard
         metric_column_name = model_guard.model_info.target_name
 
+        df = copy_df.copy(deep=True)
+        df.rename(columns={""text"": ""promptText""}, inplace=True)
         with mock.patch.object(dr.Deployment, ""get"") as m, mock.patch(
             ""datarobot_dome.guard_executor.predict""
         ) as n:
@@ -46,7 +50,7 @@ def test_block_intervention(self, pipeline_and_guard, copy_df, toxicity_guard_de
             )
 
             response_df = GuardExecutor(pipeline).run_model_guard(
-                model_guard, copy_df, GuardStage.PROMPT
+                model_guard, df, GuardStage.PROMPT
             )
             # ensure intervention
             assert ""blocked_promptText"" in response_df.columns
@@ -71,6 +75,8 @@ def test_report_intervention(
         model_guard = pipeline.get_postscore_guards()[0]
         metric_column_name = model_guard.model_info.target_name
 
+        df = copy_df.copy(deep=True)
+        df.rename(columns={""text"": ""resultText""}, inplace=True)
         with mock.patch.object(dr.Deployment, ""get"") as m, mock.patch(
             ""datarobot_dome.guard_executor.predict""
         ) as n:
@@ -83,7 +89,7 @@ def test_report_intervention(
             )
 
             response_df = GuardExecutor(pipeline).run_model_guard(
-                model_guard, copy_df, GuardStage.RESPONSE
+                model_guard, df, GuardStage.RESPONSE
             )
             # ensure intervention
             assert ""reported_resultText"" in response_df.columns
@@ -142,6 +148,14 @@ def test_matches_intervention(
 
 
 class TestModelGuardExecution(TestGuardExecution):
+    @pytest.fixture
+    def copy_df(self):
+        return pd.DataFrame(
+            {
+                ""promptText"": [""abc"", ""def"", ""pqr""],
+            }
+        )
+
     @pytest.mark.parametrize(""timed_out"", [True, False])
     def test_run_model_guard_timeout(
         self,
@@ -174,8 +188,136 @@ def test_run_model_guard_timeout(
             # ensure intervention for the 3rd prompt
             assert response_df[""blocked_promptText""].tolist() == expected_list
 
-    def test_run_model_guard_non_consecutive_index(self):
-        pass
+    def test_run_model_guard_exception(
+        self,
+        pipeline_and_guard,
+        copy_df,
+        toxicity_guard_deployment,
+    ):
+        pipeline, model_guard = pipeline_and_guard
+        with mock.patch.object(dr.Deployment, ""get"") as m, mock.patch(
+            ""datarobot_dome.guard_executor.predict""
+        ) as n:
+            m.return_value = toxicity_guard_deployment
+            n.side_effect = Exception(""Some Random Mocked Exception"")
+
+            response_df = GuardExecutor(pipeline).run_model_guard(
+                model_guard, copy_df, GuardStage.PROMPT
+            )
+            # ensure no intervention
+            assert response_df[""blocked_promptText""].tolist() == [False, False, False]
+
+    def test_run_model_guard_missing_metric_column(
+        self,
+        pipeline_and_guard,
+        copy_df,
+        toxicity_guard_deployment,
+    ):
+        pipeline, model_guard = pipeline_and_guard
+        with mock.patch.object(dr.Deployment, ""get"") as m, mock.patch(
+            ""datarobot_dome.guard_executor.predict""
+        ) as n:
+            m.return_value = toxicity_guard_deployment
+            n.return_value = (
+                pd.DataFrame({""blah"": [0.1, 0.2, 0.5]}),
+                mock.ANY,
+            )
+
+            response_df = GuardExecutor(pipeline).run_model_guard(
+                model_guard, copy_df, GuardStage.PROMPT
+            )
+            # ensure no intervention
+            assert response_df[""blocked_promptText""].tolist() == [False, False, False]
+
+    def test_column_name_conversions(
+        self,
+        pipeline_and_guard,
+        copy_df,
+        toxicity_guard_deployment,
+        toxicity_model_guard_config,
+    ):
+        pipeline, model_guard = pipeline_and_guard
+        metric_column_name = model_guard.model_info.target_name
+
+        copy_df_orig_col_name = copy_df.columns[0]
+        guard_input_column = model_guard.model_info.input_column_name
+        guard_stage = toxicity_model_guard_config[""stage""]
+        expected_df = copy_df.copy(deep=True)
+        expected_df.rename(columns={copy_df_orig_col_name: guard_input_column}, inplace=True)
+        with mock.patch.object(dr.Deployment, ""get"") as m, mock.patch(
+            ""datarobot_dome.guard_executor.predict""
+        ) as datarobot_predict:
+            m.return_value = toxicity_guard_deployment
+            datarobot_predict.return_value = (
+                pd.DataFrame({metric_column_name: [0.1, 0.2, 0.5]}),
+                mock.ANY,
+            )
+
+            response_df = GuardExecutor(pipeline).run_model_guard(
+                model_guard, copy_df, GuardStage.PROMPT
+            )
+            datarobot_predict.assert_called_once()
+            args, kwargs = datarobot_predict.call_args
+            # predict needs to be called with 'text' column as configured
+            pd.testing.assert_frame_equal(args[1], expected_df, check_like=True, check_dtype=False)
+            assert kwargs[""timeout""] == DEFAULT_GUARD_PREDICTION_TIMEOUT_IN_SEC
+
+            # After execution of guard, ensure names are switched back
+            assert copy_df_orig_col_name in response_df.columns
+            assert guard_input_column not in response_df.columns
+
+            # Also, we are saving the predictions in guard specific column, ensure
+            # that it is present
+            stage_metric_column_nam = Guard.get_stage_str(guard_stage) + ""_"" + metric_column_name
+            assert stage_metric_column_nam in response_df.columns
+            assert response_df[stage_metric_column_nam].tolist() == [0.1, 0.2, 0.5]
+
+    def test_run_model_guard_non_consecutive_index(
+        self,
+        pipeline_and_guard,
+        toxicity_guard_deployment,
+    ):
+        copy_df = pd.DataFrame(
+            {
+                ""promptText"": [""abc"", ""def"", ""xyz""],
+            },
+            index=[0, 1, 3],
+        )
+        pipeline, model_guard = pipeline_and_guard
+        metric_column_name = model_guard.model_info.target_name
+        with mock.patch.object(dr.Deployment, ""get"") as m, mock.patch(
+            ""datarobot_dome.guard_executor.predict""
+        ) as n:
+            m.return_value = toxicity_guard_deployment
+            n.return_value = (
+                pd.DataFrame({metric_column_name: [0.1, 0.2, 0.5]}),
+                mock.ANY,
+            )
+
+            response_df = GuardExecutor(pipeline).run_model_guard(
+                model_guard, copy_df, GuardStage.PROMPT
+            )
+            # ensure intervention
+            assert response_df[""blocked_promptText""].tolist() == [False, False, True]
+            assert response_df.index.tolist() == copy_df.index.tolist()
+
+    @pytest.mark.parametrize(
+        ""column_name"", [""blah"", ""text""], ids=[""missing_column"", ""duplicate_column""]
+    )
+    def test_conflicting_input_columns(self, pipeline_and_guard, column_name):
+        copy_df = pd.DataFrame(
+            {
+                column_name: [""abc"", ""def"", ""xyz""],
+            },
+        )
+        pipeline, model_guard = pipeline_and_guard
+        with mock.patch.object(dr.Deployment, ""get"") as m:
+            m.return_value = mock.ANY
+            response_df = GuardExecutor(pipeline).run_model_guard(
+                model_guard, copy_df, GuardStage.PROMPT
+            )
+            # ensure no intervention - no exception
+            assert response_df[""blocked_promptText""].tolist() == [False, False, False]
 
 
 class TestOOTBGuardExecution(TestGuardExecution):",,"Model guard checks

If someone accidently configures the input_column_name for the model guard as promptText and the actual input is also promptText, in the response phase, moderation renames the resultText as promptText and there are 2 promptText columns - not acceptable.  Verify this before making request

In the response dataframe ensure that target_name is present, if not raise the error

Handle all other exceptions","Handle all kinds of exceptions
Move column renaming code inside model guard
More test cases

diff --git a/datarobot_dome/guard_executor.py b/datarobot_dome/guard_executor.py
index d76f999..dae9409 100644
--- a/datarobot_dome/guard_executor.py
+++ b/datarobot_dome/guard_executor.py
@@ -70,17 +70,18 @@ def _map_column_names(self, guard, copy_df, stage, is_guard_executed):
                 f""Expecting column {from_column_name} in DF, but was missing. ""
                 f""Stage: {stage} guard executed: {is_guard_executed}""
             )
+        if to_column_name in copy_df.columns:
+            raise ValueError(
+                f""Column {to_column_name} already exists in DF, can't rename it""
+                f""Stage: {stage} guard executed: {is_guard_executed}""
+            )
         copy_df.rename(columns={from_column_name: to_column_name}, inplace=True)
 
     def run_guard(self, guard, copy_df, stage):
-        if guard.type == GuardType.MODEL:
-            self._map_column_names(guard, copy_df, stage, False)
         start_time = time.time()
         executor = getattr(self, self.guard_executor_map[guard.type])
         df = executor(guard, copy_df, stage)
         end_time = time.time()
-        if guard.type == GuardType.MODEL:
-            self._map_column_names(guard, df, stage, True)
 
         latency = end_time - start_time
         if isinstance(guard, OOTBGuard) and guard.ootb_type == OOTBType.TOKEN_COUNT:
@@ -142,29 +143,46 @@ def run_model_guard(self, guard, copy_df, stage):
         metric_column = guard.model_info.target_name
 
         try:
+            self._map_column_names(guard, copy_df, stage, False)
+
             result_df, response_headers = predict(
                 deployment,
                 copy_df[[guard.model_info.input_column_name]],
                 timeout=DEFAULT_GUARD_PREDICTION_TIMEOUT_IN_SEC,
             )
-        except requests.exceptions.Timeout:
-            self._logger.error(f'Timed out waiting for guard ""{guard.name}"" to predict')
+            if metric_column not in result_df.columns:
+                # This is caught anyways in the exception handling code and masked
+                raise ValueError(
+                    f""Missing output column {metric_column} in the model guard response""
+                    f""Columns obtained: {result_df.columns}""
+                )
+            # Ensure that index of result and copy dfs are same, so that concat will work
+            # correctly
+            result_df.index = copy_df.index
+            copy_df = pd.concat([copy_df, result_df[metric_column]], axis=""columns"")
+
+            copy_df, _ = self._intervene(guard, copy_df, stage, metric_column)
+            # eg. toxicity_toxic_PREDICTION should be renamed to ""Prompts_toxicity_toxic_PREDICTION""
+            # and ""Response_toxicity_toxic_PREDICTION"", if toxicity is configured for both
+            # prompts and responses
+            copy_df.rename(
+                columns={metric_column: Guard.get_stage_str(stage) + ""_"" + metric_column},
+                inplace=True,
+            )
+        except Exception as ex:
+            if isinstance(ex, requests.exceptions.Timeout):
+                self._logger.error(f'Timed out waiting for guard ""{guard.name}"" to predict')
+            else:
+                self._logger.error(f'Predictions failed for model guard ""{guard.name}"": {ex}')
+                self._logger.error(traceback.format_exc())
+            # No intervention
             copy_df = self._dont_intervene(guard, copy_df, stage)
-            return copy_df
-
-        # Ensure that index of result and copy dfs are same, so that concat will work
-        # correctly
-        result_df.index = copy_df.index
-        copy_df = pd.concat([copy_df, result_df[metric_column]], axis=""columns"")
-
-        copy_df, num_intervened = self._intervene(guard, copy_df, stage, metric_column)
-        # eg. toxicity_toxic_PREDICTION should be renamed to ""Prompts_toxicity_toxic_PREDICTION""
-        # and ""Response_toxicity_toxic_PREDICTION"", if toxicity is configured for both
-        # prompts and responses
-        copy_df.rename(
-            columns={metric_column: Guard.get_stage_str(stage) + ""_"" + metric_column},
-            inplace=True,
-        )
+            if isinstance(ex, ValueError):
+                # If it was an error in mapping the column names in the first place, we can't
+                # map them back, so return immediately
+                return copy_df
+
+        self._map_column_names(guard, copy_df, stage, True)
         return copy_df
 
     def _dont_intervene(self, guard, copy_df, stage):
diff --git a/tests/unit/test_guard_executor.py b/tests/unit/test_guard_executor.py
index 9c49500..06673ee 100644
--- a/tests/unit/test_guard_executor.py
+++ b/tests/unit/test_guard_executor.py
@@ -11,6 +11,8 @@
 from datarobot_dome.constants import GuardAction
 from datarobot_dome.constants import GuardOperatorType
 from datarobot_dome.constants import GuardStage
+from datarobot_dome.guard import Guard
+from datarobot_dome.guard_executor import DEFAULT_GUARD_PREDICTION_TIMEOUT_IN_SEC
 from datarobot_dome.guard_executor import AsyncGuardExecutor
 from datarobot_dome.guard_executor import GuardExecutor
 from tests.helpers import get_pipeline_with_guards
@@ -34,6 +36,8 @@ def test_block_intervention(self, pipeline_and_guard, copy_df, toxicity_guard_de
         pipeline, model_guard = pipeline_and_guard
         metric_column_name = model_guard.model_info.target_name
 
+        df = copy_df.copy(deep=True)
+        df.rename(columns={""text"": ""promptText""}, inplace=True)
         with mock.patch.object(dr.Deployment, ""get"") as m, mock.patch(
             ""datarobot_dome.guard_executor.predict""
         ) as n:
@@ -46,7 +50,7 @@ def test_block_intervention(self, pipeline_and_guard, copy_df, toxicity_guard_de
             )
 
             response_df = GuardExecutor(pipeline).run_model_guard(
-                model_guard, copy_df, GuardStage.PROMPT
+                model_guard, df, GuardStage.PROMPT
             )
             # ensure intervention
             assert ""blocked_promptText"" in response_df.columns
@@ -71,6 +75,8 @@ def test_report_intervention(
         model_guard = pipeline.get_postscore_guards()[0]
         metric_column_name = model_guard.model_info.target_name
 
+        df = copy_df.copy(deep=True)
+        df.rename(columns={""text"": ""resultText""}, inplace=True)
         with mock.patch.object(dr.Deployment, ""get"") as m, mock.patch(
             ""datarobot_dome.guard_executor.predict""
         ) as n:
@@ -83,7 +89,7 @@ def test_report_intervention(
             )
 
             response_df = GuardExecutor(pipeline).run_model_guard(
-                model_guard, copy_df, GuardStage.RESPONSE
+                model_guard, df, GuardStage.RESPONSE
             )
             # ensure intervention
             assert ""reported_resultText"" in response_df.columns
@@ -142,6 +148,14 @@ def test_matches_intervention(
 
 
 class TestModelGuardExecution(TestGuardExecution):
+    @pytest.fixture
+    def copy_df(self):
+        return pd.DataFrame(
+            {
+                ""promptText"": [""abc"", ""def"", ""pqr""],
+            }
+        )
+
     @pytest.mark.parametrize(""timed_out"", [True, False])
     def test_run_model_guard_timeout(
         self,
@@ -174,8 +188,136 @@ def test_run_model_guard_timeout(
             # ensure intervention for the 3rd prompt
             assert response_df[""blocked_promptText""].tolist() == expected_list
 
-    def test_run_model_guard_non_consecutive_index(self):
-        pass
+    def test_run_model_guard_exception(
+        self,
+        pipeline_and_guard,
+        copy_df,
+        toxicity_guard_deployment,
+    ):
+        pipeline, model_guard = pipeline_and_guard
+        with mock.patch.object(dr.Deployment, ""get"") as m, mock.patch(
+            ""datarobot_dome.guard_executor.predict""
+        ) as n:
+            m.return_value = toxicity_guard_deployment
+            n.side_effect = Exception(""Some Random Mocked Exception"")
+
+            response_df = GuardExecutor(pipeline).run_model_guard(
+                model_guard, copy_df, GuardStage.PROMPT
+            )
+            # ensure no intervention
+            assert response_df[""blocked_promptText""].tolist() == [False, False, False]
+
+    def test_run_model_guard_missing_metric_column(
+        self,
+        pipeline_and_guard,
+        copy_df,
+        toxicity_guard_deployment,
+    ):
+        pipeline, model_guard = pipeline_and_guard
+        with mock.patch.object(dr.Deployment, ""get"") as m, mock.patch(
+            ""datarobot_dome.guard_executor.predict""
+        ) as n:
+            m.return_value = toxicity_guard_deployment
+            n.return_value = (
+                pd.DataFrame({""blah"": [0.1, 0.2, 0.5]}),
+                mock.ANY,
+            )
+
+            response_df = GuardExecutor(pipeline).run_model_guard(
+                model_guard, copy_df, GuardStage.PROMPT
+            )
+            # ensure no intervention
+            assert response_df[""blocked_promptText""].tolist() == [False, False, False]
+
+    def test_column_name_conversions(
+        self,
+        pipeline_and_guard,
+        copy_df,
+        toxicity_guard_deployment,
+        toxicity_model_guard_config,
+    ):
+        pipeline, model_guard = pipeline_and_guard
+        metric_column_name = model_guard.model_info.target_name
+
+        copy_df_orig_col_name = copy_df.columns[0]
+        guard_input_column = model_guard.model_info.input_column_name
+        guard_stage = toxicity_model_guard_config[""stage""]
+        expected_df = copy_df.copy(deep=True)
+        expected_df.rename(columns={copy_df_orig_col_name: guard_input_column}, inplace=True)
+        with mock.patch.object(dr.Deployment, ""get"") as m, mock.patch(
+            ""datarobot_dome.guard_executor.predict""
+        ) as datarobot_predict:
+            m.return_value = toxicity_guard_deployment
+            datarobot_predict.return_value = (
+                pd.DataFrame({metric_column_name: [0.1, 0.2, 0.5]}),
+                mock.ANY,
+            )
+
+            response_df = GuardExecutor(pipeline).run_model_guard(
+                model_guard, copy_df, GuardStage.PROMPT
+            )
+            datarobot_predict.assert_called_once()
+            args, kwargs = datarobot_predict.call_args
+            # predict needs to be called with 'text' column as configured
+            pd.testing.assert_frame_equal(args[1], expected_df, check_like=True, check_dtype=False)
+            assert kwargs[""timeout""] == DEFAULT_GUARD_PREDICTION_TIMEOUT_IN_SEC
+
+            # After execution of guard, ensure names are switched back
+            assert copy_df_orig_col_name in response_df.columns
+            assert guard_input_column not in response_df.columns
+
+            # Also, we are saving the predictions in guard specific column, ensure
+            # that it is present
+            stage_metric_column_nam = Guard.get_stage_str(guard_stage) + ""_"" + metric_column_name
+            assert stage_metric_column_nam in response_df.columns
+            assert response_df[stage_metric_column_nam].tolist() == [0.1, 0.2, 0.5]
+
+    def test_run_model_guard_non_consecutive_index(
+        self,
+        pipeline_and_guard,
+        toxicity_guard_deployment,
+    ):
+        copy_df = pd.DataFrame(
+            {
+                ""promptText"": [""abc"", ""def"", ""xyz""],
+            },
+            index=[0, 1, 3],
+        )
+        pipeline, model_guard = pipeline_and_guard
+        metric_column_name = model_guard.model_info.target_name
+        with mock.patch.object(dr.Deployment, ""get"") as m, mock.patch(
+            ""datarobot_dome.guard_executor.predict""
+        ) as n:
+            m.return_value = toxicity_guard_deployment
+            n.return_value = (
+                pd.DataFrame({metric_column_name: [0.1, 0.2, 0.5]}),
+                mock.ANY,
+            )
+
+            response_df = GuardExecutor(pipeline).run_model_guard(
+                model_guard, copy_df, GuardStage.PROMPT
+            )
+            # ensure intervention
+            assert response_df[""blocked_promptText""].tolist() == [False, False, True]
+            assert response_df.index.tolist() == copy_df.index.tolist()
+
+    @pytest.mark.parametrize(
+        ""column_name"", [""blah"", ""text""], ids=[""missing_column"", ""duplicate_column""]
+    )
+    def test_conflicting_input_columns(self, pipeline_and_guard, column_name):
+        copy_df = pd.DataFrame(
+            {
+                column_name: [""abc"", ""def"", ""xyz""],
+            },
+        )
+        pipeline, model_guard = pipeline_and_guard
+        with mock.patch.object(dr.Deployment, ""get"") as m:
+            m.return_value = mock.ANY
+            response_df = GuardExecutor(pipeline).run_model_guard(
+                model_guard, copy_df, GuardStage.PROMPT
+            )
+            # ensure no intervention - no exception
+            assert response_df[""blocked_promptText""].tolist() == [False, False, False]
 
 
 class TestOOTBGuardExecution(TestGuardExecution):"
Making edits to Custom Model resets the value of the Resource Bundle,"Not sure if the bug is in the FE or BE but when making edits to the ENV or to a runtime param value, the Resource Bundle goes back to some default value.","Don't set default bundle so early

Remove stuff that was never implemented

Add ordering to resource bundles

Sometimes it is useful to sort them based on resources.

Fix how we set default resource bundle

Before we were setting the default too early before we check prior saved version.

Remove defunct test

Fix lint

Mock out ff in tests

Add skeleton for tests of bundle service

Add tests for service abstraction

Update tests for bundle service

Cleanup resource bundle API tests

The tests were relying on too much deep knowledge of how service abstraction worked. That moved into service tests so now these tests can mock service object

Add helper to translate maximumMemory to resource bundle

Add tests for deducing resource bundle
","diff --git a/public_api/custom_model/common.py b/public_api/custom_model/common.py
index 782fb6b8d516ba..428382c387e993 100644
--- a/public_api/custom_model/common.py
+++ b/public_api/custom_model/common.py
@@ -36,7 +36,6 @@
 from common.services.custom_models.exceptions import CustomModelVersionFrozenUpdate
 from common.services.data_management.catalog_service import get_catalog_and_version
 from common.services.execution_environment import ExecutionEnvironmentService
-from common.services.feature_toggling import is_feature_enabled
 from common.services.flippers import GlobalFlipper
 from common.services.model_deployment import ModelDeploymentService
 from common.services.organization import OrganizationService
@@ -215,7 +214,6 @@ def get_custom_model_user_configured_resources(uid, data):
     validate_memory_resources_values(
         uid, persistent, resources.get('desired_memory'), resources.get('maximum_memory')
     )
-    set_default_resource_bundle_id(uid, resources)
 
     validate_replicas_resource_value(uid, persistent, resources.get('replicas'))
     remove_disabled_custom_inference_model_resource_attributes(resources)
@@ -291,24 +289,6 @@ def validate_memory_resources_values(uid, persistent, desired_memory, maximum_me
         raise CustomModelInvalidParameter(gettext(msg))
 
 
-def set_default_resource_bundle_id(uid, resources):
-    """"""
-    Sets default resource bundle id if no other resource information is provided
-
-    :param uid:  ObjectId, user id
-    :param resources:  dict
-    """"""
-    maximum_memory = resources.get('maximum_memory')
-    resource_bundle_id = resources.get('resource_bundle_id')
-    # Order of checks is intentional here to avoid needless DB query for common case.
-    if (
-        maximum_memory is None
-        and resource_bundle_id is None
-        and is_feature_enabled(uid, 'MLOPS_RESOURCE_REQUEST_BUNDLES')
-    ):
-        resources['resource_bundle_id'] = EngConfig['CUSTOM_MODEL_DEFAULT_RESOURCE_BUNDLE_ID']
-
-
 def user_org(uid, persistent):
     user_service = UserServiceBase(persistent=persistent)
     org_id = user_service.get_org_id(uid)
diff --git a/raptor_lake/custom_model/services/custom_task_version_service.py b/raptor_lake/custom_model/services/custom_task_version_service.py
index cc797277de3a92..a075dccb875c3e 100644
--- a/raptor_lake/custom_model/services/custom_task_version_service.py
+++ b/raptor_lake/custom_model/services/custom_task_version_service.py
@@ -112,6 +112,8 @@
 from raptor_lake.dependencies.exceptions import CustomDependencySupportError
 from raptor_lake.dependencies.services import BaseDependencyParserService
 from raptor_lake.execution_environment.services import ExecutionEnvironmentVersionService
+from raptor_lake.resource_request_bundles.entities import ResourceRequestBundleUseCases
+from raptor_lake.resource_request_bundles.helpers import convert_maximum_memory_to_resource_bundle
 from raptor_lake.workspace.enums import SourceType
 from raptor_lake.workspace.services import WorkspaceItemService
 from raptor_lake.workspace.services import WorkspaceService
@@ -253,6 +255,54 @@ def _get_next_version(
         )
         return version_bumped
 
+    def _deduce_network_egress_policy(self, value_from_baseline):
+        """"""
+        Deduce the network egress policy based on the value from the baseline model
+        """"""
+        if is_feature_enabled(self.user_id, 'PUBLIC_NETWORK_ACCESS_FOR_ALL_CUSTOM_MODELS'):
+            if value_from_baseline is None:
+                return NetworkEgressPolicy.public.value
+        else:
+            if value_from_baseline in [
+                NetworkEgressPolicy.dr_api_access.value,
+                NetworkEgressPolicy.public.value,
+            ]:
+                return NetworkEgressPolicy.none.value
+        return value_from_baseline
+
+    def _deduce_resource_bundle(self, value_from_baseline, baseline_model, user_resources):
+        """"""
+        Deduce the resource bundle based on the value from the baseline model
+        """"""
+        if not is_feature_enabled(self.user_id, 'MLOPS_RESOURCE_REQUEST_BUNDLES'):
+            return None
+
+        # Order matters here: if the FF was off then we should NOT copy from baseline because that
+        # would be hidden from the user (as the UI wouldn't even show a resource bundle is
+        # associated with the model).
+        if value_from_baseline is not None:
+            return value_from_baseline
+
+        # Try and deduce the bundle from either the current or previous memory settings.
+        if max_memory := user_resources.get(
+            ""maximum_memory"", getattr(baseline_model, ""maximum_memory"")
+        ):
+            try:
+                bundle = convert_maximum_memory_to_resource_bundle(
+                    self.user_id, max_memory, use_cases=ResourceRequestBundleUseCases.CUSTOM_MODEL
+                )
+                return bundle.id
+            except ValueError:
+                raise UnprocessableRequestError(
+                    gettext(
+                        ""Failed to deduce resource bundle from maximum memory setting. Please try""
+                        "" again by providing a valid `resource_bundle_id`.""
+                    )
+                )
+        else:
+            # Fall back to global default if all else fails
+            return EngConfig['CUSTOM_MODEL_DEFAULT_RESOURCE_BUNDLE_ID']
+
     def _deduce_resources_if_not_provided(self, custom_task, current_version, user_resources):
         """"""
         Returns the proper custom task's resources, which depends on user provided, previous
@@ -264,17 +314,11 @@ def _deduce_resources_if_not_provided(self, custom_task, current_version, user_r
             if user_resources.get(resource) is None:
                 value_from_baseline = getattr(baseline_model, resource)
                 if resource == 'network_egress_policy' and custom_task.is_inference:
-                    if is_feature_enabled(
-                        self.user_id, 'PUBLIC_NETWORK_ACCESS_FOR_ALL_CUSTOM_MODELS'
-                    ):
-                        if value_from_baseline is None:
-                            value_from_baseline = NetworkEgressPolicy.public.value
-                    else:
-                        if value_from_baseline in [
-                            NetworkEgressPolicy.dr_api_access.value,
-                            NetworkEgressPolicy.public.value,
-                        ]:
-                            value_from_baseline = NetworkEgressPolicy.none.value
+                    value_from_baseline = self._deduce_network_egress_policy(value_from_baseline)
+                elif resource == ""resource_bundle_id"" and custom_task.is_inference:
+                    value_from_baseline = self._deduce_resource_bundle(
+                        value_from_baseline, baseline_model, user_resources
+                    )
                 user_resources[resource] = value_from_baseline
 
         return user_resources
diff --git a/raptor_lake/resource_request_bundles/entities.py b/raptor_lake/resource_request_bundles/entities.py
index 5ae879d4402681..22ad00f0cac65a 100644
--- a/raptor_lake/resource_request_bundles/entities.py
+++ b/raptor_lake/resource_request_bundles/entities.py
@@ -72,3 +72,10 @@ class ResourceRequestBundle(MarkDeletedDtoMixin, Dto):
     @property
     def has_gpu(self):
         return self.gpu_count > 0
+
+    def __lt__(self, other):
+        return (self.gpu_count, self.memory_bytes, self.cpu_count) < (
+            other.gpu_count,
+            other.memory_bytes,
+            other.cpu_count,
+        )
diff --git a/raptor_lake/resource_request_bundles/helpers.py b/raptor_lake/resource_request_bundles/helpers.py
new file mode 100644
index 00000000000000..bddc046ed41778
--- /dev/null
+++ b/raptor_lake/resource_request_bundles/helpers.py
@@ -0,0 +1,32 @@
+#
+# Copyright 2024 DataRobot, Inc. and its affiliates.
+#
+# All rights reserved.
+#
+# DataRobot, Inc. Confidential.
+#
+# This is unpublished proprietary source code of DataRobot, Inc.
+# and its affiliates.
+#
+# The copyright notice above does not evidence any actual or intended
+# publication of such source code.
+from typing import Optional
+
+from bson import ObjectId
+
+from raptor_lake.resource_request_bundles.entities import ResourceRequestBundle
+from raptor_lake.resource_request_bundles.entities import ResourceRequestBundleUseCases
+from raptor_lake.resource_request_bundles.services import ResourceRequestBundleService
+
+
+def convert_maximum_memory_to_resource_bundle(
+    user_id: ObjectId,
+    maximum_memory_bytes: int,
+    use_cases: Optional[ResourceRequestBundleUseCases] = None,
+) -> ResourceRequestBundle:
+    bundles = ResourceRequestBundleService(user_id=user_id).list(use_cases=use_cases)
+    bundles.sort()
+    for bundle in bundles:
+        if maximum_memory_bytes <= bundle.memory_bytes:
+            return bundle
+    raise ValueError(f""No resource bundle found for maximum memory {maximum_memory_bytes} bytes"")
diff --git a/raptor_lake/resource_request_bundles/services.py b/raptor_lake/resource_request_bundles/services.py
index 87b10963101961..6d01b0285b483d 100644
--- a/raptor_lake/resource_request_bundles/services.py
+++ b/raptor_lake/resource_request_bundles/services.py
@@ -11,16 +11,12 @@
 # The copyright notice above does not evidence any actual or intended
 # publication of such source code.
 from itertools import chain
-from typing import Optional
 
-import drdbs
 from bson import ObjectId
 
 from common.exceptions_core_backend import PermissionsError
 from common.exceptions_core_backend import ResourceNotFoundError
 from common.services.feature_toggling import is_feature_enabled
-from common.wrappers import database
-from config.dynamic_config import DynamicConfig
 from config.engine import EngConfig
 from dr_libs.utilities.object_id import to_object_id
 
@@ -30,11 +26,8 @@
 class ResourceRequestBundleService:
     dto_class = ResourceRequestBundle
 
-    def __init__(self, user_id: ObjectId, persistent: Optional[drdbs.mongo_db.MongoDB] = None):
+    def __init__(self, user_id: ObjectId, persistent=None):  # pylint: disable=unused-argument
         self.user_id = to_object_id(user_id)
-        self.persistent = persistent or database.new_persistent()
-        # TODO: implement dyconfig to allow dynamically overriding static values
-        self.dyconfig = DynamicConfig(persistent=self.persistent).from_mongo()
 
         self._dto_factory = self.dto_class.from_storage
         self._lookup_by_id = {
diff --git a/tests/backend/unit/public_api/custom_models/test_custom_model_version_api.py b/tests/backend/unit/public_api/custom_models/test_custom_model_version_api.py
index 03a5909240fd90..7ab5cf960c0f7b 100644
--- a/tests/backend/unit/public_api/custom_models/test_custom_model_version_api.py
+++ b/tests/backend/unit/public_api/custom_models/test_custom_model_version_api.py
@@ -693,39 +693,6 @@ def test_model_version_create_with_resource_bundle(
         user_resources = kwargs[""user_resources""]
         assert user_resources.get(""resource_bundle_id"") == str(resource_bundle_id)
 
-    @pytest.mark.parametrize(""resource_bundle_enabled"", [False, True])
-    def test_model_version_create_default_resource_bundle(
-        self,
-        client,
-        mock_create_custom_task_version,
-        resource_bundle_enabled,
-    ):
-        base_environment_id = ObjectId()
-        custom_task_id = ObjectId()
-        resource_bundle_id = str(ObjectId())
-
-        payload = {'baseEnvironmentId': base_environment_id}
-
-        with mock.patch.dict(
-            EngConfig, {""CUSTOM_MODEL_DEFAULT_RESOURCE_BUNDLE_ID"": resource_bundle_id}
-        ), patch_feature_flag('MLOPS_RESOURCE_REQUEST_BUNDLES', resource_bundle_enabled):
-            response = client.post(
-                self.url_template.format(custom_task_id),
-                data=payload,
-                headers=self.headers,
-            )
-
-        assert response.status_code == 201, response.data
-        mock_create_custom_task_version.assert_called_once()
-        kwargs = mock_create_custom_task_version.call_args[1]
-
-        assert ""user_resources"" in kwargs
-        user_resources = kwargs[""user_resources""]
-        if resource_bundle_enabled:
-            assert user_resources.get(""resource_bundle_id"") == resource_bundle_id
-        else:
-            assert user_resources.get(""resource_bundle_id"") == None
-
     @patch_feature_flag('MLOPS_RESOURCE_REQUEST_BUNDLES', True)
     def test_model_version_create_with_resource_bundle_and_max_mem(
         self,
diff --git a/tests/backend/unit/public_api/mlops/test_resource_request_bundles.py b/tests/backend/unit/public_api/mlops/test_resource_request_bundles.py
index 74999a5f3734d5..08f1032b7b5eb9 100644
--- a/tests/backend/unit/public_api/mlops/test_resource_request_bundles.py
+++ b/tests/backend/unit/public_api/mlops/test_resource_request_bundles.py
@@ -15,74 +15,14 @@
 
 import pytest
 
+from common.exceptions_core_backend import ResourceNotFoundError
 from config.engine import EngConfig
+from raptor_lake.resource_request_bundles.entities import ResourceRequestBundle
+from raptor_lake.resource_request_bundles.services import ResourceRequestBundleService
 from tests.common.utils import patch_feature_flag
 
 logger = logging.getLogger(__name__)
 
-FAKE_LRS_GPU_CONTAINER_SIZES = [
-    {
-        ""id"": ""gpu.small"",
-        ""name"": ""GPU Small"",
-        ""description"": ""1 x NVIDIA T4 | 16GB VRAM | 4 CPU | 16GB RAM"",
-        ""gpu_maker"": ""nvidia"",
-        ""gpu_type_label"": ""nvidia-t4-x"",
-        ""gpu_count"": 1,
-        ""cpu_count"": 4,
-        ""memory_mb"": 16000,
-        ""use_cases"": [""customModel"", ""customJob""],
-    },
-    {
-        # Have a silly ID that would never be in the actual list
-        ""id"": ""gpu.humongous"",
-        ""name"": ""GPU Large"",
-        ""description"": ""1 x NVIDIA A10 | 24GB VRAM | 8 CPU | 32GB RAM"",
-        ""gpu_maker"": ""nvidia"",
-        ""gpu_type_label"": ""nvidia-a10g-2x"",
-        ""gpu_count"": 1,
-        ""cpu_count"": 8,
-        ""memory_mb"": 32000,
-        ""use_cases"": [""customModel""],
-    },
-]
-
-FAKE_LRS_CPU_CONTAINER_SIZES = [
-    {
-        ""id"": ""cpu_default"",
-        ""name"": ""CPU Other"",
-        ""description"": ""Try out CPU only bundles"",
-        ""cpu_count"": 1,
-        ""memory_mb"": 512,
-        ""use_cases"": [""customModel""],
-    },
-    {
-        ""id"": ""cpu_old"",
-        ""name"": ""CPU Deprecated"",
-        ""description"": ""Bundle that we don't want used anymore"",
-        ""cpu_count"": 1,
-        ""memory_mb"": 512,
-        ""use_cases"": [""customModel""],
-        # Test that we don't list deleted bundles
-        ""is_deleted"": True,
-    },
-]
-
-
-@pytest.fixture(autouse=True)
-def mock_lrs_gpu_container_sizes(request):
-    if ""useactualbundles"" in request.keywords:
-        yield
-        return
-    # Override actual bundle list so these tests don't need to change when the actual list changes
-    with mock.patch.dict(
-        EngConfig,
-        {
-            ""LRS_GPU_CONTAINER_SIZES"": FAKE_LRS_GPU_CONTAINER_SIZES,
-            ""LRS_CPU_CONTAINER_SIZES"": FAKE_LRS_CPU_CONTAINER_SIZES,
-        },
-    ):
-        yield
-
 
 @pytest.fixture(autouse=True)
 def enable_resource_bundle_ff():
@@ -96,7 +36,52 @@ def enable_lrs_gpu_support():
         yield
 
 
-@pytest.mark.usefixtures(""mock_private_feature_flag"")
+@pytest.fixture
+def cpu_bundle():
+    return ResourceRequestBundle(
+        id=""cpu_default"",
+        name=""CPU Other"",
+        description=""Try out CPU only bundles"",
+        cpu_count=1,
+        memory_bytes=512 * 1024 * 1024,
+        use_cases=[""customModel""],
+    )
+
+
+@pytest.fixture
+def gpu_bundle():
+    return ResourceRequestBundle(
+        id=""gpu.small"",
+        name=""GPU Small"",
+        description=""1 x NVIDIA T4 | 16GB VRAM | 4 CPU | 16GB RAM"",
+        cpu_count=4,
+        memory_bytes=16777216000,
+        gpu_maker=""nvidia"",
+        gpu_count=1,
+        use_cases=[""customModel"", ""customJob""],
+    )
+
+
+@pytest.fixture
+def deleted_bundle():
+    return ResourceRequestBundle(
+        id=""cpu_old"",
+        name=""CPU Deprecated"",
+        description=""Bundle that we don't want used anymore"",
+        cpu_count=1,
+        memory_bytes=536870912,
+        use_cases=[""customModel""],
+        is_deleted=True,
+    )
+
+
+@pytest.fixture(autouse=True)
+def toggle_gpu_ff(request):
+    flag_value = getattr(request, ""param"", False)
+    with patch_feature_flag(""CUSTOM_MODEL_GPU_INFERENCE"", flag_value):
+        yield flag_value
+
+
 class TestResourceRequestBundleList:
     """"""
     Test listing Resource Request Bundles
@@ -104,44 +89,31 @@ class TestResourceRequestBundleList:
 
     BASE_URL = ""/mlops/compute/bundles/""
 
-    @pytest.mark.parametrize(
-        ""gpu_ff,expected_total"", [(True, 3), (False, 1)], ids=[""gpu_enabled"", ""gpu_disabled""]
-    )
-    def test_no_filter(self, client, gpu_ff, expected_total):
-        with patch_feature_flag(""CUSTOM_MODEL_GPU_INFERENCE"", gpu_ff):
-            response = client.get(self.BASE_URL)
+    @pytest.fixture(autouse=True)
+    def mock_bundle_svc_list(self, cpu_bundle):
+        with mock.patch.object(ResourceRequestBundleService, ""list"") as mocker:
+            mocker.return_value = [cpu_bundle]
+            yield mocker
+
+    def test_no_filter(self, client, mock_bundle_svc_list):
+        response = client.get(self.BASE_URL)
+        mock_bundle_svc_list.assert_called_once_with()
         assert response.status_code == 200
-        assert len(response.json[""data""]) == expected_total
+        assert len(response.json[""data""]) == 1
+        assert response.json[""data""][0][""id""] == ""cpu_default""
 
-    @pytest.mark.parametrize(
-        ""gpu_ff,expected_total"", [(True, 1), (False, 0)], ids=[""gpu_enabled"", ""gpu_disabled""]
-    )
-    def test_with_filter(self, client, gpu_ff, expected_total):
-        with patch_feature_flag(""CUSTOM_MODEL_GPU_INFERENCE"", gpu_ff):
-            response = client.get(self.BASE_URL, query_string={""useCases"": ""customJob""})
+    def test_with_filter(self, client, mock_bundle_svc_list):
+        response = client.get(self.BASE_URL, query_string={""useCases"": ""customJob""})
+        mock_bundle_svc_list.assert_called_once_with(use_cases=""customJob"")
         assert response.status_code == 200
-        assert len(response.json[""data""]) == expected_total
+        assert ""data"" in response.json
 
     def test_with_invalid_filter(self, client):
         response = client.get(self.BASE_URL, query_string={""useCases"": ""CustomSomething""})
         assert response.status_code == 400
         assert ""does not match allowed values"" in response.json[""message""][""useCases""]
 
-    @pytest.mark.useactualbundles
-    @patch_feature_flag(""CUSTOM_MODEL_GPU_INFERENCE"", True)
-    def test_actual_bundle_list(self, client):
-        """"""Test with the actual bundle list to make sure it parses our DTOs""""""
-        response = client.get(self.BASE_URL)
-        assert response.status_code == 200
-        # Sanity check that we aren't using the fake list
-        assert ""gpu.humongous"" not in response.text, ""Test looks like it is using fake bundle list""
-        n_cpu_bundles = len(EngConfig[""LRS_CPU_CONTAINER_SIZES""])
-        n_gpu_bundles = len(EngConfig[""LRS_GPU_CONTAINER_SIZES""])
-        logger.info(""Found %d CPU bundles and %d GPU bundles"", n_cpu_bundles, n_gpu_bundles)
-        assert len(response.json[""data""]) == n_cpu_bundles + n_gpu_bundles
-
 
-@pytest.mark.usefixtures(""mock_private_feature_flag"")
 class TestResourceRequestBundleRetrieve:
     """"""
     Test listing Resource Request Bundles
@@ -149,17 +121,24 @@ class TestResourceRequestBundleRetrieve:
 
     BASE_URL = ""/mlops/compute/bundles""
 
-    @patch_feature_flag(""CUSTOM_MODEL_GPU_INFERENCE"", False)
-    def test_not_found(self, client):
+    @pytest.fixture(autouse=True)
+    def mock_bundle_svc_get(self, gpu_bundle):
+        with mock.patch.object(ResourceRequestBundleService, ""get_object"") as mocker:
+            mocker.return_value = gpu_bundle
+            yield mocker
+
+    def test_not_found(self, client, mock_bundle_svc_get):
+        mock_bundle_svc_get.side_effect = ResourceNotFoundError(""'does_not_exist' not found"")
         response = client.get(f""{self.BASE_URL}/does_not_exist/"")
         assert response.status_code == 404
         assert ""not found"" in response.json[""message""]
 
-    @pytest.mark.parametrize(""gpu_ff"", [True, False], ids=[""gpu_enabled"", ""gpu_disabled""])
-    def test_get(self, client, gpu_ff):
-        with patch_feature_flag(""CUSTOM_MODEL_GPU_INFERENCE"", gpu_ff):
-            response = client.get(f""{self.BASE_URL}/gpu.small/"")
-        if gpu_ff:
+    @pytest.mark.parametrize(
+        ""toggle_gpu_ff"", [True, False], ids=[""gpu_enabled"", ""gpu_disabled""], indirect=True
+    )
+    def test_get(self, client, toggle_gpu_ff):
+        response = client.get(f""{self.BASE_URL}/gpu.small/"")
+        if toggle_gpu_ff:
             assert response.status_code == 200
             assert response.json == {
                 ""id"": ""gpu.small"",
@@ -174,28 +153,13 @@ def test_get(self, client, gpu_ff):
             }
         else:
             assert response.status_code == 403
-            assert ""not enabled"" in response.json[""message""]
-
-    @patch_feature_flag(""CUSTOM_MODEL_GPU_INFERENCE"", False)
-    def test_get_cpu_resource(self, client):
-        response = client.get(f""{self.BASE_URL}/cpu_default/"")
-        assert response.status_code == 200
-        assert response.json == {
-            ""id"": ""cpu_default"",
-            ""name"": ""CPU Other"",
-            ""description"": ""Try out CPU only bundles"",
-            ""cpuCount"": 1,
-            ""memoryBytes"": 536870912,
-            ""gpuMaker"": None,
-            ""gpuCount"": 0,
-            ""useCases"": [""customModel""],
-            ""hasGpu"": False,
-        }
+            assert ""CUSTOM_MODEL_GPU_INFERENCE is not enabled"" in response.json[""message""]
 
-    @patch_feature_flag(""CUSTOM_MODEL_GPU_INFERENCE"", False)
-    def test_get_deleted(self, client):
+    def test_get_deleted(self, client, mock_bundle_svc_get, deleted_bundle):
         """"""You can look up an archived bundle by ID""""""
+        mock_bundle_svc_get.return_value = deleted_bundle
         response = client.get(f""{self.BASE_URL}/cpu_old/"")
+        mock_bundle_svc_get.assert_called_once_with(""cpu_old"", show_deleted=True)
         assert response.status_code == 200
         assert response.json == {
             ""id"": ""cpu_old"",
diff --git a/tests/backend/unit/raptor_lake/custom_model/test_custom_model_version_service.py b/tests/backend/unit/raptor_lake/custom_model/test_custom_model_version_service.py
index ffe9a32913bb5c..dd24cb41e2e902 100644
--- a/tests/backend/unit/raptor_lake/custom_model/test_custom_model_version_service.py
+++ b/tests/backend/unit/raptor_lake/custom_model/test_custom_model_version_service.py
@@ -32,6 +32,7 @@
 from common.entities.execution_environment import ExecutionEnvironment
 from common.enum_execution_environment import ExecutionEnvironmentProgrammingLanguage
 from common.services.execution_environment import ExecutionEnvironmentService
+from config.engine import EngConfig
 from execute.image_builder.services import BaseDockerImageService
 from raptor_lake.custom_model.entities import CustomTaskVersion
 from raptor_lake.custom_model.exceptions import CustomModelImageInvalidVersion
@@ -99,6 +100,21 @@ def mock_get_lpr_by_environment_and_package_version():
         yield mock_func
 
 
+@pytest.fixture()
+def toggle_resource_bundle_ff(request):
+    flag_value = getattr(request, ""param"", False)
+    with patch_feature_flag('MLOPS_RESOURCE_REQUEST_BUNDLES', flag_value):
+        yield flag_value
+
+
+@pytest.fixture()
+def mock_gpu_inference_ff():
+    with patch_feature_flag('CUSTOM_MODEL_GPU_INFERENCE', False), patch.dict(
+        EngConfig, {'LRS_GPU_ENABLED': False}
+    ):
+        yield
+
+
 # copied from tests/backend/unit/public_api/custom_models/conftest.py
 
 
@@ -667,6 +683,7 @@ def test_parse_file_paths():
         assert parsed.workspace_id == workspace_id
 
 
+@pytest.mark.usefixtures('toggle_resource_bundle_ff', 'mock_gpu_inference_ff')
 class TestResourcesMigration:
     """"""
     Contains unit-tests for resources migration from custom model, custom model version subjected
@@ -876,6 +893,109 @@ def test_network_egress_policy_migration_from_custom_model_version_subjected_to_
             assert result_resources == expected_resources
 
 
+@pytest.mark.parametrize(
+    ""toggle_resource_bundle_ff"",
+    [True, False],
+    ids=[""enable_resource_bundles"", ""disable_resource_bundles""],
+    indirect=True,
+)
+@pytest.mark.usefixtures('mock_gpu_inference_ff')
+class TestWithResourceBundle:
+    """"""
+    Contains unit-tests dealing with transition to using resource-bundles instead of memory/cpu
+    """"""
+
+    @pytest.fixture(autouse=True)
+    def mock_public_network_access_ff(self):
+        with patch_feature_flag('PUBLIC_NETWORK_ACCESS_FOR_ALL_CUSTOM_MODELS', False):
+            yield
+
+    @pytest.fixture
+    def custom_model(self, user_id):
+        custom_model = CustomTaskDto(
+            user_id=user_id,
+            workspace_id=ObjectId(),
+            name='model',
+            target_options=CustomModelTargetOptions(target_type=CustomModelTargetType.REGRESSION),
+            custom_model_type=CustomModelType.CUSTOM_INFERENCE,
+            is_time_series=False,
+        )
+        return custom_model
+
+    def test_resource_bundle_provided_by_user(
+        self,
+        toggle_resource_bundle_ff,
+        custom_task_version_service,
+        custom_model,
+        custom_model_version,
+    ):
+        if not toggle_resource_bundle_ff:
+            # This testcase doesn't make sense when resource bundles are disabled
+            pytest.skip(""Resource bundles are disabled"")
+        result_resources = custom_task_version_service._deduce_resources_if_not_provided(
+            custom_model,
+            custom_model_version,
+            user_resources={""resource_bundle_id"": ""cpu.something""},
+        )
+        assert result_resources[""resource_bundle_id""] == ""cpu.something""
+
+    def test_resource_bundle_from_prior_version(
+        self,
+        toggle_resource_bundle_ff,
+        custom_task_version_service,
+        custom_model,
+        custom_model_version,
+    ):
+        custom_model_version.resource_bundle_id = ""cpu.something""
+        result_resources = custom_task_version_service._deduce_resources_if_not_provided(
+            custom_model,
+            custom_model_version,
+            user_resources={},
+        )
+
+        if toggle_resource_bundle_ff:
+            assert result_resources[""resource_bundle_id""] == ""cpu.something""
+        else:
+            assert result_resources[""resource_bundle_id""] is None
+
+    def test_resource_bundle_from_maximum_memory(
+        self,
+        toggle_resource_bundle_ff,
+        custom_task_version_service,
+        custom_model,
+        custom_model_version,
+    ):
+        result_resources = custom_task_version_service._deduce_resources_if_not_provided(
+            custom_model,
+            custom_model_version,
+            user_resources={""maximum_memory"": 2**20},
+        )
+        if toggle_resource_bundle_ff:
+            assert result_resources[""resource_bundle_id""] == ""cpu.nano""
+        else:
+            assert result_resources[""resource_bundle_id""] is None
+
+    def test_resource_bundle_from_global_default(
+        self,
+        toggle_resource_bundle_ff,
+        custom_task_version_service,
+        custom_model,
+        custom_model_version,
+    ):
+        result_resources = custom_task_version_service._deduce_resources_if_not_provided(
+            custom_model,
+            custom_model_version,
+            user_resources={},
+        )
+        if toggle_resource_bundle_ff:
+            assert (
+                result_resources[""resource_bundle_id""]
+                == EngConfig[""CUSTOM_MODEL_DEFAULT_RESOURCE_BUNDLE_ID""]
+            )
+        else:
+            assert result_resources[""resource_bundle_id""] is None
+
+
 class TestBuildDependencyImage:
     """"""
     Contains use cases to test supported and unsupported parsers for different programming
diff --git a/tests/backend/unit/raptor_lake/resource_request_bundles/__init__.py b/tests/backend/unit/raptor_lake/resource_request_bundles/__init__.py
new file mode 100644
index 00000000000000..e69de29bb2d1d6
diff --git a/tests/backend/unit/raptor_lake/resource_request_bundles/test_helpers.py b/tests/backend/unit/raptor_lake/resource_request_bundles/test_helpers.py
new file mode 100644
index 00000000000000..7a06fe735e5c68
--- /dev/null
+++ b/tests/backend/unit/raptor_lake/resource_request_bundles/test_helpers.py
@@ -0,0 +1,134 @@
+#
+# Copyright 2024 DataRobot, Inc. and its affiliates.
+#
+# All rights reserved.
+#
+# DataRobot, Inc. Confidential.
+#
+# This is unpublished proprietary source code of DataRobot, Inc.
+# and its affiliates.
+#
+# The copyright notice above does not evidence any actual or intended
+# publication of such source code.
+import logging
+from unittest import mock
+
+import pytest
+from bson import ObjectId
+
+from raptor_lake.resource_request_bundles.entities import ResourceRequestBundle
+from raptor_lake.resource_request_bundles.helpers import convert_maximum_memory_to_resource_bundle
+from raptor_lake.resource_request_bundles.services import ResourceRequestBundleService
+from tests.common.utils import patch_feature_flag
+
+logger = logging.getLogger(__name__)
+
+
+@pytest.fixture
+def user_id():
+    return ObjectId()
+
+
+@pytest.fixture
+def bundles():
+    return [
+        ResourceRequestBundle(
+            id=""gpu.medium"",
+            name=""GPU Medium"",
+            description="""",
+            cpu_count=4,
+            memory_bytes=37,
+            gpu_count=2,
+            use_cases=[""customModel""],
+        ),
+        ResourceRequestBundle(
+            id=""cpu.two"",
+            name=""CPU Two"",
+            description="""",
+            cpu_count=2,
+            memory_bytes=10,
+            use_cases=[""customModel""],
+        ),
+        ResourceRequestBundle(
+            id=""cpu.one"",
+            name=""CPU One"",
+            description="""",
+            cpu_count=1,
+            memory_bytes=10,
+            use_cases=[""customModel""],
+        ),
+        ResourceRequestBundle(
+            id=""gpu.small"",
+            name=""GPU Small"",
+            description="""",
+            cpu_count=2,
+            memory_bytes=35,
+            gpu_count=1,
+            use_cases=[""customModel""],
+        ),
+        ResourceRequestBundle(
+            id=""cpu.three"",
+            name=""CPU Three"",
+            description="""",
+            cpu_count=1,
+            memory_bytes=36,
+            use_cases=[""customModel""],
+        ),
+    ]
+
+
+@pytest.fixture(autouse=True)
+def enable_gpu_ff():
+    with patch_feature_flag(""CUSTOM_MODEL_GPU_INFERENCE"", True):
+        yield
+
+
+@pytest.fixture(autouse=True)
+def mock_bundle_svc_list(bundles):
+    with mock.patch.object(ResourceRequestBundleService, ""list"") as mocker:
+        mocker.return_value = bundles
+        yield mocker
+
+
+def test_sorting_resource_bundles(bundles):
+    bundles.sort()
+    assert [bundle.id for bundle in bundles] == [
+        ""cpu.one"",
+        ""cpu.two"",
+        ""cpu.three"",
+        ""gpu.small"",
+        ""gpu.medium"",
+    ]
+
+
+def test_sorting_resource_bundles_reversed(bundles):
+    bundles.sort(reverse=True)
+    assert [bundle.id for bundle in bundles] == [
+        ""gpu.medium"",
+        ""gpu.small"",
+        ""cpu.three"",
+        ""cpu.two"",
+        ""cpu.one"",
+    ]
+
+
+@pytest.mark.parametrize(
+    ""max_memory,expected"",
+    [
+        (5, ""cpu.one""),
+        (10, ""cpu.one""),
+        (20, ""cpu.three""),
+        (30, ""cpu.three""),
+        (35, ""cpu.three""),
+        (36, ""cpu.three""),
+        (37, ""gpu.medium""),
+    ],
+)
+def test_convert_maximum_memory_to_resource_bundle(max_memory, expected, user_id):
+    bundle = convert_maximum_memory_to_resource_bundle(user_id, max_memory)
+    assert bundle.id == expected
+
+
+def test_convert_maximum_memory_to_resource_bundle_out_of_bounds(user_id):
+    with pytest.raises(ValueError, match=""No resource bundle found for maximum memory 38 bytes""):
+        convert_maximum_memory_to_resource_bundle(user_id, 38)
diff --git a/tests/backend/unit/raptor_lake/resource_request_bundles/test_services.py b/tests/backend/unit/raptor_lake/resource_request_bundles/test_services.py
new file mode 100644
index 00000000000000..d5e1399f2e001d
--- /dev/null
+++ b/tests/backend/unit/raptor_lake/resource_request_bundles/test_services.py
@@ -0,0 +1,186 @@
+#
+# Copyright 2024 DataRobot, Inc. and its affiliates.
+#
+# All rights reserved.
+#
+# DataRobot, Inc. Confidential.
+#
+# This is unpublished proprietary source code of DataRobot, Inc.
+# and its affiliates.
+#
+# The copyright notice above does not evidence any actual or intended
+# publication of such source code.
+import logging
+from unittest import mock
+
+import pytest
+
+from common.exceptions_core_backend import PermissionsError
+from common.exceptions_core_backend import ResourceNotFoundError
+from config.engine import EngConfig
+from raptor_lake.resource_request_bundles.entities import ResourceRequestBundleUseCases
+from raptor_lake.resource_request_bundles.services import ResourceRequestBundleService
+from tests.common.utils import patch_feature_flag
+
+logger = logging.getLogger(__name__)
+
+FAKE_LRS_GPU_CONTAINER_SIZES = [
+    {
+        ""id"": ""gpu.small"",
+        ""name"": ""GPU Small"",
+        ""description"": ""1 x NVIDIA T4 | 16GB VRAM | 4 CPU | 16GB RAM"",
+        ""gpu_maker"": ""nvidia"",
+        ""gpu_type_label"": ""nvidia-t4-x"",
+        ""gpu_count"": 1,
+        ""cpu_count"": 4,
+        ""memory_mb"": 16000,
+        ""use_cases"": [""customModel"", ""customJob""],
+    },
+    {
+        ""id"": ""gpu.humongous"",
+        ""name"": ""GPU Large"",
+        ""description"": ""1 x NVIDIA A10 | 24GB VRAM | 8 CPU | 32GB RAM"",
+        ""gpu_maker"": ""nvidia"",
+        ""gpu_type_label"": ""nvidia-a10g-2x"",
+        ""gpu_count"": 1,
+        ""cpu_count"": 8,
+        ""memory_mb"": 32000,
+        ""use_cases"": [""customModel""],
+    },
+]
+
+FAKE_LRS_CPU_CONTAINER_SIZES = [
+    {
+        ""id"": ""cpu_default"",
+        ""name"": ""CPU Other"",
+        ""description"": ""Try out CPU only bundles"",
+        ""cpu_count"": 1,
+        ""memory_mb"": 512,
+        ""use_cases"": [""customModel""],
+    },
+    {
+        ""id"": ""cpu_old"",
+        ""name"": ""CPU Deprecated"",
+        ""description"": ""Bundle that we don't want used anymore"",
+        ""cpu_count"": 1,
+        ""memory_mb"": 512,
+        ""use_cases"": [""customModel""],
+        # Test that we don't list deleted bundles
+        ""is_deleted"": True,
+    },
+]
+
+
+@pytest.fixture(autouse=True)
+def mock_lrs_gpu_container_sizes(request):
+    if ""useactualbundles"" in request.keywords:
+        yield
+        return
+    # Override actual bundle list so these tests don't need to change when the actual list changes
+    with mock.patch.dict(
+        EngConfig,
+        {
+            ""LRS_GPU_CONTAINER_SIZES"": FAKE_LRS_GPU_CONTAINER_SIZES,
+            ""LRS_CPU_CONTAINER_SIZES"": FAKE_LRS_CPU_CONTAINER_SIZES,
+        },
+    ):
+        yield
+
+
+@pytest.fixture(autouse=True)
+def enable_lrs_gpu_support():
+    with mock.patch.dict(EngConfig, {""LRS_GPU_ENABLED"": True}):
+        yield
+
+
+@pytest.fixture
+def bundle_service(request):
+    with patch_feature_flag(""CUSTOM_MODEL_GPU_INFERENCE"", ""enablegpu"" in request.keywords):
+        yield ResourceRequestBundleService(user_id=""66441d650942c92702a9b596"")
+
+
+def test_get(bundle_service):
+    bundle = bundle_service.get(""cpu_default"")
+    assert bundle.id == ""cpu_default""
+    assert bundle.description == ""Try out CPU only bundles""
+
+
+def test_get_deleted(bundle_service):
+    bundle = bundle_service.get(""cpu_old"", show_deleted=True)
+    assert bundle.id == ""cpu_old""
+    assert bundle.description == ""Bundle that we don't want used anymore""
+
+
+def test_get_deleted_not_found(bundle_service):
+    with pytest.raises(ResourceNotFoundError, match=""ResourceBundle with id 'cpu_old'""):
+        bundle_service.get(""cpu_old"")
+
+
+def test_get_not_found(bundle_service):
+    with pytest.raises(ResourceNotFoundError, match=""ResourceBundle with id 'cpu_not_found'""):
+        bundle_service.get(""cpu_not_found"")
+
+
+def test_get_permissions_error(bundle_service):
+    with pytest.raises(PermissionsError, match=""CUSTOM_MODEL_GPU_INFERENCE is not enabled""):
+        bundle_service.get(""gpu.small"")
+
+
+def test_list(bundle_service):
+    bundles = bundle_service.list()
+    assert len(bundles) == 1
+    assert bundles[0].id == ""cpu_default""
+
+
+def test_list_show_deleted(bundle_service):
+    bundles = bundle_service.list(show_deleted=True)
+    assert len(bundles) == 2
+    assert tuple(bundle.id for bundle in bundles) == (""cpu_default"", ""cpu_old"")
+
+
+@pytest.mark.enablegpu
+def test_list_with_gpus(bundle_service):
+    bundles = bundle_service.list()
+    assert len(bundles) == 3
+    assert tuple(bundle.id for bundle in bundles) == (""cpu_default"", ""gpu.small"", ""gpu.humongous"")
+
+
+@pytest.mark.enablegpu
+def test_list_with_use_cases(bundle_service):
+    bundles = bundle_service.list(use_cases=ResourceRequestBundleUseCases.CUSTOM_JOB)
+    assert len(bundles) == 1
+    assert bundles[0].id == ""gpu.small""
+
+
+def test_list_with_use_cases_empty(bundle_service):
+    bundles = bundle_service.list(use_cases=ResourceRequestBundleUseCases.CUSTOM_APPLICATION)
+    assert len(bundles) == 0
+
+
+@pytest.mark.enablegpu
+def test_list_multiple_filters(bundle_service):
+    bundles = bundle_service.list(
+        use_cases=ResourceRequestBundleUseCases.CUSTOM_MODEL, show_deleted=True
+    )
+    assert len(bundles) == 4
+    assert tuple(bundle.id for bundle in bundles) == (
+        ""cpu_default"",
+        ""cpu_old"",
+        ""gpu.small"",
+        ""gpu.humongous"",
+    )
+
+
+@pytest.mark.enablegpu
+@pytest.mark.useactualbundles
+def test_list_with_actual_bundles(bundle_service):
+    """"""Test with the actual bundle list to make sure all are parsed by our DTO""""""
+    n_cpu_bundles = len(EngConfig[""LRS_CPU_CONTAINER_SIZES""])
+    n_gpu_bundles = len(EngConfig[""LRS_GPU_CONTAINER_SIZES""])
+
+    bundles = bundle_service.list(show_deleted=True)
+    assert ""cpu_default"" not in set(
+        bundle.id for bundle in bundles
+    ), ""Test looks like it is using fake bundle list""
+    logger.info(""Found %d CPU bundles and %d GPU bundles"", n_cpu_bundles, n_gpu_bundles)
+    assert len(bundles) == n_cpu_bundles + n_gpu_bundles",,"Making edits to Custom Model resets the value of the Resource Bundle

Not sure if the bug is in the FE or BE but when making edits to the ENV or to a runtime param value, the Resource Bundle goes back to some default value.","Don't set default bundle so early

Remove stuff that was never implemented

Add ordering to resource bundles

Sometimes it is useful to sort them based on resources.

Fix how we set default resource bundle

Before we were setting the default too early before we check prior saved version.

Remove defunct test

Fix lint

Mock out ff in tests

Add skeleton for tests of bundle service

Add tests for service abstraction

Update tests for bundle service

Cleanup resource bundle API tests

The tests were relying on too much deep knowledge of how service abstraction worked. That moved into service tests so now these tests can mock service object

Add helper to translate maximumMemory to resource bundle

Add tests for deducing resource bundle


diff --git a/public_api/custom_model/common.py b/public_api/custom_model/common.py
index 782fb6b8d516ba..428382c387e993 100644
--- a/public_api/custom_model/common.py
+++ b/public_api/custom_model/common.py
@@ -36,7 +36,6 @@
 from common.services.custom_models.exceptions import CustomModelVersionFrozenUpdate
 from common.services.data_management.catalog_service import get_catalog_and_version
 from common.services.execution_environment import ExecutionEnvironmentService
-from common.services.feature_toggling import is_feature_enabled
 from common.services.flippers import GlobalFlipper
 from common.services.model_deployment import ModelDeploymentService
 from common.services.organization import OrganizationService
@@ -215,7 +214,6 @@ def get_custom_model_user_configured_resources(uid, data):
     validate_memory_resources_values(
         uid, persistent, resources.get('desired_memory'), resources.get('maximum_memory')
     )
-    set_default_resource_bundle_id(uid, resources)
 
     validate_replicas_resource_value(uid, persistent, resources.get('replicas'))
     remove_disabled_custom_inference_model_resource_attributes(resources)
@@ -291,24 +289,6 @@ def validate_memory_resources_values(uid, persistent, desired_memory, maximum_me
         raise CustomModelInvalidParameter(gettext(msg))
 
 
-def set_default_resource_bundle_id(uid, resources):
-    """"""
-    Sets default resource bundle id if no other resource information is provided
-
-    :param uid:  ObjectId, user id
-    :param resources:  dict
-    """"""
-    maximum_memory = resources.get('maximum_memory')
-    resource_bundle_id = resources.get('resource_bundle_id')
-    # Order of checks is intentional here to avoid needless DB query for common case.
-    if (
-        maximum_memory is None
-        and resource_bundle_id is None
-        and is_feature_enabled(uid, 'MLOPS_RESOURCE_REQUEST_BUNDLES')
-    ):
-        resources['resource_bundle_id'] = EngConfig['CUSTOM_MODEL_DEFAULT_RESOURCE_BUNDLE_ID']
-
-
 def user_org(uid, persistent):
     user_service = UserServiceBase(persistent=persistent)
     org_id = user_service.get_org_id(uid)
diff --git a/raptor_lake/custom_model/services/custom_task_version_service.py b/raptor_lake/custom_model/services/custom_task_version_service.py
index cc797277de3a92..a075dccb875c3e 100644
--- a/raptor_lake/custom_model/services/custom_task_version_service.py
+++ b/raptor_lake/custom_model/services/custom_task_version_service.py
@@ -112,6 +112,8 @@
 from raptor_lake.dependencies.exceptions import CustomDependencySupportError
 from raptor_lake.dependencies.services import BaseDependencyParserService
 from raptor_lake.execution_environment.services import ExecutionEnvironmentVersionService
+from raptor_lake.resource_request_bundles.entities import ResourceRequestBundleUseCases
+from raptor_lake.resource_request_bundles.helpers import convert_maximum_memory_to_resource_bundle
 from raptor_lake.workspace.enums import SourceType
 from raptor_lake.workspace.services import WorkspaceItemService
 from raptor_lake.workspace.services import WorkspaceService
@@ -253,6 +255,54 @@ def _get_next_version(
         )
         return version_bumped
 
+    def _deduce_network_egress_policy(self, value_from_baseline):
+        """"""
+        Deduce the network egress policy based on the value from the baseline model
+        """"""
+        if is_feature_enabled(self.user_id, 'PUBLIC_NETWORK_ACCESS_FOR_ALL_CUSTOM_MODELS'):
+            if value_from_baseline is None:
+                return NetworkEgressPolicy.public.value
+        else:
+            if value_from_baseline in [
+                NetworkEgressPolicy.dr_api_access.value,
+                NetworkEgressPolicy.public.value,
+            ]:
+                return NetworkEgressPolicy.none.value
+        return value_from_baseline
+
+    def _deduce_resource_bundle(self, value_from_baseline, baseline_model, user_resources):
+        """"""
+        Deduce the resource bundle based on the value from the baseline model
+        """"""
+        if not is_feature_enabled(self.user_id, 'MLOPS_RESOURCE_REQUEST_BUNDLES'):
+            return None
+
+        # Order matters here: if the FF was off then we should NOT copy from baseline because that
+        # would be hidden from the user (as the UI wouldn't even show a resource bundle is
+        # associated with the model).
+        if value_from_baseline is not None:
+            return value_from_baseline
+
+        # Try and deduce the bundle from either the current or previous memory settings.
+        if max_memory := user_resources.get(
+            ""maximum_memory"", getattr(baseline_model, ""maximum_memory"")
+        ):
+            try:
+                bundle = convert_maximum_memory_to_resource_bundle(
+                    self.user_id, max_memory, use_cases=ResourceRequestBundleUseCases.CUSTOM_MODEL
+                )
+                return bundle.id
+            except ValueError:
+                raise UnprocessableRequestError(
+                    gettext(
+                        ""Failed to deduce resource bundle from maximum memory setting. Please try""
+                        "" again by providing a valid `resource_bundle_id`.""
+                    )
+                )
+        else:
+            # Fall back to global default if all else fails
+            return EngConfig['CUSTOM_MODEL_DEFAULT_RESOURCE_BUNDLE_ID']
+
     def _deduce_resources_if_not_provided(self, custom_task, current_version, user_resources):
         """"""
         Returns the proper custom task's resources, which depends on user provided, previous
@@ -264,17 +314,11 @@ def _deduce_resources_if_not_provided(self, custom_task, current_version, user_r
             if user_resources.get(resource) is None:
                 value_from_baseline = getattr(baseline_model, resource)
                 if resource == 'network_egress_policy' and custom_task.is_inference:
-                    if is_feature_enabled(
-                        self.user_id, 'PUBLIC_NETWORK_ACCESS_FOR_ALL_CUSTOM_MODELS'
-                    ):
-                        if value_from_baseline is None:
-                            value_from_baseline = NetworkEgressPolicy.public.value
-                    else:
-                        if value_from_baseline in [
-                            NetworkEgressPolicy.dr_api_access.value,
-                            NetworkEgressPolicy.public.value,
-                        ]:
-                            value_from_baseline = NetworkEgressPolicy.none.value
+                    value_from_baseline = self._deduce_network_egress_policy(value_from_baseline)
+                elif resource == ""resource_bundle_id"" and custom_task.is_inference:
+                    value_from_baseline = self._deduce_resource_bundle(
+                        value_from_baseline, baseline_model, user_resources
+                    )
                 user_resources[resource] = value_from_baseline
 
         return user_resources
diff --git a/raptor_lake/resource_request_bundles/entities.py b/raptor_lake/resource_request_bundles/entities.py
index 5ae879d4402681..22ad00f0cac65a 100644
--- a/raptor_lake/resource_request_bundles/entities.py
+++ b/raptor_lake/resource_request_bundles/entities.py
@@ -72,3 +72,10 @@ class ResourceRequestBundle(MarkDeletedDtoMixin, Dto):
     @property
     def has_gpu(self):
         return self.gpu_count > 0
+
+    def __lt__(self, other):
+        return (self.gpu_count, self.memory_bytes, self.cpu_count) < (
+            other.gpu_count,
+            other.memory_bytes,
+            other.cpu_count,
+        )
diff --git a/raptor_lake/resource_request_bundles/helpers.py b/raptor_lake/resource_request_bundles/helpers.py
new file mode 100644
index 00000000000000..bddc046ed41778
--- /dev/null
+++ b/raptor_lake/resource_request_bundles/helpers.py
@@ -0,0 +1,32 @@
+#
+# Copyright 2024 DataRobot, Inc. and its affiliates.
+#
+# All rights reserved.
+#
+# DataRobot, Inc. Confidential.
+#
+# This is unpublished proprietary source code of DataRobot, Inc.
+# and its affiliates.
+#
+# The copyright notice above does not evidence any actual or intended
+# publication of such source code.
+from typing import Optional
+
+from bson import ObjectId
+
+from raptor_lake.resource_request_bundles.entities import ResourceRequestBundle
+from raptor_lake.resource_request_bundles.entities import ResourceRequestBundleUseCases
+from raptor_lake.resource_request_bundles.services import ResourceRequestBundleService
+
+
+def convert_maximum_memory_to_resource_bundle(
+    user_id: ObjectId,
+    maximum_memory_bytes: int,
+    use_cases: Optional[ResourceRequestBundleUseCases] = None,
+) -> ResourceRequestBundle:
+    bundles = ResourceRequestBundleService(user_id=user_id).list(use_cases=use_cases)
+    bundles.sort()
+    for bundle in bundles:
+        if maximum_memory_bytes <= bundle.memory_bytes:
+            return bundle
+    raise ValueError(f""No resource bundle found for maximum memory {maximum_memory_bytes} bytes"")
diff --git a/raptor_lake/resource_request_bundles/services.py b/raptor_lake/resource_request_bundles/services.py
index 87b10963101961..6d01b0285b483d 100644
--- a/raptor_lake/resource_request_bundles/services.py
+++ b/raptor_lake/resource_request_bundles/services.py
@@ -11,16 +11,12 @@
 # The copyright notice above does not evidence any actual or intended
 # publication of such source code.
 from itertools import chain
-from typing import Optional
 
-import drdbs
 from bson import ObjectId
 
 from common.exceptions_core_backend import PermissionsError
 from common.exceptions_core_backend import ResourceNotFoundError
 from common.services.feature_toggling import is_feature_enabled
-from common.wrappers import database
-from config.dynamic_config import DynamicConfig
 from config.engine import EngConfig
 from dr_libs.utilities.object_id import to_object_id
 
@@ -30,11 +26,8 @@
 class ResourceRequestBundleService:
     dto_class = ResourceRequestBundle
 
-    def __init__(self, user_id: ObjectId, persistent: Optional[drdbs.mongo_db.MongoDB] = None):
+    def __init__(self, user_id: ObjectId, persistent=None):  # pylint: disable=unused-argument
         self.user_id = to_object_id(user_id)
-        self.persistent = persistent or database.new_persistent()
-        # TODO: implement dyconfig to allow dynamically overriding static values
-        self.dyconfig = DynamicConfig(persistent=self.persistent).from_mongo()
 
         self._dto_factory = self.dto_class.from_storage
         self._lookup_by_id = {
diff --git a/tests/backend/unit/public_api/custom_models/test_custom_model_version_api.py b/tests/backend/unit/public_api/custom_models/test_custom_model_version_api.py
index 03a5909240fd90..7ab5cf960c0f7b 100644
--- a/tests/backend/unit/public_api/custom_models/test_custom_model_version_api.py
+++ b/tests/backend/unit/public_api/custom_models/test_custom_model_version_api.py
@@ -693,39 +693,6 @@ def test_model_version_create_with_resource_bundle(
         user_resources = kwargs[""user_resources""]
         assert user_resources.get(""resource_bundle_id"") == str(resource_bundle_id)
 
-    @pytest.mark.parametrize(""resource_bundle_enabled"", [False, True])
-    def test_model_version_create_default_resource_bundle(
-        self,
-        client,
-        mock_create_custom_task_version,
-        resource_bundle_enabled,
-    ):
-        base_environment_id = ObjectId()
-        custom_task_id = ObjectId()
-        resource_bundle_id = str(ObjectId())
-
-        payload = {'baseEnvironmentId': base_environment_id}
-
-        with mock.patch.dict(
-            EngConfig, {""CUSTOM_MODEL_DEFAULT_RESOURCE_BUNDLE_ID"": resource_bundle_id}
-        ), patch_feature_flag('MLOPS_RESOURCE_REQUEST_BUNDLES', resource_bundle_enabled):
-            response = client.post(
-                self.url_template.format(custom_task_id),
-                data=payload,
-                headers=self.headers,
-            )
-
-        assert response.status_code == 201, response.data
-        mock_create_custom_task_version.assert_called_once()
-        kwargs = mock_create_custom_task_version.call_args[1]
-
-        assert ""user_resources"" in kwargs
-        user_resources = kwargs[""user_resources""]
-        if resource_bundle_enabled:
-            assert user_resources.get(""resource_bundle_id"") == resource_bundle_id
-        else:
-            assert user_resources.get(""resource_bundle_id"") == None
-
     @patch_feature_flag('MLOPS_RESOURCE_REQUEST_BUNDLES', True)
     def test_model_version_create_with_resource_bundle_and_max_mem(
         self,
diff --git a/tests/backend/unit/public_api/mlops/test_resource_request_bundles.py b/tests/backend/unit/public_api/mlops/test_resource_request_bundles.py
index 74999a5f3734d5..08f1032b7b5eb9 100644
--- a/tests/backend/unit/public_api/mlops/test_resource_request_bundles.py
+++ b/tests/backend/unit/public_api/mlops/test_resource_request_bundles.py
@@ -15,74 +15,14 @@
 
 import pytest
 
+from common.exceptions_core_backend import ResourceNotFoundError
 from config.engine import EngConfig
+from raptor_lake.resource_request_bundles.entities import ResourceRequestBundle
+from raptor_lake.resource_request_bundles.services import ResourceRequestBundleService
 from tests.common.utils import patch_feature_flag
 
 logger = logging.getLogger(__name__)
 
-FAKE_LRS_GPU_CONTAINER_SIZES = [
-    {
-        ""id"": ""gpu.small"",
-        ""name"": ""GPU Small"",
-        ""description"": ""1 x NVIDIA T4 | 16GB VRAM | 4 CPU | 16GB RAM"",
-        ""gpu_maker"": ""nvidia"",
-        ""gpu_type_label"": ""nvidia-t4-x"",
-        ""gpu_count"": 1,
-        ""cpu_count"": 4,
-        ""memory_mb"": 16000,
-        ""use_cases"": [""customModel"", ""customJob""],
-    },
-    {
-        # Have a silly ID that would never be in the actual list
-        ""id"": ""gpu.humongous"",
-        ""name"": ""GPU Large"",
-        ""description"": ""1 x NVIDIA A10 | 24GB VRAM | 8 CPU | 32GB RAM"",
-        ""gpu_maker"": ""nvidia"",
-        ""gpu_type_label"": ""nvidia-a10g-2x"",
-        ""gpu_count"": 1,
-        ""cpu_count"": 8,
-        ""memory_mb"": 32000,
-        ""use_cases"": [""customModel""],
-    },
-]
-
-FAKE_LRS_CPU_CONTAINER_SIZES = [
-    {
-        ""id"": ""cpu_default"",
-        ""name"": ""CPU Other"",
-        ""description"": ""Try out CPU only bundles"",
-        ""cpu_count"": 1,
-        ""memory_mb"": 512,
-        ""use_cases"": [""customModel""],
-    },
-    {
-        ""id"": ""cpu_old"",
-        ""name"": ""CPU Deprecated"",
-        ""description"": ""Bundle that we don't want used anymore"",
-        ""cpu_count"": 1,
-        ""memory_mb"": 512,
-        ""use_cases"": [""customModel""],
-        # Test that we don't list deleted bundles
-        ""is_deleted"": True,
-    },
-]
-
-
-@pytest.fixture(autouse=True)
-def mock_lrs_gpu_container_sizes(request):
-    if ""useactualbundles"" in request.keywords:
-        yield
-        return
-    # Override actual bundle list so these tests don't need to change when the actual list changes
-    with mock.patch.dict(
-        EngConfig,
-        {
-            ""LRS_GPU_CONTAINER_SIZES"": FAKE_LRS_GPU_CONTAINER_SIZES,
-            ""LRS_CPU_CONTAINER_SIZES"": FAKE_LRS_CPU_CONTAINER_SIZES,
-        },
-    ):
-        yield
-
 
 @pytest.fixture(autouse=True)
 def enable_resource_bundle_ff():
@@ -96,7 +36,52 @@ def enable_lrs_gpu_support():
         yield
 
 
-@pytest.mark.usefixtures(""mock_private_feature_flag"")
+@pytest.fixture
+def cpu_bundle():
+    return ResourceRequestBundle(
+        id=""cpu_default"",
+        name=""CPU Other"",
+        description=""Try out CPU only bundles"",
+        cpu_count=1,
+        memory_bytes=512 * 1024 * 1024,
+        use_cases=[""customModel""],
+    )
+
+
+@pytest.fixture
+def gpu_bundle():
+    return ResourceRequestBundle(
+        id=""gpu.small"",
+        name=""GPU Small"",
+        description=""1 x NVIDIA T4 | 16GB VRAM | 4 CPU | 16GB RAM"",
+        cpu_count=4,
+        memory_bytes=16777216000,
+        gpu_maker=""nvidia"",
+        gpu_count=1,
+        use_cases=[""customModel"", ""customJob""],
+    )
+
+
+@pytest.fixture
+def deleted_bundle():
+    return ResourceRequestBundle(
+        id=""cpu_old"",
+        name=""CPU Deprecated"",
+        description=""Bundle that we don't want used anymore"",
+        cpu_count=1,
+        memory_bytes=536870912,
+        use_cases=[""customModel""],
+        is_deleted=True,
+    )
+
+
+@pytest.fixture(autouse=True)
+def toggle_gpu_ff(request):
+    flag_value = getattr(request, ""param"", False)
+    with patch_feature_flag(""CUSTOM_MODEL_GPU_INFERENCE"", flag_value):
+        yield flag_value
+
+
 class TestResourceRequestBundleList:
     """"""
     Test listing Resource Request Bundles
@@ -104,44 +89,31 @@ class TestResourceRequestBundleList:
 
     BASE_URL = ""/mlops/compute/bundles/""
 
-    @pytest.mark.parametrize(
-        ""gpu_ff,expected_total"", [(True, 3), (False, 1)], ids=[""gpu_enabled"", ""gpu_disabled""]
-    )
-    def test_no_filter(self, client, gpu_ff, expected_total):
-        with patch_feature_flag(""CUSTOM_MODEL_GPU_INFERENCE"", gpu_ff):
-            response = client.get(self.BASE_URL)
+    @pytest.fixture(autouse=True)
+    def mock_bundle_svc_list(self, cpu_bundle):
+        with mock.patch.object(ResourceRequestBundleService, ""list"") as mocker:
+            mocker.return_value = [cpu_bundle]
+            yield mocker
+
+    def test_no_filter(self, client, mock_bundle_svc_list):
+        response = client.get(self.BASE_URL)
+        mock_bundle_svc_list.assert_called_once_with()
         assert response.status_code == 200
-        assert len(response.json[""data""]) == expected_total
+        assert len(response.json[""data""]) == 1
+        assert response.json[""data""][0][""id""] == ""cpu_default""
 
-    @pytest.mark.parametrize(
-        ""gpu_ff,expected_total"", [(True, 1), (False, 0)], ids=[""gpu_enabled"", ""gpu_disabled""]
-    )
-    def test_with_filter(self, client, gpu_ff, expected_total):
-        with patch_feature_flag(""CUSTOM_MODEL_GPU_INFERENCE"", gpu_ff):
-            response = client.get(self.BASE_URL, query_string={""useCases"": ""customJob""})
+    def test_with_filter(self, client, mock_bundle_svc_list):
+        response = client.get(self.BASE_URL, query_string={""useCases"": ""customJob""})
+        mock_bundle_svc_list.assert_called_once_with(use_cases=""customJob"")
         assert response.status_code == 200
-        assert len(response.json[""data""]) == expected_total
+        assert ""data"" in response.json
 
     def test_with_invalid_filter(self, client):
         response = client.get(self.BASE_URL, query_string={""useCases"": ""CustomSomething""})
         assert response.status_code == 400
         assert ""does not match allowed values"" in response.json[""message""][""useCases""]
 
-    @pytest.mark.useactualbundles
-    @patch_feature_flag(""CUSTOM_MODEL_GPU_INFERENCE"", True)
-    def test_actual_bundle_list(self, client):
-        """"""Test with the actual bundle list to make sure it parses our DTOs""""""
-        response = client.get(self.BASE_URL)
-        assert response.status_code == 200
-        # Sanity check that we aren't using the fake list
-        assert ""gpu.humongous"" not in response.text, ""Test looks like it is using fake bundle list""
-        n_cpu_bundles = len(EngConfig[""LRS_CPU_CONTAINER_SIZES""])
-        n_gpu_bundles = len(EngConfig[""LRS_GPU_CONTAINER_SIZES""])
-        logger.info(""Found %d CPU bundles and %d GPU bundles"", n_cpu_bundles, n_gpu_bundles)
-        assert len(response.json[""data""]) == n_cpu_bundles + n_gpu_bundles
-
 
-@pytest.mark.usefixtures(""mock_private_feature_flag"")
 class TestResourceRequestBundleRetrieve:
     """"""
     Test listing Resource Request Bundles
@@ -149,17 +121,24 @@ class TestResourceRequestBundleRetrieve:
 
     BASE_URL = ""/mlops/compute/bundles""
 
-    @patch_feature_flag(""CUSTOM_MODEL_GPU_INFERENCE"", False)
-    def test_not_found(self, client):
+    @pytest.fixture(autouse=True)
+    def mock_bundle_svc_get(self, gpu_bundle):
+        with mock.patch.object(ResourceRequestBundleService, ""get_object"") as mocker:
+            mocker.return_value = gpu_bundle
+            yield mocker
+
+    def test_not_found(self, client, mock_bundle_svc_get):
+        mock_bundle_svc_get.side_effect = ResourceNotFoundError(""'does_not_exist' not found"")
         response = client.get(f""{self.BASE_URL}/does_not_exist/"")
         assert response.status_code == 404
         assert ""not found"" in response.json[""message""]
 
-    @pytest.mark.parametrize(""gpu_ff"", [True, False], ids=[""gpu_enabled"", ""gpu_disabled""])
-    def test_get(self, client, gpu_ff):
-        with patch_feature_flag(""CUSTOM_MODEL_GPU_INFERENCE"", gpu_ff):
-            response = client.get(f""{self.BASE_URL}/gpu.small/"")
-        if gpu_ff:
+    @pytest.mark.parametrize(
+        ""toggle_gpu_ff"", [True, False], ids=[""gpu_enabled"", ""gpu_disabled""], indirect=True
+    )
+    def test_get(self, client, toggle_gpu_ff):
+        response = client.get(f""{self.BASE_URL}/gpu.small/"")
+        if toggle_gpu_ff:
             assert response.status_code == 200
             assert response.json == {
                 ""id"": ""gpu.small"",
@@ -174,28 +153,13 @@ def test_get(self, client, gpu_ff):
             }
         else:
             assert response.status_code == 403
-            assert ""not enabled"" in response.json[""message""]
-
-    @patch_feature_flag(""CUSTOM_MODEL_GPU_INFERENCE"", False)
-    def test_get_cpu_resource(self, client):
-        response = client.get(f""{self.BASE_URL}/cpu_default/"")
-        assert response.status_code == 200
-        assert response.json == {
-            ""id"": ""cpu_default"",
-            ""name"": ""CPU Other"",
-            ""description"": ""Try out CPU only bundles"",
-            ""cpuCount"": 1,
-            ""memoryBytes"": 536870912,
-            ""gpuMaker"": None,
-            ""gpuCount"": 0,
-            ""useCases"": [""customModel""],
-            ""hasGpu"": False,
-        }
+            assert ""CUSTOM_MODEL_GPU_INFERENCE is not enabled"" in response.json[""message""]
 
-    @patch_feature_flag(""CUSTOM_MODEL_GPU_INFERENCE"", False)
-    def test_get_deleted(self, client):
+    def test_get_deleted(self, client, mock_bundle_svc_get, deleted_bundle):
         """"""You can look up an archived bundle by ID""""""
+        mock_bundle_svc_get.return_value = deleted_bundle
         response = client.get(f""{self.BASE_URL}/cpu_old/"")
+        mock_bundle_svc_get.assert_called_once_with(""cpu_old"", show_deleted=True)
         assert response.status_code == 200
         assert response.json == {
             ""id"": ""cpu_old"",
diff --git a/tests/backend/unit/raptor_lake/custom_model/test_custom_model_version_service.py b/tests/backend/unit/raptor_lake/custom_model/test_custom_model_version_service.py
index ffe9a32913bb5c..dd24cb41e2e902 100644
--- a/tests/backend/unit/raptor_lake/custom_model/test_custom_model_version_service.py
+++ b/tests/backend/unit/raptor_lake/custom_model/test_custom_model_version_service.py
@@ -32,6 +32,7 @@
 from common.entities.execution_environment import ExecutionEnvironment
 from common.enum_execution_environment import ExecutionEnvironmentProgrammingLanguage
 from common.services.execution_environment import ExecutionEnvironmentService
+from config.engine import EngConfig
 from execute.image_builder.services import BaseDockerImageService
 from raptor_lake.custom_model.entities import CustomTaskVersion
 from raptor_lake.custom_model.exceptions import CustomModelImageInvalidVersion
@@ -99,6 +100,21 @@ def mock_get_lpr_by_environment_and_package_version():
         yield mock_func
 
 
+@pytest.fixture()
+def toggle_resource_bundle_ff(request):
+    flag_value = getattr(request, ""param"", False)
+    with patch_feature_flag('MLOPS_RESOURCE_REQUEST_BUNDLES', flag_value):
+        yield flag_value
+
+
+@pytest.fixture()
+def mock_gpu_inference_ff():
+    with patch_feature_flag('CUSTOM_MODEL_GPU_INFERENCE', False), patch.dict(
+        EngConfig, {'LRS_GPU_ENABLED': False}
+    ):
+        yield
+
+
 # copied from tests/backend/unit/public_api/custom_models/conftest.py
 
 
@@ -667,6 +683,7 @@ def test_parse_file_paths():
         assert parsed.workspace_id == workspace_id
 
 
+@pytest.mark.usefixtures('toggle_resource_bundle_ff', 'mock_gpu_inference_ff')
 class TestResourcesMigration:
     """"""
     Contains unit-tests for resources migration from custom model, custom model version subjected
@@ -876,6 +893,109 @@ def test_network_egress_policy_migration_from_custom_model_version_subjected_to_
             assert result_resources == expected_resources
 
 
+@pytest.mark.parametrize(
+    ""toggle_resource_bundle_ff"",
+    [True, False],
+    ids=[""enable_resource_bundles"", ""disable_resource_bundles""],
+    indirect=True,
+)
+@pytest.mark.usefixtures('mock_gpu_inference_ff')
+class TestWithResourceBundle:
+    """"""
+    Contains unit-tests dealing with transition to using resource-bundles instead of memory/cpu
+    """"""
+
+    @pytest.fixture(autouse=True)
+    def mock_public_network_access_ff(self):
+        with patch_feature_flag('PUBLIC_NETWORK_ACCESS_FOR_ALL_CUSTOM_MODELS', False):
+            yield
+
+    @pytest.fixture
+    def custom_model(self, user_id):
+        custom_model = CustomTaskDto(
+            user_id=user_id,
+            workspace_id=ObjectId(),
+            name='model',
+            target_options=CustomModelTargetOptions(target_type=CustomModelTargetType.REGRESSION),
+            custom_model_type=CustomModelType.CUSTOM_INFERENCE,
+            is_time_series=False,
+        )
+        return custom_model
+
+    def test_resource_bundle_provided_by_user(
+        self,
+        toggle_resource_bundle_ff,
+        custom_task_version_service,
+        custom_model,
+        custom_model_version,
+    ):
+        if not toggle_resource_bundle_ff:
+            # This testcase doesn't make sense when resource bundles are disabled
+            pytest.skip(""Resource bundles are disabled"")
+        result_resources = custom_task_version_service._deduce_resources_if_not_provided(
+            custom_model,
+            custom_model_version,
+            user_resources={""resource_bundle_id"": ""cpu.something""},
+        )
+        assert result_resources[""resource_bundle_id""] == ""cpu.something""
+
+    def test_resource_bundle_from_prior_version(
+        self,
+        toggle_resource_bundle_ff,
+        custom_task_version_service,
+        custom_model,
+        custom_model_version,
+    ):
+        custom_model_version.resource_bundle_id = ""cpu.something""
+        result_resources = custom_task_version_service._deduce_resources_if_not_provided(
+            custom_model,
+            custom_model_version,
+            user_resources={},
+        )
+
+        if toggle_resource_bundle_ff:
+            assert result_resources[""resource_bundle_id""] == ""cpu.something""
+        else:
+            assert result_resources[""resource_bundle_id""] is None
+
+    def test_resource_bundle_from_maximum_memory(
+        self,
+        toggle_resource_bundle_ff,
+        custom_task_version_service,
+        custom_model,
+        custom_model_version,
+    ):
+        result_resources = custom_task_version_service._deduce_resources_if_not_provided(
+            custom_model,
+            custom_model_version,
+            user_resources={""maximum_memory"": 2**20},
+        )
+        if toggle_resource_bundle_ff:
+            assert result_resources[""resource_bundle_id""] == ""cpu.nano""
+        else:
+            assert result_resources[""resource_bundle_id""] is None
+
+    def test_resource_bundle_from_global_default(
+        self,
+        toggle_resource_bundle_ff,
+        custom_task_version_service,
+        custom_model,
+        custom_model_version,
+    ):
+        result_resources = custom_task_version_service._deduce_resources_if_not_provided(
+            custom_model,
+            custom_model_version,
+            user_resources={},
+        )
+        if toggle_resource_bundle_ff:
+            assert (
+                result_resources[""resource_bundle_id""]
+                == EngConfig[""CUSTOM_MODEL_DEFAULT_RESOURCE_BUNDLE_ID""]
+            )
+        else:
+            assert result_resources[""resource_bundle_id""] is None
+
+
 class TestBuildDependencyImage:
     """"""
     Contains use cases to test supported and unsupported parsers for different programming
diff --git a/tests/backend/unit/raptor_lake/resource_request_bundles/__init__.py b/tests/backend/unit/raptor_lake/resource_request_bundles/__init__.py
new file mode 100644
index 00000000000000..e69de29bb2d1d6
diff --git a/tests/backend/unit/raptor_lake/resource_request_bundles/test_helpers.py b/tests/backend/unit/raptor_lake/resource_request_bundles/test_helpers.py
new file mode 100644
index 00000000000000..7a06fe735e5c68
--- /dev/null
+++ b/tests/backend/unit/raptor_lake/resource_request_bundles/test_helpers.py
@@ -0,0 +1,134 @@
+#
+# Copyright 2024 DataRobot, Inc. and its affiliates.
+#
+# All rights reserved.
+#
+# DataRobot, Inc. Confidential.
+#
+# This is unpublished proprietary source code of DataRobot, Inc.
+# and its affiliates.
+#
+# The copyright notice above does not evidence any actual or intended
+# publication of such source code.
+import logging
+from unittest import mock
+
+import pytest
+from bson import ObjectId
+
+from raptor_lake.resource_request_bundles.entities import ResourceRequestBundle
+from raptor_lake.resource_request_bundles.helpers import convert_maximum_memory_to_resource_bundle
+from raptor_lake.resource_request_bundles.services import ResourceRequestBundleService
+from tests.common.utils import patch_feature_flag
+
+logger = logging.getLogger(__name__)
+
+
+@pytest.fixture
+def user_id():
+    return ObjectId()
+
+
+@pytest.fixture
+def bundles():
+    return [
+        ResourceRequestBundle(
+            id=""gpu.medium"",
+            name=""GPU Medium"",
+            description="""",
+            cpu_count=4,
+            memory_bytes=37,
+            gpu_count=2,
+            use_cases=[""customModel""],
+        ),
+        ResourceRequestBundle(
+            id=""cpu.two"",
+            name=""CPU Two"",
+            description="""",
+            cpu_count=2,
+            memory_bytes=10,
+            use_cases=[""customModel""],
+        ),
+        ResourceRequestBundle(
+            id=""cpu.one"",
+            name=""CPU One"",
+            description="""",
+            cpu_count=1,
+            memory_bytes=10,
+            use_cases=[""customModel""],
+        ),
+        ResourceRequestBundle(
+            id=""gpu.small"",
+            name=""GPU Small"",
+            description="""",
+            cpu_count=2,
+            memory_bytes=35,
+            gpu_count=1,
+            use_cases=[""customModel""],
+        ),
+        ResourceRequestBundle(
+            id=""cpu.three"",
+            name=""CPU Three"",
+            description="""",
+            cpu_count=1,
+            memory_bytes=36,
+            use_cases=[""customModel""],
+        ),
+    ]
+
+
+@pytest.fixture(autouse=True)
+def enable_gpu_ff():
+    with patch_feature_flag(""CUSTOM_MODEL_GPU_INFERENCE"", True):
+        yield
+
+
+@pytest.fixture(autouse=True)
+def mock_bundle_svc_list(bundles):
+    with mock.patch.object(ResourceRequestBundleService, ""list"") as mocker:
+        mocker.return_value = bundles
+        yield mocker
+
+
+def test_sorting_resource_bundles(bundles):
+    bundles.sort()
+    assert [bundle.id for bundle in bundles] == [
+        ""cpu.one"",
+        ""cpu.two"",
+        ""cpu.three"",
+        ""gpu.small"",
+        ""gpu.medium"",
+    ]
+
+
+def test_sorting_resource_bundles_reversed(bundles):
+    bundles.sort(reverse=True)
+    assert [bundle.id for bundle in bundles] == [
+        ""gpu.medium"",
+        ""gpu.small"",
+        ""cpu.three"",
+        ""cpu.two"",
+        ""cpu.one"",
+    ]
+
+
+@pytest.mark.parametrize(
+    ""max_memory,expected"",
+    [
+        (5, ""cpu.one""),
+        (10, ""cpu.one""),
+        (20, ""cpu.three""),
+        (30, ""cpu.three""),
+        (35, ""cpu.three""),
+        (36, ""cpu.three""),
+        (37, ""gpu.medium""),
+    ],
+)
+def test_convert_maximum_memory_to_resource_bundle(max_memory, expected, user_id):
+    bundle = convert_maximum_memory_to_resource_bundle(user_id, max_memory)
+    assert bundle.id == expected
+
+
+def test_convert_maximum_memory_to_resource_bundle_out_of_bounds(user_id):
+    with pytest.raises(ValueError, match=""No resource bundle found for maximum memory 38 bytes""):
+        convert_maximum_memory_to_resource_bundle(user_id, 38)
diff --git a/tests/backend/unit/raptor_lake/resource_request_bundles/test_services.py b/tests/backend/unit/raptor_lake/resource_request_bundles/test_services.py
new file mode 100644
index 00000000000000..d5e1399f2e001d
--- /dev/null
+++ b/tests/backend/unit/raptor_lake/resource_request_bundles/test_services.py
@@ -0,0 +1,186 @@
+#
+# Copyright 2024 DataRobot, Inc. and its affiliates.
+#
+# All rights reserved.
+#
+# DataRobot, Inc. Confidential.
+#
+# This is unpublished proprietary source code of DataRobot, Inc.
+# and its affiliates.
+#
+# The copyright notice above does not evidence any actual or intended
+# publication of such source code.
+import logging
+from unittest import mock
+
+import pytest
+
+from common.exceptions_core_backend import PermissionsError
+from common.exceptions_core_backend import ResourceNotFoundError
+from config.engine import EngConfig
+from raptor_lake.resource_request_bundles.entities import ResourceRequestBundleUseCases
+from raptor_lake.resource_request_bundles.services import ResourceRequestBundleService
+from tests.common.utils import patch_feature_flag
+
+logger = logging.getLogger(__name__)
+
+FAKE_LRS_GPU_CONTAINER_SIZES = [
+    {
+        ""id"": ""gpu.small"",
+        ""name"": ""GPU Small"",
+        ""description"": ""1 x NVIDIA T4 | 16GB VRAM | 4 CPU | 16GB RAM"",
+        ""gpu_maker"": ""nvidia"",
+        ""gpu_type_label"": ""nvidia-t4-x"",
+        ""gpu_count"": 1,
+        ""cpu_count"": 4,
+        ""memory_mb"": 16000,
+        ""use_cases"": [""customModel"", ""customJob""],
+    },
+    {
+        ""id"": ""gpu.humongous"",
+        ""name"": ""GPU Large"",
+        ""description"": ""1 x NVIDIA A10 | 24GB VRAM | 8 CPU | 32GB RAM"",
+        ""gpu_maker"": ""nvidia"",
+        ""gpu_type_label"": ""nvidia-a10g-2x"",
+        ""gpu_count"": 1,
+        ""cpu_count"": 8,
+        ""memory_mb"": 32000,
+        ""use_cases"": [""customModel""],
+    },
+]
+
+FAKE_LRS_CPU_CONTAINER_SIZES = [
+    {
+        ""id"": ""cpu_default"",
+        ""name"": ""CPU Other"",
+        ""description"": ""Try out CPU only bundles"",
+        ""cpu_count"": 1,
+        ""memory_mb"": 512,
+        ""use_cases"": [""customModel""],
+    },
+    {
+        ""id"": ""cpu_old"",
+        ""name"": ""CPU Deprecated"",
+        ""description"": ""Bundle that we don't want used anymore"",
+        ""cpu_count"": 1,
+        ""memory_mb"": 512,
+        ""use_cases"": [""customModel""],
+        # Test that we don't list deleted bundles
+        ""is_deleted"": True,
+    },
+]
+
+
+@pytest.fixture(autouse=True)
+def mock_lrs_gpu_container_sizes(request):
+    if ""useactualbundles"" in request.keywords:
+        yield
+        return
+    # Override actual bundle list so these tests don't need to change when the actual list changes
+    with mock.patch.dict(
+        EngConfig,
+        {
+            ""LRS_GPU_CONTAINER_SIZES"": FAKE_LRS_GPU_CONTAINER_SIZES,
+            ""LRS_CPU_CONTAINER_SIZES"": FAKE_LRS_CPU_CONTAINER_SIZES,
+        },
+    ):
+        yield
+
+
+@pytest.fixture(autouse=True)
+def enable_lrs_gpu_support():
+    with mock.patch.dict(EngConfig, {""LRS_GPU_ENABLED"": True}):
+        yield
+
+
+@pytest.fixture
+def bundle_service(request):
+    with patch_feature_flag(""CUSTOM_MODEL_GPU_INFERENCE"", ""enablegpu"" in request.keywords):
+        yield ResourceRequestBundleService(user_id=""66441d650942c92702a9b596"")
+
+
+def test_get(bundle_service):
+    bundle = bundle_service.get(""cpu_default"")
+    assert bundle.id == ""cpu_default""
+    assert bundle.description == ""Try out CPU only bundles""
+
+
+def test_get_deleted(bundle_service):
+    bundle = bundle_service.get(""cpu_old"", show_deleted=True)
+    assert bundle.id == ""cpu_old""
+    assert bundle.description == ""Bundle that we don't want used anymore""
+
+
+def test_get_deleted_not_found(bundle_service):
+    with pytest.raises(ResourceNotFoundError, match=""ResourceBundle with id 'cpu_old'""):
+        bundle_service.get(""cpu_old"")
+
+
+def test_get_not_found(bundle_service):
+    with pytest.raises(ResourceNotFoundError, match=""ResourceBundle with id 'cpu_not_found'""):
+        bundle_service.get(""cpu_not_found"")
+
+
+def test_get_permissions_error(bundle_service):
+    with pytest.raises(PermissionsError, match=""CUSTOM_MODEL_GPU_INFERENCE is not enabled""):
+        bundle_service.get(""gpu.small"")
+
+
+def test_list(bundle_service):
+    bundles = bundle_service.list()
+    assert len(bundles) == 1
+    assert bundles[0].id == ""cpu_default""
+
+
+def test_list_show_deleted(bundle_service):
+    bundles = bundle_service.list(show_deleted=True)
+    assert len(bundles) == 2
+    assert tuple(bundle.id for bundle in bundles) == (""cpu_default"", ""cpu_old"")
+
+
+@pytest.mark.enablegpu
+def test_list_with_gpus(bundle_service):
+    bundles = bundle_service.list()
+    assert len(bundles) == 3
+    assert tuple(bundle.id for bundle in bundles) == (""cpu_default"", ""gpu.small"", ""gpu.humongous"")
+
+
+@pytest.mark.enablegpu
+def test_list_with_use_cases(bundle_service):
+    bundles = bundle_service.list(use_cases=ResourceRequestBundleUseCases.CUSTOM_JOB)
+    assert len(bundles) == 1
+    assert bundles[0].id == ""gpu.small""
+
+
+def test_list_with_use_cases_empty(bundle_service):
+    bundles = bundle_service.list(use_cases=ResourceRequestBundleUseCases.CUSTOM_APPLICATION)
+    assert len(bundles) == 0
+
+
+@pytest.mark.enablegpu
+def test_list_multiple_filters(bundle_service):
+    bundles = bundle_service.list(
+        use_cases=ResourceRequestBundleUseCases.CUSTOM_MODEL, show_deleted=True
+    )
+    assert len(bundles) == 4
+    assert tuple(bundle.id for bundle in bundles) == (
+        ""cpu_default"",
+        ""cpu_old"",
+        ""gpu.small"",
+        ""gpu.humongous"",
+    )
+
+
+@pytest.mark.enablegpu
+@pytest.mark.useactualbundles
+def test_list_with_actual_bundles(bundle_service):
+    """"""Test with the actual bundle list to make sure all are parsed by our DTO""""""
+    n_cpu_bundles = len(EngConfig[""LRS_CPU_CONTAINER_SIZES""])
+    n_gpu_bundles = len(EngConfig[""LRS_GPU_CONTAINER_SIZES""])
+
+    bundles = bundle_service.list(show_deleted=True)
+    assert ""cpu_default"" not in set(
+        bundle.id for bundle in bundles
+    ), ""Test looks like it is using fake bundle list""
+    logger.info(""Found %d CPU bundles and %d GPU bundles"", n_cpu_bundles, n_gpu_bundles)
+    assert len(bundles) == n_cpu_bundles + n_gpu_bundles"
Data Drift Not Tracking,"Customer Kafene on app.datarobot.com is deploying a custom model on the platform. The data drift report is not tracking the top 25 variables, only 7 variables show up on the chart. The variables that show up are also of lower importance as well.

Custom Model ID:  662fc44de237e6efa39dae1a

Deployment ID: 662fff72004d4cad52f8ed22

This has also been replicated by the CFDS on the account.

 

Could I please get some assistance in figuring out why the data drift has lower importance variables and significantly less than the 25 limit?","This PR fixes feature drift tracking for custom model deployments.
When we prepare data from prediction dataset for processing we select only important features for processing. There is a bug in code that call different method for custom models importance during training data establishment and prediction data processing.
Here is method that select important features for custom models https://github.com/datarobot/DataRobot/blob/master/predictions_monitor/processors/project_to_pgsql.py#L1006 as you can see first iff is checking for custom model and timeseries
In prediction data processor check only verify if it is custom model https://github.com/datarobot/DataRobot/blob/master/modmon_worker/modmon_predictions_data/pgsql.py#L4082
Added check for timeseries so importance is the same
","diff --git a/MLOps_Tests/mmm/tests/backend/unit/predictions_monitor/processors/test_pgsql.py b/MLOps_Tests/mmm/tests/backend/unit/predictions_monitor/processors/test_pgsql.py
index f61886ef145446..8e4ad8d29a663e 100644
--- a/MLOps_Tests/mmm/tests/backend/unit/predictions_monitor/processors/test_pgsql.py
+++ b/MLOps_Tests/mmm/tests/backend/unit/predictions_monitor/processors/test_pgsql.py
@@ -20,6 +20,7 @@
 
 from common.entities.model_deploy import ModelDeployment
 from common.entities.model_package import ModelPackage
+from common.services.insights.feature_impact.service import FeatureImpactInsightService
 from modmon_worker.modmon_predictions_data.pgsql import PgsqlAggregatingProcessor
 from modmon_worker.modmon_predictions_data.pgsql import prediction_input_info_dataframe
 
@@ -191,3 +192,80 @@ def test_prediction_input_info_dataframe_unsupervised(model_kind):
             model_package=model_package,
         )
     assert target_feature_name not in features
+
+
+@pytest.mark.parametrize(
+    'is_time_seriese',
+    [True, False],
+)
+def test_custom_model_importance_methods(is_time_seriese):
+    """"""
+    This test check that target drift is not supported for unsupervised projects
+    and we do not include target feature in the output
+    """"""
+    request = {
+        'model_id': ObjectId(),
+        'pid': ObjectId(),
+        'uid': None,  # user_id is not necessary here
+        'project_data': {
+            'target': {
+                'name': 'target',
+                # this is here to test support of the older project
+                'type': 'Binary',
+            }
+        },
+        'custom_model': True,
+    }
+    deployment = ModelDeployment(segment_attributes=['feature1', ""feature2""])
+    df_dict = {""feature1"": [""a"", ""b""], ""feature2"": [""c"", ""d""]}
+    scorinng_dataframe = pd.DataFrame(df_dict)
+    predictions = [0.5, 0.5]
+    model_package = ModelPackage(
+        target={
+            'class_mapping': {'0': 0.0, '1': 1.0},
+            'missing_maps_to': None,
+            'name': None,
+            'prediction_threshold': 0.5,
+            'type': 'Binary',
+        },
+        model_id=str(ObjectId()),
+        model_kind={'is_time_series': is_time_seriese},
+        datasets={},
+        source_meta={},
+        model_description={},
+        accuracy={},
+    )
+    feature_impact_service_instance_mock = mock.Mock()
+    feature_impact_chart_data = mock.Mock(return_value=[(""feature1"", 1.0), (""feature"", 0.8)])
+    feature_impact_service_instance_mock.get_chart_data = feature_impact_chart_data
+
+    feature_impact_cls_path = (
+        'modmon_worker.modmon_predictions_data.pgsql.FeatureImpactInsightService'
+    )
+    with mock.patch(""modmon_worker.modmon_predictions_data.pgsql.fetch_simplified_eda""), mock.patch(
+        ""modmon_worker.modmon_predictions_data.pgsql.column_importance_from_eda_map""
+    ), mock.patch(
+        'modmon_worker.modmon_predictions_data.pgsql.normalize_feature_importance'
+    ) as normalize_feature_importance_mock, mock.patch(
+        feature_impact_cls_path, spec=FeatureImpactInsightService
+    ) as feature_impact_cls_mock:
+        feature_impact_cls_mock.return_value = feature_impact_service_instance_mock
+        normalize_feature_importance_mock.return_value = {
+            ""feature1"": {'importance': 1.0},
+            ""feature"": {'importance': 0.8},
+        }
+        prediction_input_info_dataframe(
+            deployment=deployment,
+            request=request,
+            scoring_dataframe=scorinng_dataframe,
+            predictions=predictions,
+            model_package=model_package,
+        )
+        # Check if correct method is called to get important features depending if custom model is time series or not
+        if is_time_seriese:
+            normalize_feature_importance_mock.assert_called_once()
+            feature_impact_chart_data.assert_not_called()
+
+        else:
+            normalize_feature_importance_mock.assert_not_called()
+            feature_impact_chart_data.assert_called_once()
diff --git a/modmon_worker/modmon_predictions_data/pgsql.py b/modmon_worker/modmon_predictions_data/pgsql.py
index 79b0e33f45da2f..3f2524273ef6f0 100644
--- a/modmon_worker/modmon_predictions_data/pgsql.py
+++ b/modmon_worker/modmon_predictions_data/pgsql.py
@@ -4079,7 +4079,7 @@ def prediction_input_info_dataframe(
         descending_importance = _feature_importance[request['model_id']]
 
     except KeyError:
-        if request.get('custom_model'):
+        if request.get('custom_model') and model_package.model_kind.is_time_series:
             importance = column_importance_from_eda_map(eda)
             importance = normalize_feature_importance(importance, target)
             importance = [(name, feature['importance']) for name, feature in importance.items()]",,"Data Drift Not Tracking

Customer Kafene on app.datarobot.com is deploying a custom model on the platform. The data drift report is not tracking the top 25 variables, only 7 variables show up on the chart. The variables that show up are also of lower importance as well.

Custom Model ID:  662fc44de237e6efa39dae1a

Deployment ID: 662fff72004d4cad52f8ed22

This has also been replicated by the CFDS on the account.

 

Could I please get some assistance in figuring out why the data drift has lower importance variables and significantly less than the 25 limit?","This PR fixes feature drift tracking for custom model deployments.
When we prepare data from prediction dataset for processing we select only important features for processing. There is a bug in code that call different method for custom models importance during training data establishment and prediction data processing.
Here is method that select important features for custom models https://github.com/datarobot/DataRobot/blob/master/predictions_monitor/processors/project_to_pgsql.py#L1006 as you can see first iff is checking for custom model and timeseries
In prediction data processor check only verify if it is custom model https://github.com/datarobot/DataRobot/blob/master/modmon_worker/modmon_predictions_data/pgsql.py#L4082
Added check for timeseries so importance is the same


diff --git a/MLOps_Tests/mmm/tests/backend/unit/predictions_monitor/processors/test_pgsql.py b/MLOps_Tests/mmm/tests/backend/unit/predictions_monitor/processors/test_pgsql.py
index f61886ef145446..8e4ad8d29a663e 100644
--- a/MLOps_Tests/mmm/tests/backend/unit/predictions_monitor/processors/test_pgsql.py
+++ b/MLOps_Tests/mmm/tests/backend/unit/predictions_monitor/processors/test_pgsql.py
@@ -20,6 +20,7 @@
 
 from common.entities.model_deploy import ModelDeployment
 from common.entities.model_package import ModelPackage
+from common.services.insights.feature_impact.service import FeatureImpactInsightService
 from modmon_worker.modmon_predictions_data.pgsql import PgsqlAggregatingProcessor
 from modmon_worker.modmon_predictions_data.pgsql import prediction_input_info_dataframe
 
@@ -191,3 +192,80 @@ def test_prediction_input_info_dataframe_unsupervised(model_kind):
             model_package=model_package,
         )
     assert target_feature_name not in features
+
+
+@pytest.mark.parametrize(
+    'is_time_seriese',
+    [True, False],
+)
+def test_custom_model_importance_methods(is_time_seriese):
+    """"""
+    This test check that target drift is not supported for unsupervised projects
+    and we do not include target feature in the output
+    """"""
+    request = {
+        'model_id': ObjectId(),
+        'pid': ObjectId(),
+        'uid': None,  # user_id is not necessary here
+        'project_data': {
+            'target': {
+                'name': 'target',
+                # this is here to test support of the older project
+                'type': 'Binary',
+            }
+        },
+        'custom_model': True,
+    }
+    deployment = ModelDeployment(segment_attributes=['feature1', ""feature2""])
+    df_dict = {""feature1"": [""a"", ""b""], ""feature2"": [""c"", ""d""]}
+    scorinng_dataframe = pd.DataFrame(df_dict)
+    predictions = [0.5, 0.5]
+    model_package = ModelPackage(
+        target={
+            'class_mapping': {'0': 0.0, '1': 1.0},
+            'missing_maps_to': None,
+            'name': None,
+            'prediction_threshold': 0.5,
+            'type': 'Binary',
+        },
+        model_id=str(ObjectId()),
+        model_kind={'is_time_series': is_time_seriese},
+        datasets={},
+        source_meta={},
+        model_description={},
+        accuracy={},
+    )
+    feature_impact_service_instance_mock = mock.Mock()
+    feature_impact_chart_data = mock.Mock(return_value=[(""feature1"", 1.0), (""feature"", 0.8)])
+    feature_impact_service_instance_mock.get_chart_data = feature_impact_chart_data
+
+    feature_impact_cls_path = (
+        'modmon_worker.modmon_predictions_data.pgsql.FeatureImpactInsightService'
+    )
+    with mock.patch(""modmon_worker.modmon_predictions_data.pgsql.fetch_simplified_eda""), mock.patch(
+        ""modmon_worker.modmon_predictions_data.pgsql.column_importance_from_eda_map""
+    ), mock.patch(
+        'modmon_worker.modmon_predictions_data.pgsql.normalize_feature_importance'
+    ) as normalize_feature_importance_mock, mock.patch(
+        feature_impact_cls_path, spec=FeatureImpactInsightService
+    ) as feature_impact_cls_mock:
+        feature_impact_cls_mock.return_value = feature_impact_service_instance_mock
+        normalize_feature_importance_mock.return_value = {
+            ""feature1"": {'importance': 1.0},
+            ""feature"": {'importance': 0.8},
+        }
+        prediction_input_info_dataframe(
+            deployment=deployment,
+            request=request,
+            scoring_dataframe=scorinng_dataframe,
+            predictions=predictions,
+            model_package=model_package,
+        )
+        # Check if correct method is called to get important features depending if custom model is time series or not
+        if is_time_seriese:
+            normalize_feature_importance_mock.assert_called_once()
+            feature_impact_chart_data.assert_not_called()
+
+        else:
+            normalize_feature_importance_mock.assert_not_called()
+            feature_impact_chart_data.assert_called_once()
diff --git a/modmon_worker/modmon_predictions_data/pgsql.py b/modmon_worker/modmon_predictions_data/pgsql.py
index 79b0e33f45da2f..3f2524273ef6f0 100644
--- a/modmon_worker/modmon_predictions_data/pgsql.py
+++ b/modmon_worker/modmon_predictions_data/pgsql.py
@@ -4079,7 +4079,7 @@ def prediction_input_info_dataframe(
         descending_importance = _feature_importance[request['model_id']]
 
     except KeyError:
-        if request.get('custom_model'):
+        if request.get('custom_model') and model_package.model_kind.is_time_series:
             importance = column_importance_from_eda_map(eda)
             importance = normalize_feature_importance(importance, target)
             importance = [(name, feature['importance']) for name, feature in importance.items()]"
"Default for column filtering should be ""don't filter"" for TextGen deployments",We found a case that was confusing where a GenAI deployment became difficult to use because column filtering was turned on. GenAI doesn’t need column filtering and we should make the defualt to OFF for this target like (like we do for Time Series),"For text-gen target type by default don't filter columns
Tests","diff --git a/raptor_lake/custom_model/services/custom_task_version_service.py b/raptor_lake/custom_model/services/custom_task_version_service.py
index 8822683e3e16b1..b066a205dfea0c 100644
--- a/raptor_lake/custom_model/services/custom_task_version_service.py
+++ b/raptor_lake/custom_model/services/custom_task_version_service.py
@@ -28,6 +28,7 @@
 from common.contexts.tenant import TenantContext
 from common.contexts.tenant import provided_tenant_context
 from common.entities.custom_models.custom_task import CustomModelResourcesMixin
+from common.entities.custom_models.custom_task import CustomModelTargetType
 from common.entities.custom_models.custom_task import CustomTaskDto
 from common.entities.custom_models.custom_task import NetworkEgressPolicy
 from common.entities.customer_code_execution.custom_task_user_secrets import (
@@ -801,9 +802,15 @@ def _resolve_filter_prediction_input_value(
         elif filter_prediction_input_assign_rule == FilterPredictionInputAssignRule.DO_NOT_FILTER:
             should_filter_prediction_input = False
         else:
-            should_filter_prediction_input = (
-                current_version.should_filter_prediction_input if current_version else True
-            )
+            if current_version:
+                should_filter_prediction_input = current_version.should_filter_prediction_input
+            else:
+                should_filter_prediction_input = (
+                    False
+                    if custom_task.target_type == CustomModelTargetType.TEXT_GENERATION
+                    else True
+                )
+
         return should_filter_prediction_input
 
     def _create_version(
diff --git a/tests/backend/unit/raptor_lake/custom_model/test_custom_model_version_service.py b/tests/backend/unit/raptor_lake/custom_model/test_custom_model_version_service.py
index bcd26ed270b443..ffe9a32913bb5c 100644
--- a/tests/backend/unit/raptor_lake/custom_model/test_custom_model_version_service.py
+++ b/tests/backend/unit/raptor_lake/custom_model/test_custom_model_version_service.py
@@ -1017,18 +1017,39 @@ def test_for_non_time_series_custom_models_with_given_filter_value(
     @pytest.mark.parametrize(
         'current_version, expected_value',
         [
-            (None, True),
             (Mock(should_filter_prediction_input=True), True),
             (Mock(should_filter_prediction_input=False), False),
         ],
-        ids=['no-previous-version', 'previous-true', 'previous-false'],
+        ids=['previous-true', 'previous-false'],
     )
-    def test_for_non_time_series_from_previous_version(self, current_version, expected_value):
+    @pytest.mark.parametrize(
+        'target_type', [CustomModelTargetType.REGRESSION, CustomModelTargetType.TEXT_GENERATION]
+    )
+    def test_for_non_time_series_from_previous_version(
+        self, current_version, expected_value, target_type
+    ):
         assert (
             CustomTaskVersionService._resolve_filter_prediction_input_value(
-                custom_task=Mock(is_time_series=False),
+                custom_task=Mock(is_time_series=False, target_type=target_type),
                 current_version=current_version,
                 filter_prediction_input_assign_rule=FilterPredictionInputAssignRule.FROM_PREVIOUS,
             )
             is expected_value
         )
+
+    @pytest.mark.parametrize(
+        'target_type, expected_value',
+        [
+            (CustomModelTargetType.REGRESSION, True),
+            (CustomModelTargetType.TEXT_GENERATION, False),
+        ],
+    )
+    def test_for_non_time_series_from_previous_version_default(self, target_type, expected_value):
+        assert (
+            CustomTaskVersionService._resolve_filter_prediction_input_value(
+                custom_task=Mock(is_time_series=False, target_type=target_type),
+                current_version=None,
+                filter_prediction_input_assign_rule=FilterPredictionInputAssignRule.FROM_PREVIOUS,
+            )
+            is expected_value
+        )",,"Default for column filtering should be ""don't filter"" for TextGen deployments

We found a case that was confusing where a GenAI deployment became difficult to use because column filtering was turned on. GenAI doesn’t need column filtering and we should make the defualt to OFF for this target like (like we do for Time Series)","For text-gen target type by default don't filter columns
Tests

diff --git a/raptor_lake/custom_model/services/custom_task_version_service.py b/raptor_lake/custom_model/services/custom_task_version_service.py
index 8822683e3e16b1..b066a205dfea0c 100644
--- a/raptor_lake/custom_model/services/custom_task_version_service.py
+++ b/raptor_lake/custom_model/services/custom_task_version_service.py
@@ -28,6 +28,7 @@
 from common.contexts.tenant import TenantContext
 from common.contexts.tenant import provided_tenant_context
 from common.entities.custom_models.custom_task import CustomModelResourcesMixin
+from common.entities.custom_models.custom_task import CustomModelTargetType
 from common.entities.custom_models.custom_task import CustomTaskDto
 from common.entities.custom_models.custom_task import NetworkEgressPolicy
 from common.entities.customer_code_execution.custom_task_user_secrets import (
@@ -801,9 +802,15 @@ def _resolve_filter_prediction_input_value(
         elif filter_prediction_input_assign_rule == FilterPredictionInputAssignRule.DO_NOT_FILTER:
             should_filter_prediction_input = False
         else:
-            should_filter_prediction_input = (
-                current_version.should_filter_prediction_input if current_version else True
-            )
+            if current_version:
+                should_filter_prediction_input = current_version.should_filter_prediction_input
+            else:
+                should_filter_prediction_input = (
+                    False
+                    if custom_task.target_type == CustomModelTargetType.TEXT_GENERATION
+                    else True
+                )
+
         return should_filter_prediction_input
 
     def _create_version(
diff --git a/tests/backend/unit/raptor_lake/custom_model/test_custom_model_version_service.py b/tests/backend/unit/raptor_lake/custom_model/test_custom_model_version_service.py
index bcd26ed270b443..ffe9a32913bb5c 100644
--- a/tests/backend/unit/raptor_lake/custom_model/test_custom_model_version_service.py
+++ b/tests/backend/unit/raptor_lake/custom_model/test_custom_model_version_service.py
@@ -1017,18 +1017,39 @@ def test_for_non_time_series_custom_models_with_given_filter_value(
     @pytest.mark.parametrize(
         'current_version, expected_value',
         [
-            (None, True),
             (Mock(should_filter_prediction_input=True), True),
             (Mock(should_filter_prediction_input=False), False),
         ],
-        ids=['no-previous-version', 'previous-true', 'previous-false'],
+        ids=['previous-true', 'previous-false'],
     )
-    def test_for_non_time_series_from_previous_version(self, current_version, expected_value):
+    @pytest.mark.parametrize(
+        'target_type', [CustomModelTargetType.REGRESSION, CustomModelTargetType.TEXT_GENERATION]
+    )
+    def test_for_non_time_series_from_previous_version(
+        self, current_version, expected_value, target_type
+    ):
         assert (
             CustomTaskVersionService._resolve_filter_prediction_input_value(
-                custom_task=Mock(is_time_series=False),
+                custom_task=Mock(is_time_series=False, target_type=target_type),
                 current_version=current_version,
                 filter_prediction_input_assign_rule=FilterPredictionInputAssignRule.FROM_PREVIOUS,
             )
             is expected_value
         )
+
+    @pytest.mark.parametrize(
+        'target_type, expected_value',
+        [
+            (CustomModelTargetType.REGRESSION, True),
+            (CustomModelTargetType.TEXT_GENERATION, False),
+        ],
+    )
+    def test_for_non_time_series_from_previous_version_default(self, target_type, expected_value):
+        assert (
+            CustomTaskVersionService._resolve_filter_prediction_input_value(
+                custom_task=Mock(is_time_series=False, target_type=target_type),
+                current_version=None,
+                filter_prediction_input_assign_rule=FilterPredictionInputAssignRule.FROM_PREVIOUS,
+            )
+            is expected_value
+        )"
CCM creates DRCA baked prediction servers by defaultlt,"Since DRCA is going away, we have migrated all prediction servers from DRCA baked AMI to packer-baked AMI. Unfortunately, we forgot to change the default, and hence prediction servers that have been launched on SaaS in the past 2 weeks still use DRCA baked AMI.

We have to prevent this from happening by making sure that packer-based AMI is what we use when (1) create a new prediction server, and (2) when we upgrade prediction server to the latest artifact.","Use latest BASE_AMI unless
artifactId is specified.","diff --git a/orm_next/api/mutations.py b/orm_next/api/mutations.py
index 4c51b9b6..94047a89 100644
--- a/orm_next/api/mutations.py
+++ b/orm_next/api/mutations.py
@@ -161,10 +161,8 @@ def _get_owner(owner_data=None):
         return owner
 
     @staticmethod
-    def _get_artifact(artifact_id=None, kind=ArtifactType.AMI):
-        """"""If artifact is not passed explicitly, default to the latest one available
-        of the specified kind.
-        """"""
+    def _get_artifact(artifact_id=None):
+        """"""If artifact is not passed explicitly, default to the latest available.""""""
         artifacts_loader = inject.instance(loaders.ArtifactsLoader)
         if artifact_id is not None:
             artifact = artifacts_loader.get_matched(id=artifact_id)
@@ -174,7 +172,7 @@ def _get_artifact(artifact_id=None, kind=ArtifactType.AMI):
                     message=""There is no artifact:id {}"".format(artifact_id),
                 )
         else:
-            artifact = artifacts_loader.get_latest(kind)
+            artifact = artifacts_loader.get_latest()
 
         return artifact
 
@@ -283,10 +281,7 @@ def clone_cluster(cls, existing_cluster, artifact_id=None):
             ""cloned_from_cluster_id"": str(existing_cluster.id),
         }
 
-        artifact = CreateCluster._get_artifact(
-            artifact_id,
-            CloneCluster._get_kind(existing_cluster.artifact_id)
-        )
+        artifact = CreateCluster._get_artifact(artifact_id)
         owner = CreateCluster._get_owner({""id"": existing_cluster.owner_id})
 
         return CreateCluster.create_cluster(
diff --git a/orm_next/services/loaders.py b/orm_next/services/loaders.py
index 3fc5bb8e..0411573a 100644
--- a/orm_next/services/loaders.py
+++ b/orm_next/services/loaders.py
@@ -721,8 +721,10 @@ def create(
 
         return self.get_matched(**spec)
 
-    def get_latest(self, kind=ArtifactType.AMI):
-        cur = self.filter(limit=1, kind=kind.value).sort(""created_at"", pymongo.DESCENDING)
+    def get_latest(self):
+        # The new (BASE_AMI) artifact type has been well tested by now. Whenever
+        # we want the latest ""artifact"", we should return the BASE_AMI one.
+        cur = self.filter(limit=1, kind=ArtifactType.BASE_AMI.value).sort(""created_at"", pymongo.DESCENDING)
         artifacts = [self.managed_type.from_mongo(**entry) for entry in cur]
         return artifacts[0] if len(artifacts) else None
 
diff --git a/tests/api/test_api_schema.py b/tests/api/test_api_schema.py
index 97a1898d..ade9fb78 100644
--- a/tests/api/test_api_schema.py
+++ b/tests/api/test_api_schema.py
@@ -1083,7 +1083,7 @@ def test_set_envvars_bad_envvars(
 @pytest.mark.usefixtures('mocked_ec2')
 @pytest.mark.parametrize(""kind"", [base.ArtifactType.AMI, base.ArtifactType.BASE_AMI])
 def test_clone_cluster(
-    graphql_client, celery_app_with_tasks, clusters_with_owners, autoscaling_create_mock, kind
+    graphql_client, celery_app_with_tasks, clusters_with_owners, autoscaling_create_mock, kind, default_base_artifact
 ):
     c_loader = inject.instance(loaders.ClustersLoader)
 
@@ -1127,25 +1127,24 @@ def test_clone_cluster(
     assert new_cluster_metadata[""foo""] == ""bar""
 
     # check that artifact version defaults to the latest available
-    artifacts_loader = inject.instance(loaders.ArtifactsLoader)
-    assert new_cluster[""artifact""][""artifact""] == artifacts_loader.get_latest(kind).artifact
+    assert new_cluster[""artifact""][""artifact""] == default_base_artifact.artifact
     assert autoscaling_create_mock.called_once
 
 
 @pytest.mark.usefixtures('mocked_ec2')
 @pytest.mark.parametrize(""old_kind"", [base.ArtifactType.AMI, base.ArtifactType.BASE_AMI])
-@pytest.mark.parametrize(""new_kind"", [base.ArtifactType.AMI, base.ArtifactType.BASE_AMI])
 def test_clone_cluster_with_artifact(
     graphql_client,
     celery_app_with_tasks,
     clusters_with_owners,
     autoscaling_create_mock,
     old_kind,
-    new_kind,
+    each_opt_default_artifact,
+    default_base_artifact,
 ):
     # use some artifact for deployment
     art_loader = inject.instance(loaders.ArtifactsLoader)
-    expected_artifact = art_loader.get_latest(new_kind)
+    expected_artifact = each_opt_default_artifact or default_base_artifact
 
     # a test cluster (with some nodes) to be cloned
     original_cluster = clusters_with_owners[""clusters_by_kind""][old_kind][0]
@@ -1223,10 +1222,11 @@ def test_clone_all_clusters(
     clusters_with_owners_and_balancers,
     artifact,
     autoscaling_create_mock,
+    default_artifact,
+    default_base_artifact,
 ):
-    a_loader = inject.instance(loaders.ArtifactsLoader)
-    latest_artifact_id = a_loader.get_latest(base.ArtifactType.AMI).id
-    latest_base_artifact_id = a_loader.get_latest(base.ArtifactType.BASE_AMI).id
+    latest_artifact_id = default_artifact.id
+    latest_base_artifact_id = default_base_artifact.id
 
     # check that we have 3 groups each with one cluster before executing the mutation
     orig_groups = h._get_resource_groups(graphql_client)[""data""][""resourceGroups""]
@@ -1281,8 +1281,15 @@ def test_clone_all_clusters(
 
     # ensure clusters are using the expected artifacts
     artifact_counts = Counter([c.artifact_id for c in active_clusters])
-    assert artifact_counts[latest_artifact_id] == 4
-    assert artifact_counts[latest_base_artifact_id] == 2
+
+    # When artifacts are not specified, the kind=AMI clusters got clonned
+    # as kind=BASE_AMI
+    if artifact == ""implicit-latest"":
+        assert artifact_counts[latest_artifact_id] == 2
+        assert artifact_counts[latest_base_artifact_id] == 4
+    elif artifact == ""explicit-latest"":
+        assert artifact_counts[latest_artifact_id] == 4
+        assert artifact_counts[latest_base_artifact_id] == 2
 
 
 @pytest.mark.usefixtures('mocked_ec2')
@@ -3521,20 +3528,15 @@ def test_tasks_resource_link(
 @pytest.mark.usefixtures(""test_app"")
 def test_artifacts_latest():
     art_loader = inject.instance(loaders.ArtifactsLoader)
-    art0 = art_loader.create(""some_artifact0"", base.ArtifactType.AMI)
+    _ = art_loader.create(""some_artifact0"", base.ArtifactType.AMI)
     base_art0 = art_loader.create(""some_base_artifact0"", base.ArtifactType.BASE_AMI,
                                   dr_cluster_id=""cluster1"")
     sleep(1)  # ensure distinct created_at values
-    art1 = art_loader.create(""some_artifact1"", base.ArtifactType.AMI)
+    _ = art_loader.create(""some_artifact1"", base.ArtifactType.AMI)
     base_art1 = art_loader.create(""some_base_artifact1"", base.ArtifactType.BASE_AMI,
                                   dr_cluster_id=""cluster1"")
 
-    latest_art = art_loader.get_latest(base.ArtifactType.AMI)
-
-    assert art0.created_at < art1.created_at
-    assert latest_art.id == art1.id
-
-    latest_base_art = art_loader.get_latest(base.ArtifactType.BASE_AMI)
+    latest_base_art = art_loader.get_latest()
 
     assert base_art0.created_at < base_art1.created_at
     assert latest_base_art.id == base_art1.id
@@ -3545,7 +3547,7 @@ def test_default_artifact(
     graphql_client,
     celery_app_with_tasks,
     resource_owner,
-    default_artifact,
+    default_base_artifact,
 ):
     clusters_loader = loaders.ClustersLoader(types.Cluster)
     # WHEN
@@ -3570,7 +3572,7 @@ def test_default_artifact(
     cluster_id = rv[""data""][""createCluster""][""cluster""][""id""]
 
     cluster = clusters_loader.get_matched(id=cluster_id)
-    assert cluster.artifact_id == default_artifact.id
+    assert cluster.artifact_id == default_base_artifact.id
 
 
 @pytest.mark.usefixtures(""default_artifact"", ""default_base_artifact"")
diff --git a/tests/api/test_resource_groups_api.py b/tests/api/test_resource_groups_api.py
index 2f606e67..f6069a98 100644
--- a/tests/api/test_resource_groups_api.py
+++ b/tests/api/test_resource_groups_api.py
@@ -58,7 +58,7 @@ def test_create_resource_group_bad_envvars(
     graphql_client,
     celery_app_with_tasks,
     resource_owner,
-    default_artifact,
+    default_base_artifact,
     envvar
 ):
     # DO
@@ -96,9 +96,9 @@ def test_create_resource_group_good(
     celery_app_with_tasks,
     resource_owner,
     each_opt_default_artifact,
-    default_artifact
+    default_base_artifact
 ):
-    expected_artifact = each_opt_default_artifact or default_artifact
+    expected_artifact = each_opt_default_artifact or default_base_artifact
     provider_name = ""noop""
     clusters_loader = loaders.ClustersLoader(types.Cluster)
 
@@ -150,7 +150,7 @@ def test_create_resource_group(
     graphql_client,
     celery_app_with_tasks,
     resource_owner,
-    default_artifact,
+    default_base_artifact,
     snapshot,
     provider,
 ):
@@ -228,7 +228,7 @@ def test_create_resource_group_nondefault_dns_zone(
     graphql_client,
     celery_app_with_tasks,
     resource_owner,
-    default_artifact,
+    default_base_artifact,
     snapshot,
     provider,
 ):
@@ -796,7 +796,7 @@ def test_create_resource_group_custom_policy_name(
     graphql_client,
     celery_app_with_tasks,
     resource_owner,
-    default_artifact,
+    default_base_artifact,
     snapshot,
     provider,
 ):
diff --git a/tests_e2e/test_artifacts.py b/tests_e2e/test_artifacts.py
index f30d3385..182ff890 100644
--- a/tests_e2e/test_artifacts.py
+++ b/tests_e2e/test_artifacts.py
@@ -83,7 +83,7 @@ def test_add_artifact(gq_client, cache):
         ""artifact"": ami[""ImageId""],
         ""description"": artifact_desc,
         ""drClusterId"": ""ccm-e2e"",
-        ""kind"": ""AMI"",
+        ""kind"": ""BASE_AMI"",
     }
 
     rv = gq_client.execute(gql(ARTIFACT_MUTATION), variable_values=vars)",,"CCM creates DRCA baked prediction servers by defaultlt

Since DRCA is going away, we have migrated all prediction servers from DRCA baked AMI to packer-baked AMI. Unfortunately, we forgot to change the default, and hence prediction servers that have been launched on SaaS in the past 2 weeks still use DRCA baked AMI.

We have to prevent this from happening by making sure that packer-based AMI is what we use when (1) create a new prediction server, and (2) when we upgrade prediction server to the latest artifact.","Use latest BASE_AMI unless
artifactId is specified.

diff --git a/orm_next/api/mutations.py b/orm_next/api/mutations.py
index 4c51b9b6..94047a89 100644
--- a/orm_next/api/mutations.py
+++ b/orm_next/api/mutations.py
@@ -161,10 +161,8 @@ def _get_owner(owner_data=None):
         return owner
 
     @staticmethod
-    def _get_artifact(artifact_id=None, kind=ArtifactType.AMI):
-        """"""If artifact is not passed explicitly, default to the latest one available
-        of the specified kind.
-        """"""
+    def _get_artifact(artifact_id=None):
+        """"""If artifact is not passed explicitly, default to the latest available.""""""
         artifacts_loader = inject.instance(loaders.ArtifactsLoader)
         if artifact_id is not None:
             artifact = artifacts_loader.get_matched(id=artifact_id)
@@ -174,7 +172,7 @@ def _get_artifact(artifact_id=None, kind=ArtifactType.AMI):
                     message=""There is no artifact:id {}"".format(artifact_id),
                 )
         else:
-            artifact = artifacts_loader.get_latest(kind)
+            artifact = artifacts_loader.get_latest()
 
         return artifact
 
@@ -283,10 +281,7 @@ def clone_cluster(cls, existing_cluster, artifact_id=None):
             ""cloned_from_cluster_id"": str(existing_cluster.id),
         }
 
-        artifact = CreateCluster._get_artifact(
-            artifact_id,
-            CloneCluster._get_kind(existing_cluster.artifact_id)
-        )
+        artifact = CreateCluster._get_artifact(artifact_id)
         owner = CreateCluster._get_owner({""id"": existing_cluster.owner_id})
 
         return CreateCluster.create_cluster(
diff --git a/orm_next/services/loaders.py b/orm_next/services/loaders.py
index 3fc5bb8e..0411573a 100644
--- a/orm_next/services/loaders.py
+++ b/orm_next/services/loaders.py
@@ -721,8 +721,10 @@ def create(
 
         return self.get_matched(**spec)
 
-    def get_latest(self, kind=ArtifactType.AMI):
-        cur = self.filter(limit=1, kind=kind.value).sort(""created_at"", pymongo.DESCENDING)
+    def get_latest(self):
+        # The new (BASE_AMI) artifact type has been well tested by now. Whenever
+        # we want the latest ""artifact"", we should return the BASE_AMI one.
+        cur = self.filter(limit=1, kind=ArtifactType.BASE_AMI.value).sort(""created_at"", pymongo.DESCENDING)
         artifacts = [self.managed_type.from_mongo(**entry) for entry in cur]
         return artifacts[0] if len(artifacts) else None
 
diff --git a/tests/api/test_api_schema.py b/tests/api/test_api_schema.py
index 97a1898d..ade9fb78 100644
--- a/tests/api/test_api_schema.py
+++ b/tests/api/test_api_schema.py
@@ -1083,7 +1083,7 @@ def test_set_envvars_bad_envvars(
 @pytest.mark.usefixtures('mocked_ec2')
 @pytest.mark.parametrize(""kind"", [base.ArtifactType.AMI, base.ArtifactType.BASE_AMI])
 def test_clone_cluster(
-    graphql_client, celery_app_with_tasks, clusters_with_owners, autoscaling_create_mock, kind
+    graphql_client, celery_app_with_tasks, clusters_with_owners, autoscaling_create_mock, kind, default_base_artifact
 ):
     c_loader = inject.instance(loaders.ClustersLoader)
 
@@ -1127,25 +1127,24 @@ def test_clone_cluster(
     assert new_cluster_metadata[""foo""] == ""bar""
 
     # check that artifact version defaults to the latest available
-    artifacts_loader = inject.instance(loaders.ArtifactsLoader)
-    assert new_cluster[""artifact""][""artifact""] == artifacts_loader.get_latest(kind).artifact
+    assert new_cluster[""artifact""][""artifact""] == default_base_artifact.artifact
     assert autoscaling_create_mock.called_once
 
 
 @pytest.mark.usefixtures('mocked_ec2')
 @pytest.mark.parametrize(""old_kind"", [base.ArtifactType.AMI, base.ArtifactType.BASE_AMI])
-@pytest.mark.parametrize(""new_kind"", [base.ArtifactType.AMI, base.ArtifactType.BASE_AMI])
 def test_clone_cluster_with_artifact(
     graphql_client,
     celery_app_with_tasks,
     clusters_with_owners,
     autoscaling_create_mock,
     old_kind,
-    new_kind,
+    each_opt_default_artifact,
+    default_base_artifact,
 ):
     # use some artifact for deployment
     art_loader = inject.instance(loaders.ArtifactsLoader)
-    expected_artifact = art_loader.get_latest(new_kind)
+    expected_artifact = each_opt_default_artifact or default_base_artifact
 
     # a test cluster (with some nodes) to be cloned
     original_cluster = clusters_with_owners[""clusters_by_kind""][old_kind][0]
@@ -1223,10 +1222,11 @@ def test_clone_all_clusters(
     clusters_with_owners_and_balancers,
     artifact,
     autoscaling_create_mock,
+    default_artifact,
+    default_base_artifact,
 ):
-    a_loader = inject.instance(loaders.ArtifactsLoader)
-    latest_artifact_id = a_loader.get_latest(base.ArtifactType.AMI).id
-    latest_base_artifact_id = a_loader.get_latest(base.ArtifactType.BASE_AMI).id
+    latest_artifact_id = default_artifact.id
+    latest_base_artifact_id = default_base_artifact.id
 
     # check that we have 3 groups each with one cluster before executing the mutation
     orig_groups = h._get_resource_groups(graphql_client)[""data""][""resourceGroups""]
@@ -1281,8 +1281,15 @@ def test_clone_all_clusters(
 
     # ensure clusters are using the expected artifacts
     artifact_counts = Counter([c.artifact_id for c in active_clusters])
-    assert artifact_counts[latest_artifact_id] == 4
-    assert artifact_counts[latest_base_artifact_id] == 2
+
+    # When artifacts are not specified, the kind=AMI clusters got clonned
+    # as kind=BASE_AMI
+    if artifact == ""implicit-latest"":
+        assert artifact_counts[latest_artifact_id] == 2
+        assert artifact_counts[latest_base_artifact_id] == 4
+    elif artifact == ""explicit-latest"":
+        assert artifact_counts[latest_artifact_id] == 4
+        assert artifact_counts[latest_base_artifact_id] == 2
 
 
 @pytest.mark.usefixtures('mocked_ec2')
@@ -3521,20 +3528,15 @@ def test_tasks_resource_link(
 @pytest.mark.usefixtures(""test_app"")
 def test_artifacts_latest():
     art_loader = inject.instance(loaders.ArtifactsLoader)
-    art0 = art_loader.create(""some_artifact0"", base.ArtifactType.AMI)
+    _ = art_loader.create(""some_artifact0"", base.ArtifactType.AMI)
     base_art0 = art_loader.create(""some_base_artifact0"", base.ArtifactType.BASE_AMI,
                                   dr_cluster_id=""cluster1"")
     sleep(1)  # ensure distinct created_at values
-    art1 = art_loader.create(""some_artifact1"", base.ArtifactType.AMI)
+    _ = art_loader.create(""some_artifact1"", base.ArtifactType.AMI)
     base_art1 = art_loader.create(""some_base_artifact1"", base.ArtifactType.BASE_AMI,
                                   dr_cluster_id=""cluster1"")
 
-    latest_art = art_loader.get_latest(base.ArtifactType.AMI)
-
-    assert art0.created_at < art1.created_at
-    assert latest_art.id == art1.id
-
-    latest_base_art = art_loader.get_latest(base.ArtifactType.BASE_AMI)
+    latest_base_art = art_loader.get_latest()
 
     assert base_art0.created_at < base_art1.created_at
     assert latest_base_art.id == base_art1.id
@@ -3545,7 +3547,7 @@ def test_default_artifact(
     graphql_client,
     celery_app_with_tasks,
     resource_owner,
-    default_artifact,
+    default_base_artifact,
 ):
     clusters_loader = loaders.ClustersLoader(types.Cluster)
     # WHEN
@@ -3570,7 +3572,7 @@ def test_default_artifact(
     cluster_id = rv[""data""][""createCluster""][""cluster""][""id""]
 
     cluster = clusters_loader.get_matched(id=cluster_id)
-    assert cluster.artifact_id == default_artifact.id
+    assert cluster.artifact_id == default_base_artifact.id
 
 
 @pytest.mark.usefixtures(""default_artifact"", ""default_base_artifact"")
diff --git a/tests/api/test_resource_groups_api.py b/tests/api/test_resource_groups_api.py
index 2f606e67..f6069a98 100644
--- a/tests/api/test_resource_groups_api.py
+++ b/tests/api/test_resource_groups_api.py
@@ -58,7 +58,7 @@ def test_create_resource_group_bad_envvars(
     graphql_client,
     celery_app_with_tasks,
     resource_owner,
-    default_artifact,
+    default_base_artifact,
     envvar
 ):
     # DO
@@ -96,9 +96,9 @@ def test_create_resource_group_good(
     celery_app_with_tasks,
     resource_owner,
     each_opt_default_artifact,
-    default_artifact
+    default_base_artifact
 ):
-    expected_artifact = each_opt_default_artifact or default_artifact
+    expected_artifact = each_opt_default_artifact or default_base_artifact
     provider_name = ""noop""
     clusters_loader = loaders.ClustersLoader(types.Cluster)
 
@@ -150,7 +150,7 @@ def test_create_resource_group(
     graphql_client,
     celery_app_with_tasks,
     resource_owner,
-    default_artifact,
+    default_base_artifact,
     snapshot,
     provider,
 ):
@@ -228,7 +228,7 @@ def test_create_resource_group_nondefault_dns_zone(
     graphql_client,
     celery_app_with_tasks,
     resource_owner,
-    default_artifact,
+    default_base_artifact,
     snapshot,
     provider,
 ):
@@ -796,7 +796,7 @@ def test_create_resource_group_custom_policy_name(
     graphql_client,
     celery_app_with_tasks,
     resource_owner,
-    default_artifact,
+    default_base_artifact,
     snapshot,
     provider,
 ):
diff --git a/tests_e2e/test_artifacts.py b/tests_e2e/test_artifacts.py
index f30d3385..182ff890 100644
--- a/tests_e2e/test_artifacts.py
+++ b/tests_e2e/test_artifacts.py
@@ -83,7 +83,7 @@ def test_add_artifact(gq_client, cache):
         ""artifact"": ami[""ImageId""],
         ""description"": artifact_desc,
         ""drClusterId"": ""ccm-e2e"",
-        ""kind"": ""AMI"",
+        ""kind"": ""BASE_AMI"",
     }
 
     rv = gq_client.execute(gql(ARTIFACT_MUTATION), variable_values=vars)"
OpenAPI Spec Generation Doesn't Install all Dependencies,"We've seen another instance https://ci1.devinfra.drdev.io/job/DataRobot-pipelines/job/check-openapi-spec/852/ of our spec not being able to generate because dependencies have not been installed for importing. This means we can't run out spec generation job. Previously, we added the missing dependency directly into the docs `requirements.txt` file, but this isn't a good longterm solution. We should switch to installing all dependencies to ensure that we don't run into this problem again.",Run dependencies/python/update.sh before running to_openapi.py,"diff --git a/public_api/docs/Makefile b/public_api/docs/Makefile
index ccd934991bb921..1bd6ff2c04147a 100644
--- a/public_api/docs/Makefile
+++ b/public_api/docs/Makefile
@@ -52,6 +52,7 @@ install-requirements:
 	python3 -m pip install -r requirements.txt
 
 install-requirements-quantum:
+	../../dependencies/python/update.sh
 	python3 -m pip install -r requirements.txt
 
 clean:
@@ -190,12 +191,12 @@ build-docker-image:
 	sudo docker build --rm -t kyrylo/docs -f Dockerfile .
 
 OPTS    =
-openapi:
+openapi: install-requirements-quantum
 	python3 to_openapi.py $(OPTS)
 	@echo
 	@echo ""Build finished. The OpenAPI representation is in public_api/docs.""
 
-openapi-ff:  # TODO DSX-3185 Remove this target and replace with `openapi` before closing PBMP-5813.
+openapi-ff:  install-requirements-quantum # TODO DSX-3185 Remove this target and replace with `openapi` before closing PBMP-5813.
 	python3 to_openapi.py --by-feature-flags --filename public-api-ff $(OPTS)
 	@echo
 	@echo ""Build finished. The OpenAPI representation based on feature flag maturity level is in public_api/docs.""",,"OpenAPI Spec Generation Doesn't Install all Dependencies

We've seen another instance https://ci1.devinfra.drdev.io/job/DataRobot-pipelines/job/check-openapi-spec/852/ of our spec not being able to generate because dependencies have not been installed for importing. This means we can't run out spec generation job. Previously, we added the missing dependency directly into the docs `requirements.txt` file, but this isn't a good longterm solution. We should switch to installing all dependencies to ensure that we don't run into this problem again.","Run dependencies/python/update.sh before running to_openapi.py

diff --git a/public_api/docs/Makefile b/public_api/docs/Makefile
index ccd934991bb921..1bd6ff2c04147a 100644
--- a/public_api/docs/Makefile
+++ b/public_api/docs/Makefile
@@ -52,6 +52,7 @@ install-requirements:
 	python3 -m pip install -r requirements.txt
 
 install-requirements-quantum:
+	../../dependencies/python/update.sh
 	python3 -m pip install -r requirements.txt
 
 clean:
@@ -190,12 +191,12 @@ build-docker-image:
 	sudo docker build --rm -t kyrylo/docs -f Dockerfile .
 
 OPTS    =
-openapi:
+openapi: install-requirements-quantum
 	python3 to_openapi.py $(OPTS)
 	@echo
 	@echo ""Build finished. The OpenAPI representation is in public_api/docs.""
 
-openapi-ff:  # TODO DSX-3185 Remove this target and replace with `openapi` before closing PBMP-5813.
+openapi-ff:  install-requirements-quantum # TODO DSX-3185 Remove this target and replace with `openapi` before closing PBMP-5813.
 	python3 to_openapi.py --by-feature-flags --filename public-api-ff $(OPTS)
 	@echo
 	@echo ""Build finished. The OpenAPI representation based on feature flag maturity level is in public_api/docs."""
Width of minimized queue-sidebar is different depending on a project,"Not sure if you change something in CSS for sidebar, but there is a weird bug where the width depends on a left content.","Align worker controls

Adjust some css

Arevent minimized queue sidebar from fluctuating

Lint","diff --git a/client/js/workbench/usecase-model/model-info.less b/client/js/workbench/usecase-model/model-info.less
index 8f66c0c11d4e41..7bdb6d103ed3be 100644
--- a/client/js/workbench/usecase-model/model-info.less
+++ b/client/js/workbench/usecase-model/model-info.less
@@ -4,6 +4,7 @@
   display: flex;
   flex-shrink: 0;
   flex-direction: row;
+  min-height: 120px;
   background-color: var(--usecases-model-info-group-bg);
   border: 1px solid var(--usecases-model-info-group-border-color);
   border-radius: 4px;
diff --git a/client/js/workbench/usecase-model/queue-pane/queue-pane.less b/client/js/workbench/usecase-model/queue-pane/queue-pane.less
index 53a62c931dbf6f..9051687573c1aa 100644
--- a/client/js/workbench/usecase-model/queue-pane/queue-pane.less
+++ b/client/js/workbench/usecase-model/queue-pane/queue-pane.less
@@ -3,6 +3,7 @@
   padding: 0;
 
   &.minimized {
+    flex-shrink: 0;
     width: 60px;
     padding-top: @spacing-01;
 
@@ -71,6 +72,12 @@
           align-items: center;
         }
 
+        .worker-control-container {
+          display: flex;
+          align-items: center;
+          margin-top: @spacing-02;
+        }
+
         .worker-control-button {
           margin: 0 @spacing-04;
         }
diff --git a/client/js/workbench/usecase-model/queue-pane/worker-controls.js b/client/js/workbench/usecase-model/queue-pane/worker-controls.js
index 30055b01987005..e529734665cccc 100644
--- a/client/js/workbench/usecase-model/queue-pane/worker-controls.js
+++ b/client/js/workbench/usecase-model/queue-pane/worker-controls.js
@@ -50,7 +50,7 @@ export function WorkerControlsSection({
         <div className=""worker-controls"">
           <div className=""worker-control"">
             <span className=""body-secondary	worker-control-label"">{_tl`${WORKER_TYPE.CPU}`}</span>
-            <div className=""margin-top-2"">
+            <div className=""worker-control-container"">
               <Button
                 className=""worker-control-button""
                 accentType={ACCENT_TYPES.ROUND_ICON}
@@ -92,7 +92,7 @@ export function WorkerControlsSection({
           {isGPUWorkersEnabled ? (
             <div className=""worker-control"">
               <span className=""body-secondary	worker-control-label"">{_tl`${WORKER_TYPE.GPU}`}</span>
-              <div className=""margin-top-2"">
+              <div className=""worker-control-container"">
                 <Button
                   className=""worker-control-button""
                   accentType={ACCENT_TYPES.ROUND_ICON}
diff --git a/client/js/workbench/usecase-model/selected-model-pane/failed-pane/failed-pane.less b/client/js/workbench/usecase-model/selected-model-pane/failed-pane/failed-pane.less
index df8e3b0e029f6d..36e716463409c2 100644
--- a/client/js/workbench/usecase-model/selected-model-pane/failed-pane/failed-pane.less
+++ b/client/js/workbench/usecase-model/selected-model-pane/failed-pane/failed-pane.less
@@ -1,8 +1,6 @@
 @import (reference) '~less/variables';
 
 .failed-autopilot-pane {
-  margin-left: @spacing-03;
-
   .log-container {
     max-height: 50%;
     overflow: auto;",,"Width of minimized queue-sidebar is different depending on a project

Not sure if you change something in CSS for sidebar, but there is a weird bug where the width depends on a left content.","Align worker controls

Adjust some css

Arevent minimized queue sidebar from fluctuating

Lint

diff --git a/client/js/workbench/usecase-model/model-info.less b/client/js/workbench/usecase-model/model-info.less
index 8f66c0c11d4e41..7bdb6d103ed3be 100644
--- a/client/js/workbench/usecase-model/model-info.less
+++ b/client/js/workbench/usecase-model/model-info.less
@@ -4,6 +4,7 @@
   display: flex;
   flex-shrink: 0;
   flex-direction: row;
+  min-height: 120px;
   background-color: var(--usecases-model-info-group-bg);
   border: 1px solid var(--usecases-model-info-group-border-color);
   border-radius: 4px;
diff --git a/client/js/workbench/usecase-model/queue-pane/queue-pane.less b/client/js/workbench/usecase-model/queue-pane/queue-pane.less
index 53a62c931dbf6f..9051687573c1aa 100644
--- a/client/js/workbench/usecase-model/queue-pane/queue-pane.less
+++ b/client/js/workbench/usecase-model/queue-pane/queue-pane.less
@@ -3,6 +3,7 @@
   padding: 0;
 
   &.minimized {
+    flex-shrink: 0;
     width: 60px;
     padding-top: @spacing-01;
 
@@ -71,6 +72,12 @@
           align-items: center;
         }
 
+        .worker-control-container {
+          display: flex;
+          align-items: center;
+          margin-top: @spacing-02;
+        }
+
         .worker-control-button {
           margin: 0 @spacing-04;
         }
diff --git a/client/js/workbench/usecase-model/queue-pane/worker-controls.js b/client/js/workbench/usecase-model/queue-pane/worker-controls.js
index 30055b01987005..e529734665cccc 100644
--- a/client/js/workbench/usecase-model/queue-pane/worker-controls.js
+++ b/client/js/workbench/usecase-model/queue-pane/worker-controls.js
@@ -50,7 +50,7 @@ export function WorkerControlsSection({
         <div className=""worker-controls"">
           <div className=""worker-control"">
             <span className=""body-secondary	worker-control-label"">{_tl`${WORKER_TYPE.CPU}`}</span>
-            <div className=""margin-top-2"">
+            <div className=""worker-control-container"">
               <Button
                 className=""worker-control-button""
                 accentType={ACCENT_TYPES.ROUND_ICON}
@@ -92,7 +92,7 @@ export function WorkerControlsSection({
           {isGPUWorkersEnabled ? (
             <div className=""worker-control"">
               <span className=""body-secondary	worker-control-label"">{_tl`${WORKER_TYPE.GPU}`}</span>
-              <div className=""margin-top-2"">
+              <div className=""worker-control-container"">
                 <Button
                   className=""worker-control-button""
                   accentType={ACCENT_TYPES.ROUND_ICON}
diff --git a/client/js/workbench/usecase-model/selected-model-pane/failed-pane/failed-pane.less b/client/js/workbench/usecase-model/selected-model-pane/failed-pane/failed-pane.less
index df8e3b0e029f6d..36e716463409c2 100644
--- a/client/js/workbench/usecase-model/selected-model-pane/failed-pane/failed-pane.less
+++ b/client/js/workbench/usecase-model/selected-model-pane/failed-pane/failed-pane.less
@@ -1,8 +1,6 @@
 @import (reference) '~less/variables';
 
 .failed-autopilot-pane {
-  margin-left: @spacing-03;
-
   .log-container {
     max-height: 50%;
     overflow: auto;"
"""Actions"" button is not visible in sidebar view",Users can’t access “Registered model version” info,Ensure the row actions are always rendered,"diff --git a/client/js/registry/registered-models/registered-models.js b/client/js/registry/registered-models/registered-models.js
index 9b04c74f01a35e..8ea1a32ccd909d 100644
--- a/client/js/registry/registered-models/registered-models.js
+++ b/client/js/registry/registered-models/registered-models.js
@@ -433,10 +433,7 @@ export function RegisteredModels() {
                   expandedContentClassName:
                     'registered-models-table-expanded-row',
                 }}
-                // When the table is in collapsed mode, we don't want to show the actions
-                // dropdown as the same actions exist as buttons on the main view unless
-                // the details panel is only expanded through the actions menu
-                getRowActions={isTableFullWidth ? getRowActions : undefined}
+                getRowActions={getRowActions}
                 tableAriaLabel={_t('Registered models table')}
                 cellAriaLabel={_t('Click to open registered model details')}
               />
diff --git a/client/js/registry/registered-models/registered-models.test.js b/client/js/registry/registered-models/registered-models.test.js
index e1d42ea605ab64..f2010f5d373292 100644
--- a/client/js/registry/registered-models/registered-models.test.js
+++ b/client/js/registry/registered-models/registered-models.test.js
@@ -431,30 +431,6 @@ describe('RegisteredModels', function () {
   });
 
   describe('given a route other than the registered models index', function () {
-    describe('and a loading state', function () {
-      it('should render the narrow registered models table skeleton without the row actions column', function () {
-        jest
-          .spyOn(RegisteredModelsHooks, 'useRegisteredModels')
-          .mockReturnValue(HOOK_STATES.REGISTERED_MODELS.LOADING);
-
-        renderComponent({
-          initialEntries: [
-            getFullUrl(
-              routes.app.registry.registeredModels.registeredModel.info,
-              { registeredModelId: TEST_REGISTERED_MODEL.id }
-            ),
-          ],
-        });
-
-        expect(
-          screen.getByTestId('registered-models-pane-narrow')
-        ).toBeInTheDocument();
-        expect(
-          screen.getAllByTestId('react-table-skeleton-loading')
-        ).toHaveLength(120); // 20 rows * 6 columns
-      });
-    });
-
     describe('and a success state with a list of registered models', function () {
       it('should render a collapsible sidebar around the table', function () {
         jest",,"""Actions"" button is not visible in sidebar view

Users can’t access “Registered model version” info","Ensure the row actions are always rendered

diff --git a/client/js/registry/registered-models/registered-models.js b/client/js/registry/registered-models/registered-models.js
index 9b04c74f01a35e..8ea1a32ccd909d 100644
--- a/client/js/registry/registered-models/registered-models.js
+++ b/client/js/registry/registered-models/registered-models.js
@@ -433,10 +433,7 @@ export function RegisteredModels() {
                   expandedContentClassName:
                     'registered-models-table-expanded-row',
                 }}
-                // When the table is in collapsed mode, we don't want to show the actions
-                // dropdown as the same actions exist as buttons on the main view unless
-                // the details panel is only expanded through the actions menu
-                getRowActions={isTableFullWidth ? getRowActions : undefined}
+                getRowActions={getRowActions}
                 tableAriaLabel={_t('Registered models table')}
                 cellAriaLabel={_t('Click to open registered model details')}
               />
diff --git a/client/js/registry/registered-models/registered-models.test.js b/client/js/registry/registered-models/registered-models.test.js
index e1d42ea605ab64..f2010f5d373292 100644
--- a/client/js/registry/registered-models/registered-models.test.js
+++ b/client/js/registry/registered-models/registered-models.test.js
@@ -431,30 +431,6 @@ describe('RegisteredModels', function () {
   });
 
   describe('given a route other than the registered models index', function () {
-    describe('and a loading state', function () {
-      it('should render the narrow registered models table skeleton without the row actions column', function () {
-        jest
-          .spyOn(RegisteredModelsHooks, 'useRegisteredModels')
-          .mockReturnValue(HOOK_STATES.REGISTERED_MODELS.LOADING);
-
-        renderComponent({
-          initialEntries: [
-            getFullUrl(
-              routes.app.registry.registeredModels.registeredModel.info,
-              { registeredModelId: TEST_REGISTERED_MODEL.id }
-            ),
-          ],
-        });
-
-        expect(
-          screen.getByTestId('registered-models-pane-narrow')
-        ).toBeInTheDocument();
-        expect(
-          screen.getAllByTestId('react-table-skeleton-loading')
-        ).toHaveLength(120); // 20 rows * 6 columns
-      });
-    });
-
     describe('and a success state with a list of registered models', function () {
       it('should render a collapsible sidebar around the table', function () {
         jest"
Avoid feature impact and rcp jobs during timeseries/text-generation custom model replacement,"The issue is that there is an attempt to submit a feature impact and reason codes jobs during model replacement, which is not supported for time-series and text-generation custom models.","Skip feature impact and rcp jobs during model replacement for time-series and text generation custom models.
Small refactoring - rename method name to better reflect its meaning
Unit tests","diff --git a/MLOps/mmm/tests/backend/unit/predictions_monitor/services/lifecycle/mixins/test_custom_model.py b/MLOps/mmm/tests/backend/unit/predictions_monitor/services/lifecycle/mixins/test_custom_model.py
index 992b4fd3ea28ab..69f5159e7cf87b 100644
--- a/MLOps/mmm/tests/backend/unit/predictions_monitor/services/lifecycle/mixins/test_custom_model.py
+++ b/MLOps/mmm/tests/backend/unit/predictions_monitor/services/lifecycle/mixins/test_custom_model.py
@@ -156,7 +156,7 @@ class TestSubmitCustomModelPreReplacementJobs:
     @pytest.fixture
     def mocked_submit_variable_importance_job(self):
         with mock.patch.object(
-            CustomModelTaskMixin, '_submit_custom_model_variable_importance_job'
+            CustomModelTaskMixin, '_submit_custom_model_feature_impact_job'
         ) as mocked:
             yield mocked
 
@@ -343,3 +343,39 @@ def test_jobs_submission_success(
             deployment_id=deployment_id,
             model_package_id=model_package.id,
         )
+
+    @pytest.mark.usefixtures('mocked_reason_code_preview_data_not_exists')
+    @pytest.mark.parametrize(
+        'target_type, is_time_series',
+        [
+            (TargetType.REGRESSION, True),
+            (TargetType.BINARY, True),
+            (TargetType.TEXT_GENERATION, False),
+        ],
+        ids=['time-series-regression', 'time-series-binary', 'text-generation'],
+    )
+    def test_jobs_are_not_submitted_for_time_series_and_text_generation(
+        self,
+        model_package,
+        mocked_start_feature_impact_job,
+        mocked_submit_custom_model_rcp_job,
+        target_type,
+        is_time_series,
+    ):
+        """"""
+        Test that feature impact and reason codes jobs are not submitted for either
+        time-series or text generation custom models.
+        """"""
+
+        model_package.model_execution_type = ModelDeploymentType.CUSTOM_INFERENCE_MODEL
+        model_package.target.type = target_type
+        model_package.model_kind.is_time_series = is_time_series
+
+        deployment = mock.Mock(id=ObjectId())
+        mixin = CustomModelTaskMixin(deployment, user_id=ObjectId())
+        custom_model_job_qids = mixin.submit_custom_model_pre_replacement_jobs(
+            model_package, start_lrs_qids=[], pid_for_queue=ObjectId()
+        )
+        assert custom_model_job_qids is None
+        mocked_start_feature_impact_job.assert_not_called()
+        mocked_submit_custom_model_rcp_job.assert_not_called()
diff --git a/predictions_monitor/services/lifecycle/mixins/custom_model.py b/predictions_monitor/services/lifecycle/mixins/custom_model.py
index 0c71fe7892dbfd..36aa106e92efa8 100644
--- a/predictions_monitor/services/lifecycle/mixins/custom_model.py
+++ b/predictions_monitor/services/lifecycle/mixins/custom_model.py
@@ -20,7 +20,6 @@
 
 from bson import ObjectId
 
-from common.engine import TargetType
 from common.entities.model_deploy import ModelDeployment
 from common.entities.model_deploy import ModelDeploymentType
 from common.entities.model_package import ModelPackage
@@ -177,19 +176,17 @@ def submit_custom_model_pre_replacement_jobs(
         if (
             model_package.model_execution_type == ModelDeploymentType.CUSTOM_INFERENCE_MODEL
             and model_package.source_meta.project_id
+            and model_package.is_unified_predictions_supported
             and not model_package.expects_external_monitoring_data
-            and not model_package.target.type == TargetType.TEXT_GENERATION
         ):
             # if new model is custom model, submit variable importance job
-            variable_importance_qid = self._submit_custom_model_variable_importance_job(
+            variable_importance_qid = self._submit_custom_model_feature_impact_job(
                 model_package, start_lrs_qids, pid_for_queue
             )
             self._submit_custom_model_rcp_job(model_package, pid_for_queue, variable_importance_qid)
         return [variable_importance_qid] if variable_importance_qid else None
 
-    def _submit_custom_model_variable_importance_job(
-        self, model_package, start_lrs_qids, pid_for_queue
-    ):
+    def _submit_custom_model_feature_impact_job(self, model_package, start_lrs_qids, pid_for_queue):
         # type: (ModelPackage, List[int], ObjectId) -> Optional[int]
         """"""Start variable importance for custom inference model.
 ",,"Avoid feature impact and rcp jobs during timeseries/text-generation custom model replacement

The issue is that there is an attempt to submit a feature impact and reason codes jobs during model replacement, which is not supported for time-series and text-generation custom models.","Skip feature impact and rcp jobs during model replacement for time-series and text generation custom models.
Small refactoring - rename method name to better reflect its meaning
Unit tests

diff --git a/MLOps/mmm/tests/backend/unit/predictions_monitor/services/lifecycle/mixins/test_custom_model.py b/MLOps/mmm/tests/backend/unit/predictions_monitor/services/lifecycle/mixins/test_custom_model.py
index 992b4fd3ea28ab..69f5159e7cf87b 100644
--- a/MLOps/mmm/tests/backend/unit/predictions_monitor/services/lifecycle/mixins/test_custom_model.py
+++ b/MLOps/mmm/tests/backend/unit/predictions_monitor/services/lifecycle/mixins/test_custom_model.py
@@ -156,7 +156,7 @@ class TestSubmitCustomModelPreReplacementJobs:
     @pytest.fixture
     def mocked_submit_variable_importance_job(self):
         with mock.patch.object(
-            CustomModelTaskMixin, '_submit_custom_model_variable_importance_job'
+            CustomModelTaskMixin, '_submit_custom_model_feature_impact_job'
         ) as mocked:
             yield mocked
 
@@ -343,3 +343,39 @@ def test_jobs_submission_success(
             deployment_id=deployment_id,
             model_package_id=model_package.id,
         )
+
+    @pytest.mark.usefixtures('mocked_reason_code_preview_data_not_exists')
+    @pytest.mark.parametrize(
+        'target_type, is_time_series',
+        [
+            (TargetType.REGRESSION, True),
+            (TargetType.BINARY, True),
+            (TargetType.TEXT_GENERATION, False),
+        ],
+        ids=['time-series-regression', 'time-series-binary', 'text-generation'],
+    )
+    def test_jobs_are_not_submitted_for_time_series_and_text_generation(
+        self,
+        model_package,
+        mocked_start_feature_impact_job,
+        mocked_submit_custom_model_rcp_job,
+        target_type,
+        is_time_series,
+    ):
+        """"""
+        Test that feature impact and reason codes jobs are not submitted for either
+        time-series or text generation custom models.
+        """"""
+
+        model_package.model_execution_type = ModelDeploymentType.CUSTOM_INFERENCE_MODEL
+        model_package.target.type = target_type
+        model_package.model_kind.is_time_series = is_time_series
+
+        deployment = mock.Mock(id=ObjectId())
+        mixin = CustomModelTaskMixin(deployment, user_id=ObjectId())
+        custom_model_job_qids = mixin.submit_custom_model_pre_replacement_jobs(
+            model_package, start_lrs_qids=[], pid_for_queue=ObjectId()
+        )
+        assert custom_model_job_qids is None
+        mocked_start_feature_impact_job.assert_not_called()
+        mocked_submit_custom_model_rcp_job.assert_not_called()
diff --git a/predictions_monitor/services/lifecycle/mixins/custom_model.py b/predictions_monitor/services/lifecycle/mixins/custom_model.py
index 0c71fe7892dbfd..36aa106e92efa8 100644
--- a/predictions_monitor/services/lifecycle/mixins/custom_model.py
+++ b/predictions_monitor/services/lifecycle/mixins/custom_model.py
@@ -20,7 +20,6 @@
 
 from bson import ObjectId
 
-from common.engine import TargetType
 from common.entities.model_deploy import ModelDeployment
 from common.entities.model_deploy import ModelDeploymentType
 from common.entities.model_package import ModelPackage
@@ -177,19 +176,17 @@ def submit_custom_model_pre_replacement_jobs(
         if (
             model_package.model_execution_type == ModelDeploymentType.CUSTOM_INFERENCE_MODEL
             and model_package.source_meta.project_id
+            and model_package.is_unified_predictions_supported
             and not model_package.expects_external_monitoring_data
-            and not model_package.target.type == TargetType.TEXT_GENERATION
         ):
             # if new model is custom model, submit variable importance job
-            variable_importance_qid = self._submit_custom_model_variable_importance_job(
+            variable_importance_qid = self._submit_custom_model_feature_impact_job(
                 model_package, start_lrs_qids, pid_for_queue
             )
             self._submit_custom_model_rcp_job(model_package, pid_for_queue, variable_importance_qid)
         return [variable_importance_qid] if variable_importance_qid else None
 
-    def _submit_custom_model_variable_importance_job(
-        self, model_package, start_lrs_qids, pid_for_queue
-    ):
+    def _submit_custom_model_feature_impact_job(self, model_package, start_lrs_qids, pid_for_queue):
         # type: (ModelPackage, List[int], ObjectId) -> Optional[int]
         """"""Start variable importance for custom inference model.
 "
"SHAP preview ""Export"" does not provide a way to download all rows","In the modal for SHAP preview “Export”, I cannot find a way to download truly all rows for the datasource or dataset. Even if I uncheck the box Limit downloaded explanations with applied filters, I get only the rows visible in the UI. I thought this button was going to download all rows for the partition. What does it actually do? Is there a way to download truly all rows in, for example, the Validation partition?","Allow all of the rows to be returned when prediction_filter_row_count and prediction_filter_percentiles are both undefined by the user. In general we define defaults through most of the interfaces, but the request can be defined with both as None to allow the full set of data to be download as csv or json.

Make prediction_filter_row_count
Update tests to verify that all rows are actually being returned when parameters are missing","diff --git a/insights_tests/model_insights/tests/backend/integration/api_tests/test_shap_preview_api.py b/insights_tests/model_insights/tests/backend/integration/api_tests/test_shap_preview_api.py
index 173018f64b21e3..4b8a74b0eaff7c 100644
--- a/insights_tests/model_insights/tests/backend/integration/api_tests/test_shap_preview_api.py
+++ b/insights_tests/model_insights/tests/backend/integration/api_tests/test_shap_preview_api.py
@@ -46,7 +46,7 @@ def test_shap_preview_job(
     assert result[""source""] == ""training""
 
     # Assert the results are reasonable
-    assert result[""data""][""previewsCount""] == 100
+    assert result[""data""][""previewsCount""] == 156
     first_preview = result[""data""][""previews""][0]
     assert first_preview[""rowIndex""] == 124
     assert len(first_preview[""previewValues""]) == 37
@@ -81,7 +81,7 @@ def test_shap_preview_job(
             },
         )
         assert response.status_code == 200
-        assert response.json[""data""][0][""data""][""previewsCount""] == 100
+        assert response.json[""data""][0][""data""][""previewsCount""] == 156
 
         response = v2_client.get(
             f""/insights/shapPreview/models/{model_id}/"",
@@ -142,7 +142,7 @@ def test_shap_preview_job_model_specific_shap(
     assert result[""source""] == ""training""
 
     # Assert the results are reasonable
-    assert result[""data""][""previewsCount""] == 100
+    assert result[""data""][""previewsCount""] == 156
     first_preview = result[""data""][""previews""][0]
     assert first_preview[""rowIndex""] == 124
     assert len(first_preview[""previewValues""]) == 31
diff --git a/public_api/model_insights/validators.py b/public_api/model_insights/validators.py
index 75a5b46f795a72..7702b891f70679 100644
--- a/public_api/model_insights/validators.py
+++ b/public_api/model_insights/validators.py
@@ -451,8 +451,8 @@ class GetShapMatrixQueryParamsValidator(
     )
     prediction_filter_row_count = fields.IntField(
         gte=1,
-        default=100,
         description=gettext_openapi_noop(""The maximum number of preview rows to return.""),
+        optional=True,
     )
     prediction_filter_percentiles = fields.IntField(
         gte=1,",,"SHAP preview ""Export"" does not provide a way to download all rows

In the modal for SHAP preview “Export”, I cannot find a way to download truly all rows for the datasource or dataset. Even if I uncheck the box Limit downloaded explanations with applied filters, I get only the rows visible in the UI. I thought this button was going to download all rows for the partition. What does it actually do? Is there a way to download truly all rows in, for example, the Validation partition?","Allow all of the rows to be returned when prediction_filter_row_count and prediction_filter_percentiles are both undefined by the user. In general we define defaults through most of the interfaces, but the request can be defined with both as None to allow the full set of data to be download as csv or json.

Make prediction_filter_row_count
Update tests to verify that all rows are actually being returned when parameters are missing

diff --git a/insights_tests/model_insights/tests/backend/integration/api_tests/test_shap_preview_api.py b/insights_tests/model_insights/tests/backend/integration/api_tests/test_shap_preview_api.py
index 173018f64b21e3..4b8a74b0eaff7c 100644
--- a/insights_tests/model_insights/tests/backend/integration/api_tests/test_shap_preview_api.py
+++ b/insights_tests/model_insights/tests/backend/integration/api_tests/test_shap_preview_api.py
@@ -46,7 +46,7 @@ def test_shap_preview_job(
     assert result[""source""] == ""training""
 
     # Assert the results are reasonable
-    assert result[""data""][""previewsCount""] == 100
+    assert result[""data""][""previewsCount""] == 156
     first_preview = result[""data""][""previews""][0]
     assert first_preview[""rowIndex""] == 124
     assert len(first_preview[""previewValues""]) == 37
@@ -81,7 +81,7 @@ def test_shap_preview_job(
             },
         )
         assert response.status_code == 200
-        assert response.json[""data""][0][""data""][""previewsCount""] == 100
+        assert response.json[""data""][0][""data""][""previewsCount""] == 156
 
         response = v2_client.get(
             f""/insights/shapPreview/models/{model_id}/"",
@@ -142,7 +142,7 @@ def test_shap_preview_job_model_specific_shap(
     assert result[""source""] == ""training""
 
     # Assert the results are reasonable
-    assert result[""data""][""previewsCount""] == 100
+    assert result[""data""][""previewsCount""] == 156
     first_preview = result[""data""][""previews""][0]
     assert first_preview[""rowIndex""] == 124
     assert len(first_preview[""previewValues""]) == 31
diff --git a/public_api/model_insights/validators.py b/public_api/model_insights/validators.py
index 75a5b46f795a72..7702b891f70679 100644
--- a/public_api/model_insights/validators.py
+++ b/public_api/model_insights/validators.py
@@ -451,8 +451,8 @@ class GetShapMatrixQueryParamsValidator(
     )
     prediction_filter_row_count = fields.IntField(
         gte=1,
-        default=100,
         description=gettext_openapi_noop(""The maximum number of preview rows to return.""),
+        optional=True,
     )
     prediction_filter_percentiles = fields.IntField(
         gte=1,"
"SHAP preview for Holdout: something is going wrong with job deduplication, maybe related to data slice","If I go to the SHAP Preview chart and select the Holdout partition, it seems like everything works correctly. But if I then select a data slice, I eventually get a failure, and the backend traceback suggests it is because of a “duplicate” job. If I go to a fresh model and first do Holdout with a data slice, that request is successful, and then Holdout unsliced is the job that fails. So it seems like something is missing in our check for duplicate jobs and/or metadata records.","Not exactly sure why def count was being overridden from the base class, but the override changes to the data_slice_id were causing breaking failures in lookups and returning the incorrect results by incorrectly setting the data_slice_id to ANY when it should have been none. This was causing mongo and other areas to ""retrieve"" data that wouldn't normally have been retrieved and making the job checks think data had already been computed, when it had not. Fix is to just delete this function.                             

Delete the override function and default back to the parent function
Update the current data slice test to run one after another validating that when no data slice is present but a data slice has already been computed that the second call correctly does not see the data from the first call","diff --git a/insights/shap_matrix/crud.py b/insights/shap_matrix/crud.py
index f7d528a1313aed..7c5ea74e2c6e0a 100644
--- a/insights/shap_matrix/crud.py
+++ b/insights/shap_matrix/crud.py
@@ -150,18 +150,6 @@ def get_insights_for_model(
 
         return {InsightsSources.EXTERNAL_TEST_SET: entity}
 
-    def count(self, _calculated_conditions=None, **kwargs):
-        data_slice_id = kwargs[""data_slice_id""] or ALL
-        return len(
-            self.get_insights_for_model(
-                entity_id=kwargs[""entity_id""],
-                project_id=kwargs[""project_id""],
-                source=kwargs[""subset""],
-                data_slice_id=data_slice_id,
-                external_dataset_id=kwargs.get(""external_dataset_id""),
-            )
-        )
-
 
 class ShapMatrixLegacyMetadataSchema(BaseCollectionSchema):
     COLLECTION_NAME = collection_names.SHAP_MATRIX
diff --git a/insights_tests/model_insights/tests/backend/integration/api_tests/test_shap_preview_api.py b/insights_tests/model_insights/tests/backend/integration/api_tests/test_shap_preview_api.py
index b36e42e2044d93..7dd989a8100deb 100644
--- a/insights_tests/model_insights/tests/backend/integration/api_tests/test_shap_preview_api.py
+++ b/insights_tests/model_insights/tests/backend/integration/api_tests/test_shap_preview_api.py
@@ -11,7 +11,6 @@
 # The copyright notice above does not evidence any actual or intended
 # publication of such source code.
 #
-import pytest
 from mock import mock
 
 from common.constants.insights import InsightsFlags
@@ -151,17 +150,8 @@ def test_shap_preview_job_model_specific_shap(
 
 
 @mock.patch(""common.datarobot_queue.shared.rabbit.transport.RabbitMQClientPool"")
-@pytest.mark.parametrize(
-    ""enable_data_slice, expected_preview_count"",
-    [
-        (True, 13),
-        (False, 32),
-    ],
-)
 def test_shap_preview_job_per_task_model(
     mock_rabbit_mq,
-    enable_data_slice,
-    expected_preview_count,
     kickcars_200_external_project_with_simple_model_per_task_shap,
     async_post_and_run_job_history,
     v2_client,
@@ -176,26 +166,42 @@ def test_shap_preview_job_per_task_model(
         _,
     ) = kickcars_200_external_project_with_simple_model_per_task_shap
 
-    payload = {
-        ""source"": InsightsSources.VALIDATION,
-        ""entityId"": str(model_id),
-        ""entityType"": EntityType.DATAROBOT_MODEL,
-    }
-
-    if enable_data_slice:
-        # Create a dataslice on the make value of the cars
-        data_slice_obj = create_integration_test_data_slice(
-            project_id, user_id, operand=""Make"", values=[""CHEVROLET""]
-        )
-        data_slice_id = data_slice_obj.id
-        payload[""dataSliceId""] = str(data_slice_id)
+    # Create a dataslice on the make value of the cars
+    data_slice_obj = create_integration_test_data_slice(
+        project_id, user_id, operand=""Make"", values=[""CHEVROLET""]
+    )
+    data_slice_id = data_slice_obj.id
 
     with user_with_features(user_id, {InsightsFlags.UNIVERSAL_SHAP_IN_NEXTGEN: True}):
+        payload = {
+            ""source"": InsightsSources.VALIDATION,
+            ""entityId"": str(model_id),
+            ""entityType"": EntityType.DATAROBOT_MODEL,
+            ""dataSliceId"": str(data_slice_id),
+        }
+
         async_post_and_run_job_history(project_id, ""/insights/shapMatrix/"", payload)
         async_post_and_run_job_history(project_id, ""/insights/shapPreview/"", payload)
         response = v2_client.get(f""/insights/shapPreview/models/{model_id}/"")
         assert response.status_code == 200
-        assert response.json[""data""][0][""data""][""previewsCount""] == expected_preview_count
+        assert len(response.json[""data""]) == 1
+        assert response.json[""data""][0][""data""][""previewsCount""] == 13
         assert response.json[""data""][0][""data""][""previewsCount""] == len(
             response.json[""data""][0][""data""][""previews""]
         )
+
+        payload = {
+            ""source"": InsightsSources.VALIDATION,
+            ""entityId"": str(model_id),
+            ""entityType"": EntityType.DATAROBOT_MODEL,
+        }
+
+        async_post_and_run_job_history(project_id, ""/insights/shapMatrix/"", payload)
+        async_post_and_run_job_history(project_id, ""/insights/shapPreview/"", payload)
+        response = v2_client.get(f""/insights/shapPreview/models/{model_id}/"")
+        assert response.status_code == 200
+        assert len(response.json[""data""]) == 2
+        assert response.json[""data""][1][""data""][""previewsCount""] == 32
+        assert response.json[""data""][1][""data""][""previewsCount""] == len(
+            response.json[""data""][1][""data""][""previews""]
+        )",,"SHAP preview for Holdout: something is going wrong with job deduplication, maybe related to data slice

If I go to the SHAP Preview chart and select the Holdout partition, it seems like everything works correctly. But if I then select a data slice, I eventually get a failure, and the backend traceback suggests it is because of a “duplicate” job. If I go to a fresh model and first do Holdout with a data slice, that request is successful, and then Holdout unsliced is the job that fails. So it seems like something is missing in our check for duplicate jobs and/or metadata records.","Not exactly sure why def count was being overridden from the base class, but the override changes to the data_slice_id were causing breaking failures in lookups and returning the incorrect results by incorrectly setting the data_slice_id to ANY when it should have been none. This was causing mongo and other areas to ""retrieve"" data that wouldn't normally have been retrieved and making the job checks think data had already been computed, when it had not. Fix is to just delete this function.                             

Delete the override function and default back to the parent function
Update the current data slice test to run one after another validating that when no data slice is present but a data slice has already been computed that the second call correctly does not see the data from the first call

diff --git a/insights/shap_matrix/crud.py b/insights/shap_matrix/crud.py
index f7d528a1313aed..7c5ea74e2c6e0a 100644
--- a/insights/shap_matrix/crud.py
+++ b/insights/shap_matrix/crud.py
@@ -150,18 +150,6 @@ def get_insights_for_model(
 
         return {InsightsSources.EXTERNAL_TEST_SET: entity}
 
-    def count(self, _calculated_conditions=None, **kwargs):
-        data_slice_id = kwargs[""data_slice_id""] or ALL
-        return len(
-            self.get_insights_for_model(
-                entity_id=kwargs[""entity_id""],
-                project_id=kwargs[""project_id""],
-                source=kwargs[""subset""],
-                data_slice_id=data_slice_id,
-                external_dataset_id=kwargs.get(""external_dataset_id""),
-            )
-        )
-
 
 class ShapMatrixLegacyMetadataSchema(BaseCollectionSchema):
     COLLECTION_NAME = collection_names.SHAP_MATRIX
diff --git a/insights_tests/model_insights/tests/backend/integration/api_tests/test_shap_preview_api.py b/insights_tests/model_insights/tests/backend/integration/api_tests/test_shap_preview_api.py
index b36e42e2044d93..7dd989a8100deb 100644
--- a/insights_tests/model_insights/tests/backend/integration/api_tests/test_shap_preview_api.py
+++ b/insights_tests/model_insights/tests/backend/integration/api_tests/test_shap_preview_api.py
@@ -11,7 +11,6 @@
 # The copyright notice above does not evidence any actual or intended
 # publication of such source code.
 #
-import pytest
 from mock import mock
 
 from common.constants.insights import InsightsFlags
@@ -151,17 +150,8 @@ def test_shap_preview_job_model_specific_shap(
 
 
 @mock.patch(""common.datarobot_queue.shared.rabbit.transport.RabbitMQClientPool"")
-@pytest.mark.parametrize(
-    ""enable_data_slice, expected_preview_count"",
-    [
-        (True, 13),
-        (False, 32),
-    ],
-)
 def test_shap_preview_job_per_task_model(
     mock_rabbit_mq,
-    enable_data_slice,
-    expected_preview_count,
     kickcars_200_external_project_with_simple_model_per_task_shap,
     async_post_and_run_job_history,
     v2_client,
@@ -176,26 +166,42 @@ def test_shap_preview_job_per_task_model(
         _,
     ) = kickcars_200_external_project_with_simple_model_per_task_shap
 
-    payload = {
-        ""source"": InsightsSources.VALIDATION,
-        ""entityId"": str(model_id),
-        ""entityType"": EntityType.DATAROBOT_MODEL,
-    }
-
-    if enable_data_slice:
-        # Create a dataslice on the make value of the cars
-        data_slice_obj = create_integration_test_data_slice(
-            project_id, user_id, operand=""Make"", values=[""CHEVROLET""]
-        )
-        data_slice_id = data_slice_obj.id
-        payload[""dataSliceId""] = str(data_slice_id)
+    # Create a dataslice on the make value of the cars
+    data_slice_obj = create_integration_test_data_slice(
+        project_id, user_id, operand=""Make"", values=[""CHEVROLET""]
+    )
+    data_slice_id = data_slice_obj.id
 
     with user_with_features(user_id, {InsightsFlags.UNIVERSAL_SHAP_IN_NEXTGEN: True}):
+        payload = {
+            ""source"": InsightsSources.VALIDATION,
+            ""entityId"": str(model_id),
+            ""entityType"": EntityType.DATAROBOT_MODEL,
+            ""dataSliceId"": str(data_slice_id),
+        }
+
         async_post_and_run_job_history(project_id, ""/insights/shapMatrix/"", payload)
         async_post_and_run_job_history(project_id, ""/insights/shapPreview/"", payload)
         response = v2_client.get(f""/insights/shapPreview/models/{model_id}/"")
         assert response.status_code == 200
-        assert response.json[""data""][0][""data""][""previewsCount""] == expected_preview_count
+        assert len(response.json[""data""]) == 1
+        assert response.json[""data""][0][""data""][""previewsCount""] == 13
         assert response.json[""data""][0][""data""][""previewsCount""] == len(
             response.json[""data""][0][""data""][""previews""]
         )
+
+        payload = {
+            ""source"": InsightsSources.VALIDATION,
+            ""entityId"": str(model_id),
+            ""entityType"": EntityType.DATAROBOT_MODEL,
+        }
+
+        async_post_and_run_job_history(project_id, ""/insights/shapMatrix/"", payload)
+        async_post_and_run_job_history(project_id, ""/insights/shapPreview/"", payload)
+        response = v2_client.get(f""/insights/shapPreview/models/{model_id}/"")
+        assert response.status_code == 200
+        assert len(response.json[""data""]) == 2
+        assert response.json[""data""][1][""data""][""previewsCount""] == 32
+        assert response.json[""data""][1][""data""][""previewsCount""] == len(
+            response.json[""data""][1][""data""][""previews""]
+        )"
"SHAP preview for external datasets: on select, we always POST a new lift chart request, which can then fail because it's a duplicate","When I navigate to the SHAP Preview for an external dataset for the first time, it kicks off Lift and SHAP Preview jobs, as it should, and eventually displays both panels. When I go to some other page and then return to SHAP Preview and select the same external dataset, another Lift job is submitted, which eventually fails because it tries to create a duplicate metadata record in Mongo.

Instead, the existing Lift data should be used, and the duplicate job should not be submitted. I’m not sure whether the UI or the job deduplication logic on the backend is at fault here.","External test set requests would fail when different dataset_id were used due to:
the queue raising duplicate error since no external_dataset_id was ever checked
the insight metadata lookup would not consider the external_dataset_id
Changes:
Added check for external_dataset_id when source=externalTestSet in the queue signature
Consider external_dataset_id on insights metadata lookup to check for insight already calculated
Fixed tiny issue when model_package job aka already calculated in launch batch predictions
Avoided sending an exception string on the insight endpoint for 422 caused due to predictions launcher
","diff --git a/common/services/insights/job_managers.py b/common/services/insights/job_managers.py
index 44decb071b8d62..5f6898293e19bf 100644
--- a/common/services/insights/job_managers.py
+++ b/common/services/insights/job_managers.py
@@ -703,11 +703,13 @@ def get_duplicate_check_condition(self, **kwargs) -> Dict[str, Any]:
         on the respective metadata collection via get_insights_for_model() method.
         The subclasses can add relevant condition as applicable to them.""""""
         data_slice_id = kwargs.get(""data_slice_id"")
+        external_dataset_id = kwargs.get(""external_dataset_id"")
         base_condition = {
             ""project_id"": self.pid,
             ""entity_id"": self.lid,
             ""subset"": kwargs[""subset""],
             ""data_slice_id"": ObjectId(data_slice_id) if data_slice_id else None,
+            ""external_dataset_id"": ObjectId(external_dataset_id) if external_dataset_id else None,
         }
 
         return base_condition
@@ -730,6 +732,9 @@ def get_attributes_to_match_in_job_history_payload(self, **kwargs) -> Dict[str,
                 str(data_slice_id) if data_slice_id else None
             )
 
+        if ""external_dataset_id"" in kwargs:
+            base_attributes_to_match[""external_dataset_id""] = kwargs[""external_dataset_id""]
+
         return base_attributes_to_match
 
     def validate_not_duplicate(self, **kwargs):
diff --git a/common/services/model_submission/model_request.py b/common/services/model_submission/model_request.py
index 549a4bfebd5153..4dc001ceb53b17 100644
--- a/common/services/model_submission/model_request.py
+++ b/common/services/model_submission/model_request.py
@@ -66,6 +66,7 @@
 from common.enum_time_series import OTVLengthType
 from common.enum_time_series import OTVModelStatus
 from common.enum_time_series import OTVModelThresholds
+from common.enums.chartdata import InsightsSources
 from common.exceptions_core_backend import QueueException
 from common.exceptions_core_modeling import BlenderModelSubmitError
 from common.exceptions_core_modeling import InvalidFeatureListQueueException
@@ -3033,6 +3034,8 @@ def get_model_insight_fields(model):
         insight_signature['data_slice_id'] = model.get('data_slice_id')
         if 'subset' in model:
             insight_signature['subset'] = model['subset']
+            if model['subset'] == InsightsSources.EXTERNAL_TEST_SET:
+                insight_signature[""external_dataset_id""] = model.get(""external_dataset_id"")
 
     return insight_signature
 
diff --git a/insights_tests/model_insights/tests/backend/integration/common/services/insights/test_base_insights_metadata_crud_service.py b/insights_tests/model_insights/tests/backend/integration/common/services/insights/test_base_insights_metadata_crud_service.py
index 3fe039891e3fab..21c57987aefd43 100644
--- a/insights_tests/model_insights/tests/backend/integration/common/services/insights/test_base_insights_metadata_crud_service.py
+++ b/insights_tests/model_insights/tests/backend/integration/common/services/insights/test_base_insights_metadata_crud_service.py
@@ -40,14 +40,22 @@ class TestBaseInsightsMetadataCrudService(object):
             }
         ),
     )
-    # TODO: Clean this up under the context of TREX-4420 as its not processing root insights
-    #       or insights with external_dataset_ids correctly
+    @pytest.mark.parametrize(
+        ""subset, external_dataset_id"",
+        [
+            (InsightsSources.VALIDATION, None),
+            (InsightsSources.HOLDOUT, None),
+            (InsightsSources.EXTERNAL_TEST_SET, ObjectId(""635842a81c89e4e1263372d5"")),
+        ],
+        ids=[""validation"", ""holdout"", ""external_test_set""],
+    )
     # TODO: Add FE to the fixture params. Once both are set, use .all(). TREX-2733 or earlier.
-    def test_create_duplicate_raises_error(self, unittest_persistent_db, insight_name):
+    def test_create_duplicate_raises_error(
+        self, unittest_persistent_db, subset, external_dataset_id, insight_name
+    ):
         if insight_name in InsightNames.training_only():
             subset = InsightsSources.TRAINING
-        else:
-            subset = InsightsSources.VALIDATION
+
         insights_metadata_doc = {
             ""project_id"": ObjectId(),
             ""entity_id"": ObjectId(),
@@ -57,6 +65,8 @@ def test_create_duplicate_raises_error(self, unittest_persistent_db, insight_nam
             ""insights_filepath"": ""some_path_to_insights_data.json"",
             ""row_count"": 2500,
         }
+        if external_dataset_id:
+            insights_metadata_doc.update({""external_dataset_id"": external_dataset_id})
         insight_metadata_service = insight_metadata_service_factory(
             insight_name, persistent=unittest_persistent_db
         )
diff --git a/public_api/model_insights/controllers.py b/public_api/model_insights/controllers.py
index 592e64665f523d..30e3fc6aeac991 100644
--- a/public_api/model_insights/controllers.py
+++ b/public_api/model_insights/controllers.py
@@ -43,12 +43,14 @@
 from common.exceptions_core_backend import QueueException
 from common.exceptions_core_backend import ResourceNotFoundError
 from common.exceptions_core_backend import UnprocessableRequestError
+from common.exceptions_core_backend import ValidationError
 from common.exceptions_modelingmachine import MulticlassNotSupportedError
 from common.exceptions_platform_dynamic_workers import DuplicateQueueException
 from common.exceptions_time_series import AOTChartRetrievalException
 from common.exceptions_time_series import TimeSeriesException
 from common.exceptions_time_series import UnexpectedProjectType
 from common.mono_constraints import get_monotonic_constraint_features_of_leaderboard
+from common.prediction.validation import StackedPredictionRequestValidationError
 from common.services.accuracy_over_time.forecast_vs_actual import ForecastVsActualMetadataService
 from common.services.accuracy_over_time.metadata import AOTMetadataService
 from common.services.accuracy_over_time.submission import AccuracyOverTimeSubmissionService
@@ -1070,8 +1072,44 @@ def validator_valid(self, validator):
             prediction_job_qid, dependent_jobs_qids = self._launch_insight_prerequisite_jobs(
                 root, source, insight_name, dataset_id
             )
-        except Exception as e:
+        except (
+            UnprocessableRequestError,
+            ValidationError,
+            StackedPredictionRequestValidationError,
+            UnexpectedProjectType,
+            QueueException,
+        ) as e:
+            logger.info(
+                ""Could not launch pre-requisite jobs"",
+                extra={
+                    ""model_id"": self.entity_id,
+                    ""insight_name"": insight_name,
+                    ""source"": source,
+                    ""dataset_id"": dataset_id,
+                    ""external_dataset_id"": external_dataset_id,
+                    ""data_slice_id"": data_slice_id,
+                },
+                exc_info=True,
+            )
             return {""message"": six.text_type(e)}, status.HTTP_422_UNPROCESSABLE_ENTITY
+        # Catch all to log if something goes wrong with pre-requisite jobs such as predictions
+        except Exception:
+            logger.error(
+                ""Internal unexpected error launching pre-requisite jobs"",
+                extra={
+                    ""model_id"": self.entity_id,
+                    ""insight_name"": insight_name,
+                    ""source"": source,
+                    ""dataset_id"": dataset_id,
+                    ""external_dataset_id"": external_dataset_id,
+                    ""data_slice_id"": data_slice_id,
+                },
+                exc_info=True,
+            )
+            return (
+                {""message"": ""Unable to process pre-requisites.""},
+                status.HTTP_500_INTERNAL_SERVER_ERROR,
+            )
 
         async_status, results_url = self._create_status(
             insight_name=insight_name,
diff --git a/public_api/model_insights/prediction_handler.py b/public_api/model_insights/prediction_handler.py
index b4f05a9677f529..4cec5ffe713681 100644
--- a/public_api/model_insights/prediction_handler.py
+++ b/public_api/model_insights/prediction_handler.py
@@ -296,7 +296,7 @@ def launch(self, prediction_storage_service, predict_job_service):
             include_probabilities_classes=[],
             max_explanations=0,
             intake_settings=intake_settings,
-            dependent_jobs_ids=[mlpkg_job.qid],
+            dependent_jobs_ids=[mlpkg_job.qid] if mlpkg_job else [],
         )
         logger.info(
             ""Created batch predictions job for sliced insights."",
diff --git a/tests/backend/unit/common/job/test_model_insight_job.py b/tests/backend/unit/common/job/test_model_insight_job.py
index e1e35bcf3d6df8..8f214b9183647d 100644
--- a/tests/backend/unit/common/job/test_model_insight_job.py
+++ b/tests/backend/unit/common/job/test_model_insight_job.py
@@ -10,6 +10,8 @@
 #
 # The copyright notice above does not evidence any actual or intended
 # publication of such source code.
+from typing import Optional
+
 import mock
 import pytest
 from bson import ObjectId
@@ -18,18 +20,22 @@
 from common.engine import TargetType
 from common.entities.model import model_signature
 from common.enum_jobs import JobTypeEnum
+from common.enums.chartdata import InsightsSources
 from common.services.model_submission.model_request import is_model_in_queue
 from common.services.model_submission.model_request import queue_signature
 from common.services.queues.queue_service import QueueService
 
 
-@pytest.fixture()
-def model_insight_request_with_data_slice():
+def get_model_insight_request(
+    subset=InsightsSources.VALIDATION,
+    external_dataset_id=None,
+    data_slice_id: Optional[ObjectId] = ObjectId('636579d42f8bf9b5d893a026'),
+):
     request = {
         'model_type': InsightNames.for_ui_from_insight_name(InsightNames.ROC_CURVE),
-        'data_slice_id': ObjectId('636579d42f8bf9b5d893a026'),
+        'data_slice_id': data_slice_id,
         'python_version': '3.7',
-        'subset': 'validation',
+        'subset': subset,
         'dataset_id': '635842a81c89e4e1263372d5',
         'target': 'readmitted',
         'target_type': TargetType.BINARY,
@@ -54,6 +60,14 @@ def model_insight_request_with_data_slice():
         'enable_user_notifications': None,
         'dependent_jobs_qids': ['33'],
     }
+    if subset == InsightsSources.EXTERNAL_TEST_SET and external_dataset_id:
+        request.update({""external_dataset_id"": external_dataset_id})
+    return request
+
+
+@pytest.fixture()
+def model_insight_request_with_data_slice():
+    request = get_model_insight_request()
     return request
 
 
@@ -79,8 +93,19 @@ def test_queue_signature__model_insight(self, model_insight_request_with_data_sl
         insight_signature = queue_signature(insight_request)
         assert insight_signature == expected
 
-    def test_duplicate_model_insight_request(self, model_insight_request_with_data_slice):
-        new_insight_request = model_insight_request_with_data_slice
+    @pytest.mark.parametrize(
+        ""model_insight_request"",
+        [
+            get_model_insight_request(),
+            get_model_insight_request(
+                subset=InsightsSources.EXTERNAL_TEST_SET,
+                external_dataset_id=ObjectId(""635842a81c89e4e1263372d5""),
+            ),
+        ],
+        ids=[""with_data_slice"", ""with_external_dataset_id""],
+    )
+    def test_duplicate_model_insight_request(self, model_insight_request):
+        new_insight_request = model_insight_request
         queue_snapshot_with_insight_request = [new_insight_request.copy()]
         found = is_model_in_queue(new_insight_request, queue_snapshot_with_insight_request)
         assert found
@@ -98,11 +123,35 @@ def test_model_insight_differing_by_subset_is_not_duplicate(
         found = is_model_in_queue(new_insight_request, queue_snapshot_with_insight_request)
         assert not found
 
+    def test_model_insight_differing_by_external_dataset_id_is_not_duplicate(self):
+        new_insight_request = get_model_insight_request(
+            subset=InsightsSources.EXTERNAL_TEST_SET, external_dataset_id=ObjectId()
+        )
+        # queue job request
+        queued_insight_request = new_insight_request.copy()
+        queue_snapshot_with_insight_request = [queued_insight_request]
+        # insight_request with  different subset should not be considered duplicate in the queue
+        new_insight_request[""external_dataset_id""] = ObjectId()
+        assert new_insight_request[""subset""] == InsightsSources.EXTERNAL_TEST_SET
+        assert new_insight_request[""subset""] == queued_insight_request[""subset""]
+        found = is_model_in_queue(new_insight_request, queue_snapshot_with_insight_request)
+        assert not found
+
     @pytest.mark.parametrize(""data_slice_id_different_value"", [""different"", None])
+    @pytest.mark.parametrize(
+        ""subset, external_dataset_id"",
+        [
+            (InsightsSources.VALIDATION, None),
+            (InsightsSources.EXTERNAL_TEST_SET, ObjectId(""635842a81c89e4e1263372d5"")),
+        ],
+        ids=[""validation"", ""externalTestSet""],
+    )
     def test_model_insight_differing_by_data_slice_is_not_duplicate(
-        self, model_insight_request_with_data_slice, data_slice_id_different_value
+        self, subset, external_dataset_id, data_slice_id_different_value
     ):
-        new_insight_request = model_insight_request_with_data_slice
+        new_insight_request = get_model_insight_request(
+            subset=subset, external_dataset_id=external_dataset_id
+        )
         # queue job request
         queued_insight_request = new_insight_request.copy()
         queue_snapshot_with_insight_request = [queued_insight_request]
@@ -112,22 +161,44 @@ def test_model_insight_differing_by_data_slice_is_not_duplicate(
         found = is_model_in_queue(new_insight_request, queue_snapshot_with_insight_request)
         assert not found
 
+    @pytest.mark.parametrize(
+        ""subset, external_dataset_id"",
+        [
+            (InsightsSources.VALIDATION, None),
+            (InsightsSources.EXTERNAL_TEST_SET, ObjectId(""635842a81c89e4e1263372d5"")),
+        ],
+        ids=[""validation"", ""externalTestSet""],
+    )
     def test_model_insight_new_request_without_data_slice_is_not_duplicate(
-        self, model_insight_request_with_data_slice
+        self, subset, external_dataset_id
     ):
-        new_insight_request = model_insight_request_with_data_slice
+        new_insight_request = get_model_insight_request(
+            subset=subset,
+            external_dataset_id=external_dataset_id,
+        )
         # queue job request w/data_slice_id
         queue_snapshot_with_insight_request = [new_insight_request.copy()]
-        # insight_request with  different data_slice_id should not be considered duplicate in queue
-        new_insight_request.pop(""data_slice_id"")
-        assert ""data_slice_id"" not in new_insight_request
+        # new insight_request w/different data_slice_id shouldn't be considered duplicate in queue
+        new_insight_request[""data_slice_id""] = None
+        assert new_insight_request[""data_slice_id""] is None  # not in new_insight_request
         found = is_model_in_queue(new_insight_request, queue_snapshot_with_insight_request)
         assert not found
 
+    @pytest.mark.parametrize(
+        ""subset, external_dataset_id"",
+        [
+            (InsightsSources.VALIDATION, None),
+            (InsightsSources.EXTERNAL_TEST_SET, ObjectId(""635842a81c89e4e1263372d5"")),
+        ],
+        ids=[""validation"", ""externalTestSet""],
+    )
     def test_model_insight_new_request_with_data_slice_is_not_duplicate(
-        self, model_insight_request_with_data_slice
+        self, subset, external_dataset_id
     ):
-        new_insight_request = model_insight_request_with_data_slice
+        new_insight_request = get_model_insight_request(
+            subset=subset, external_dataset_id=external_dataset_id
+        )
+        # model_insight_request_with_data_slice
         # queue job request wo/data_slice_id
         queued_insight_request_wo_data_slice = new_insight_request.copy()
         queued_insight_request_wo_data_slice.pop(""data_slice_id"")
@@ -137,10 +208,20 @@ def test_model_insight_new_request_with_data_slice_is_not_duplicate(
         found = is_model_in_queue(new_insight_request, queue_snapshot_with_insight_request)
         assert not found
 
+    @pytest.mark.parametrize(
+        ""subset, external_dataset_id"",
+        [
+            (InsightsSources.VALIDATION, None),
+            (InsightsSources.EXTERNAL_TEST_SET, ObjectId(""635842a81c89e4e1263372d5"")),
+        ],
+        ids=[""validation"", ""externalTestSet""],
+    )
     def test_model_insight_differing_by_insight_name_is_not_duplicate(
-        self, model_insight_request_with_data_slice
+        self, subset, external_dataset_id
     ):
-        new_insight_request = model_insight_request_with_data_slice
+        new_insight_request = get_model_insight_request(
+            subset=subset, external_dataset_id=external_dataset_id
+        )
         # queue job request
         queued_insight_request = new_insight_request.copy()
         queue_snapshot_with_insight_request = [queued_insight_request]",,"SHAP preview for external datasets: on select, we always POST a new lift chart request, which can then fail because it's a duplicate

When I navigate to the SHAP Preview for an external dataset for the first time, it kicks off Lift and SHAP Preview jobs, as it should, and eventually displays both panels. When I go to some other page and then return to SHAP Preview and select the same external dataset, another Lift job is submitted, which eventually fails because it tries to create a duplicate metadata record in Mongo.

Instead, the existing Lift data should be used, and the duplicate job should not be submitted. I’m not sure whether the UI or the job deduplication logic on the backend is at fault here.","External test set requests would fail when different dataset_id were used due to:
the queue raising duplicate error since no external_dataset_id was ever checked
the insight metadata lookup would not consider the external_dataset_id
Changes:
Added check for external_dataset_id when source=externalTestSet in the queue signature
Consider external_dataset_id on insights metadata lookup to check for insight already calculated
Fixed tiny issue when model_package job aka already calculated in launch batch predictions
Avoided sending an exception string on the insight endpoint for 422 caused due to predictions launcher


diff --git a/common/services/insights/job_managers.py b/common/services/insights/job_managers.py
index 44decb071b8d62..5f6898293e19bf 100644
--- a/common/services/insights/job_managers.py
+++ b/common/services/insights/job_managers.py
@@ -703,11 +703,13 @@ def get_duplicate_check_condition(self, **kwargs) -> Dict[str, Any]:
         on the respective metadata collection via get_insights_for_model() method.
         The subclasses can add relevant condition as applicable to them.""""""
         data_slice_id = kwargs.get(""data_slice_id"")
+        external_dataset_id = kwargs.get(""external_dataset_id"")
         base_condition = {
             ""project_id"": self.pid,
             ""entity_id"": self.lid,
             ""subset"": kwargs[""subset""],
             ""data_slice_id"": ObjectId(data_slice_id) if data_slice_id else None,
+            ""external_dataset_id"": ObjectId(external_dataset_id) if external_dataset_id else None,
         }
 
         return base_condition
@@ -730,6 +732,9 @@ def get_attributes_to_match_in_job_history_payload(self, **kwargs) -> Dict[str,
                 str(data_slice_id) if data_slice_id else None
             )
 
+        if ""external_dataset_id"" in kwargs:
+            base_attributes_to_match[""external_dataset_id""] = kwargs[""external_dataset_id""]
+
         return base_attributes_to_match
 
     def validate_not_duplicate(self, **kwargs):
diff --git a/common/services/model_submission/model_request.py b/common/services/model_submission/model_request.py
index 549a4bfebd5153..4dc001ceb53b17 100644
--- a/common/services/model_submission/model_request.py
+++ b/common/services/model_submission/model_request.py
@@ -66,6 +66,7 @@
 from common.enum_time_series import OTVLengthType
 from common.enum_time_series import OTVModelStatus
 from common.enum_time_series import OTVModelThresholds
+from common.enums.chartdata import InsightsSources
 from common.exceptions_core_backend import QueueException
 from common.exceptions_core_modeling import BlenderModelSubmitError
 from common.exceptions_core_modeling import InvalidFeatureListQueueException
@@ -3033,6 +3034,8 @@ def get_model_insight_fields(model):
         insight_signature['data_slice_id'] = model.get('data_slice_id')
         if 'subset' in model:
             insight_signature['subset'] = model['subset']
+            if model['subset'] == InsightsSources.EXTERNAL_TEST_SET:
+                insight_signature[""external_dataset_id""] = model.get(""external_dataset_id"")
 
     return insight_signature
 
diff --git a/insights_tests/model_insights/tests/backend/integration/common/services/insights/test_base_insights_metadata_crud_service.py b/insights_tests/model_insights/tests/backend/integration/common/services/insights/test_base_insights_metadata_crud_service.py
index 3fe039891e3fab..21c57987aefd43 100644
--- a/insights_tests/model_insights/tests/backend/integration/common/services/insights/test_base_insights_metadata_crud_service.py
+++ b/insights_tests/model_insights/tests/backend/integration/common/services/insights/test_base_insights_metadata_crud_service.py
@@ -40,14 +40,22 @@ class TestBaseInsightsMetadataCrudService(object):
             }
         ),
     )
-    # TODO: Clean this up under the context of TREX-4420 as its not processing root insights
-    #       or insights with external_dataset_ids correctly
+    @pytest.mark.parametrize(
+        ""subset, external_dataset_id"",
+        [
+            (InsightsSources.VALIDATION, None),
+            (InsightsSources.HOLDOUT, None),
+            (InsightsSources.EXTERNAL_TEST_SET, ObjectId(""635842a81c89e4e1263372d5"")),
+        ],
+        ids=[""validation"", ""holdout"", ""external_test_set""],
+    )
     # TODO: Add FE to the fixture params. Once both are set, use .all(). TREX-2733 or earlier.
-    def test_create_duplicate_raises_error(self, unittest_persistent_db, insight_name):
+    def test_create_duplicate_raises_error(
+        self, unittest_persistent_db, subset, external_dataset_id, insight_name
+    ):
         if insight_name in InsightNames.training_only():
             subset = InsightsSources.TRAINING
-        else:
-            subset = InsightsSources.VALIDATION
+
         insights_metadata_doc = {
             ""project_id"": ObjectId(),
             ""entity_id"": ObjectId(),
@@ -57,6 +65,8 @@ def test_create_duplicate_raises_error(self, unittest_persistent_db, insight_nam
             ""insights_filepath"": ""some_path_to_insights_data.json"",
             ""row_count"": 2500,
         }
+        if external_dataset_id:
+            insights_metadata_doc.update({""external_dataset_id"": external_dataset_id})
         insight_metadata_service = insight_metadata_service_factory(
             insight_name, persistent=unittest_persistent_db
         )
diff --git a/public_api/model_insights/controllers.py b/public_api/model_insights/controllers.py
index 592e64665f523d..30e3fc6aeac991 100644
--- a/public_api/model_insights/controllers.py
+++ b/public_api/model_insights/controllers.py
@@ -43,12 +43,14 @@
 from common.exceptions_core_backend import QueueException
 from common.exceptions_core_backend import ResourceNotFoundError
 from common.exceptions_core_backend import UnprocessableRequestError
+from common.exceptions_core_backend import ValidationError
 from common.exceptions_modelingmachine import MulticlassNotSupportedError
 from common.exceptions_platform_dynamic_workers import DuplicateQueueException
 from common.exceptions_time_series import AOTChartRetrievalException
 from common.exceptions_time_series import TimeSeriesException
 from common.exceptions_time_series import UnexpectedProjectType
 from common.mono_constraints import get_monotonic_constraint_features_of_leaderboard
+from common.prediction.validation import StackedPredictionRequestValidationError
 from common.services.accuracy_over_time.forecast_vs_actual import ForecastVsActualMetadataService
 from common.services.accuracy_over_time.metadata import AOTMetadataService
 from common.services.accuracy_over_time.submission import AccuracyOverTimeSubmissionService
@@ -1070,8 +1072,44 @@ def validator_valid(self, validator):
             prediction_job_qid, dependent_jobs_qids = self._launch_insight_prerequisite_jobs(
                 root, source, insight_name, dataset_id
             )
-        except Exception as e:
+        except (
+            UnprocessableRequestError,
+            ValidationError,
+            StackedPredictionRequestValidationError,
+            UnexpectedProjectType,
+            QueueException,
+        ) as e:
+            logger.info(
+                ""Could not launch pre-requisite jobs"",
+                extra={
+                    ""model_id"": self.entity_id,
+                    ""insight_name"": insight_name,
+                    ""source"": source,
+                    ""dataset_id"": dataset_id,
+                    ""external_dataset_id"": external_dataset_id,
+                    ""data_slice_id"": data_slice_id,
+                },
+                exc_info=True,
+            )
             return {""message"": six.text_type(e)}, status.HTTP_422_UNPROCESSABLE_ENTITY
+        # Catch all to log if something goes wrong with pre-requisite jobs such as predictions
+        except Exception:
+            logger.error(
+                ""Internal unexpected error launching pre-requisite jobs"",
+                extra={
+                    ""model_id"": self.entity_id,
+                    ""insight_name"": insight_name,
+                    ""source"": source,
+                    ""dataset_id"": dataset_id,
+                    ""external_dataset_id"": external_dataset_id,
+                    ""data_slice_id"": data_slice_id,
+                },
+                exc_info=True,
+            )
+            return (
+                {""message"": ""Unable to process pre-requisites.""},
+                status.HTTP_500_INTERNAL_SERVER_ERROR,
+            )
 
         async_status, results_url = self._create_status(
             insight_name=insight_name,
diff --git a/public_api/model_insights/prediction_handler.py b/public_api/model_insights/prediction_handler.py
index b4f05a9677f529..4cec5ffe713681 100644
--- a/public_api/model_insights/prediction_handler.py
+++ b/public_api/model_insights/prediction_handler.py
@@ -296,7 +296,7 @@ def launch(self, prediction_storage_service, predict_job_service):
             include_probabilities_classes=[],
             max_explanations=0,
             intake_settings=intake_settings,
-            dependent_jobs_ids=[mlpkg_job.qid],
+            dependent_jobs_ids=[mlpkg_job.qid] if mlpkg_job else [],
         )
         logger.info(
             ""Created batch predictions job for sliced insights."",
diff --git a/tests/backend/unit/common/job/test_model_insight_job.py b/tests/backend/unit/common/job/test_model_insight_job.py
index e1e35bcf3d6df8..8f214b9183647d 100644
--- a/tests/backend/unit/common/job/test_model_insight_job.py
+++ b/tests/backend/unit/common/job/test_model_insight_job.py
@@ -10,6 +10,8 @@
 #
 # The copyright notice above does not evidence any actual or intended
 # publication of such source code.
+from typing import Optional
+
 import mock
 import pytest
 from bson import ObjectId
@@ -18,18 +20,22 @@
 from common.engine import TargetType
 from common.entities.model import model_signature
 from common.enum_jobs import JobTypeEnum
+from common.enums.chartdata import InsightsSources
 from common.services.model_submission.model_request import is_model_in_queue
 from common.services.model_submission.model_request import queue_signature
 from common.services.queues.queue_service import QueueService
 
 
-@pytest.fixture()
-def model_insight_request_with_data_slice():
+def get_model_insight_request(
+    subset=InsightsSources.VALIDATION,
+    external_dataset_id=None,
+    data_slice_id: Optional[ObjectId] = ObjectId('636579d42f8bf9b5d893a026'),
+):
     request = {
         'model_type': InsightNames.for_ui_from_insight_name(InsightNames.ROC_CURVE),
-        'data_slice_id': ObjectId('636579d42f8bf9b5d893a026'),
+        'data_slice_id': data_slice_id,
         'python_version': '3.7',
-        'subset': 'validation',
+        'subset': subset,
         'dataset_id': '635842a81c89e4e1263372d5',
         'target': 'readmitted',
         'target_type': TargetType.BINARY,
@@ -54,6 +60,14 @@ def model_insight_request_with_data_slice():
         'enable_user_notifications': None,
         'dependent_jobs_qids': ['33'],
     }
+    if subset == InsightsSources.EXTERNAL_TEST_SET and external_dataset_id:
+        request.update({""external_dataset_id"": external_dataset_id})
+    return request
+
+
+@pytest.fixture()
+def model_insight_request_with_data_slice():
+    request = get_model_insight_request()
     return request
 
 
@@ -79,8 +93,19 @@ def test_queue_signature__model_insight(self, model_insight_request_with_data_sl
         insight_signature = queue_signature(insight_request)
         assert insight_signature == expected
 
-    def test_duplicate_model_insight_request(self, model_insight_request_with_data_slice):
-        new_insight_request = model_insight_request_with_data_slice
+    @pytest.mark.parametrize(
+        ""model_insight_request"",
+        [
+            get_model_insight_request(),
+            get_model_insight_request(
+                subset=InsightsSources.EXTERNAL_TEST_SET,
+                external_dataset_id=ObjectId(""635842a81c89e4e1263372d5""),
+            ),
+        ],
+        ids=[""with_data_slice"", ""with_external_dataset_id""],
+    )
+    def test_duplicate_model_insight_request(self, model_insight_request):
+        new_insight_request = model_insight_request
         queue_snapshot_with_insight_request = [new_insight_request.copy()]
         found = is_model_in_queue(new_insight_request, queue_snapshot_with_insight_request)
         assert found
@@ -98,11 +123,35 @@ def test_model_insight_differing_by_subset_is_not_duplicate(
         found = is_model_in_queue(new_insight_request, queue_snapshot_with_insight_request)
         assert not found
 
+    def test_model_insight_differing_by_external_dataset_id_is_not_duplicate(self):
+        new_insight_request = get_model_insight_request(
+            subset=InsightsSources.EXTERNAL_TEST_SET, external_dataset_id=ObjectId()
+        )
+        # queue job request
+        queued_insight_request = new_insight_request.copy()
+        queue_snapshot_with_insight_request = [queued_insight_request]
+        # insight_request with  different subset should not be considered duplicate in the queue
+        new_insight_request[""external_dataset_id""] = ObjectId()
+        assert new_insight_request[""subset""] == InsightsSources.EXTERNAL_TEST_SET
+        assert new_insight_request[""subset""] == queued_insight_request[""subset""]
+        found = is_model_in_queue(new_insight_request, queue_snapshot_with_insight_request)
+        assert not found
+
     @pytest.mark.parametrize(""data_slice_id_different_value"", [""different"", None])
+    @pytest.mark.parametrize(
+        ""subset, external_dataset_id"",
+        [
+            (InsightsSources.VALIDATION, None),
+            (InsightsSources.EXTERNAL_TEST_SET, ObjectId(""635842a81c89e4e1263372d5"")),
+        ],
+        ids=[""validation"", ""externalTestSet""],
+    )
     def test_model_insight_differing_by_data_slice_is_not_duplicate(
-        self, model_insight_request_with_data_slice, data_slice_id_different_value
+        self, subset, external_dataset_id, data_slice_id_different_value
     ):
-        new_insight_request = model_insight_request_with_data_slice
+        new_insight_request = get_model_insight_request(
+            subset=subset, external_dataset_id=external_dataset_id
+        )
         # queue job request
         queued_insight_request = new_insight_request.copy()
         queue_snapshot_with_insight_request = [queued_insight_request]
@@ -112,22 +161,44 @@ def test_model_insight_differing_by_data_slice_is_not_duplicate(
         found = is_model_in_queue(new_insight_request, queue_snapshot_with_insight_request)
         assert not found
 
+    @pytest.mark.parametrize(
+        ""subset, external_dataset_id"",
+        [
+            (InsightsSources.VALIDATION, None),
+            (InsightsSources.EXTERNAL_TEST_SET, ObjectId(""635842a81c89e4e1263372d5"")),
+        ],
+        ids=[""validation"", ""externalTestSet""],
+    )
     def test_model_insight_new_request_without_data_slice_is_not_duplicate(
-        self, model_insight_request_with_data_slice
+        self, subset, external_dataset_id
     ):
-        new_insight_request = model_insight_request_with_data_slice
+        new_insight_request = get_model_insight_request(
+            subset=subset,
+            external_dataset_id=external_dataset_id,
+        )
         # queue job request w/data_slice_id
         queue_snapshot_with_insight_request = [new_insight_request.copy()]
-        # insight_request with  different data_slice_id should not be considered duplicate in queue
-        new_insight_request.pop(""data_slice_id"")
-        assert ""data_slice_id"" not in new_insight_request
+        # new insight_request w/different data_slice_id shouldn't be considered duplicate in queue
+        new_insight_request[""data_slice_id""] = None
+        assert new_insight_request[""data_slice_id""] is None  # not in new_insight_request
         found = is_model_in_queue(new_insight_request, queue_snapshot_with_insight_request)
         assert not found
 
+    @pytest.mark.parametrize(
+        ""subset, external_dataset_id"",
+        [
+            (InsightsSources.VALIDATION, None),
+            (InsightsSources.EXTERNAL_TEST_SET, ObjectId(""635842a81c89e4e1263372d5"")),
+        ],
+        ids=[""validation"", ""externalTestSet""],
+    )
     def test_model_insight_new_request_with_data_slice_is_not_duplicate(
-        self, model_insight_request_with_data_slice
+        self, subset, external_dataset_id
     ):
-        new_insight_request = model_insight_request_with_data_slice
+        new_insight_request = get_model_insight_request(
+            subset=subset, external_dataset_id=external_dataset_id
+        )
+        # model_insight_request_with_data_slice
         # queue job request wo/data_slice_id
         queued_insight_request_wo_data_slice = new_insight_request.copy()
         queued_insight_request_wo_data_slice.pop(""data_slice_id"")
@@ -137,10 +208,20 @@ def test_model_insight_new_request_with_data_slice_is_not_duplicate(
         found = is_model_in_queue(new_insight_request, queue_snapshot_with_insight_request)
         assert not found
 
+    @pytest.mark.parametrize(
+        ""subset, external_dataset_id"",
+        [
+            (InsightsSources.VALIDATION, None),
+            (InsightsSources.EXTERNAL_TEST_SET, ObjectId(""635842a81c89e4e1263372d5"")),
+        ],
+        ids=[""validation"", ""externalTestSet""],
+    )
     def test_model_insight_differing_by_insight_name_is_not_duplicate(
-        self, model_insight_request_with_data_slice
+        self, subset, external_dataset_id
     ):
-        new_insight_request = model_insight_request_with_data_slice
+        new_insight_request = get_model_insight_request(
+            subset=subset, external_dataset_id=external_dataset_id
+        )
         # queue job request
         queued_insight_request = new_insight_request.copy()
         queue_snapshot_with_insight_request = [queued_insight_request]"
Accuracy over time data is not deleted when model is deleted or project is permadeleted,"files are here:

when model is deleted `accuracyOverTime/models/<model_id>/`

when project is permadeleted.

 

Deletion should happen and tests to check it on model deletion  and project permadeletion should be added.


 

We need to make sure

deletion for old existing projects with AOT in mongo works. 

deletion for new projects works

mongo collection otp_trend_chartdata and otp_trend_metadata must be cleaned

were there any files except those stored on accuracyOverTime  level?

Steps to reproduce


- train multiseries time series project with attached dataset, ts settings are defaults
- train any model, for example Baseline Predictions Using Most Recent Value
- take model id and look for a folder with accuracy over time files, see attachment
- delete a model, files are not deleted
- delete and permadelete a project. files are not deleted","This PR adds AccuracyOverTime to SERVICE_DESCRIPTIONS_MAP_PROJECT_RELATED in order for it to be picked up for deletion when a project or model is delete.

The following are removed:

otp_actuals_chartdata mongo collection
otp_trend_chartdata mongo collection
otp_trend_metadata mongo collection
all accuracy over time files","diff --git a/common/schema/generic.py b/common/schema/generic.py
index bdfd08acd07472..6bf78915d548e6 100644
--- a/common/schema/generic.py
+++ b/common/schema/generic.py
@@ -139,11 +139,6 @@ class OtpActualsChartdataSchema(BaseCollectionSchema):
     PROJECT_ID_FIELD = ""project_id""
 
 
-class OtpTrendChartdataSchema(BaseCollectionSchema):
-    COLLECTION_NAME = collection_names.OTP_TREND_CHARTDATA
-    PROJECT_ID_FIELD = ""project_id""
-
-
 class PayoffMatricesSchema(BaseCollectionSchema):
     COLLECTION_NAME = collection_names.PAYOFF_MATRICES
     PROJECT_ID_FIELD = ""project_id""
@@ -239,7 +234,6 @@ def get_collections(cls):
             ModelInsightsSchema,
             ModelXraySchema,
             OtpActualsChartdataSchema,
-            OtpTrendChartdataSchema,
             PayoffMatricesSchema,
             PrecomputedchartdataSchema,
             PredictCodeSchema,
diff --git a/common/services/all_schemas.py b/common/services/all_schemas.py
index 5f6605517b31bf..e5b60a7ab1a5ca 100644
--- a/common/services/all_schemas.py
+++ b/common/services/all_schemas.py
@@ -246,6 +246,14 @@
         'ResidualsMetadataServiceDataDescription',
         'insights.residuals.crud.ResidualsMetadataServiceDataDescription',
     ),
+    (
+        'AccuracyOverTimeMetadataDataDescription',
+        'common.services.accuracy_over_time.metadata.AccuracyOverTimeMetadataDataDescription',
+    ),
+    (
+        'AccuracyOverTimeMetadataServiceDataDescription',
+        'insights.accuracy_over_time.crud.AccuracyOverTimeMetadataServiceDataDescription',
+    ),
     (
         'FeatureImpactMetadataServiceDataDescription',
         'insights.feature_impact.crud.FeatureImpactMetadataServiceDataDescription',
diff --git a/tests/common/helpers.py b/tests/common/helpers.py
index a073ecf77ea79b..c2b2bf666dd656 100644
--- a/tests/common/helpers.py
+++ b/tests/common/helpers.py
@@ -692,11 +692,8 @@ def are_all_created_files_really_gone(initial_state_files):
     current_state = find_all_files()
     leftover = []
     for f in current_state.difference(initial_state_files):
-        # TODO: TREX-4545, remove when AOT files are deleted
-        if '/accuracyOverTime/' in f:
-            pass
         # TODO: remove once MODEL-13407 is done
-        elif '/hotspots' in f:
+        if '/hotspots' in f:
             pass
         else:
             leftover.append(f)",,"Accuracy over time data is not deleted when model is deleted or project is permadeleted

files are here:

when model is deleted `accuracyOverTime/models/<model_id>/`

when project is permadeleted.

 

Deletion should happen and tests to check it on model deletion  and project permadeletion should be added.


 

We need to make sure

deletion for old existing projects with AOT in mongo works. 

deletion for new projects works

mongo collection otp_trend_chartdata and otp_trend_metadata must be cleaned

were there any files except those stored on accuracyOverTime  level?

Steps to reproduce


- train multiseries time series project with attached dataset, ts settings are defaults
- train any model, for example Baseline Predictions Using Most Recent Value
- take model id and look for a folder with accuracy over time files, see attachment
- delete a model, files are not deleted
- delete and permadelete a project. files are not deleted","This PR adds AccuracyOverTime to SERVICE_DESCRIPTIONS_MAP_PROJECT_RELATED in order for it to be picked up for deletion when a project or model is delete.

The following are removed:

otp_actuals_chartdata mongo collection
otp_trend_chartdata mongo collection
otp_trend_metadata mongo collection
all accuracy over time files

diff --git a/common/schema/generic.py b/common/schema/generic.py
index bdfd08acd07472..6bf78915d548e6 100644
--- a/common/schema/generic.py
+++ b/common/schema/generic.py
@@ -139,11 +139,6 @@ class OtpActualsChartdataSchema(BaseCollectionSchema):
     PROJECT_ID_FIELD = ""project_id""
 
 
-class OtpTrendChartdataSchema(BaseCollectionSchema):
-    COLLECTION_NAME = collection_names.OTP_TREND_CHARTDATA
-    PROJECT_ID_FIELD = ""project_id""
-
-
 class PayoffMatricesSchema(BaseCollectionSchema):
     COLLECTION_NAME = collection_names.PAYOFF_MATRICES
     PROJECT_ID_FIELD = ""project_id""
@@ -239,7 +234,6 @@ def get_collections(cls):
             ModelInsightsSchema,
             ModelXraySchema,
             OtpActualsChartdataSchema,
-            OtpTrendChartdataSchema,
             PayoffMatricesSchema,
             PrecomputedchartdataSchema,
             PredictCodeSchema,
diff --git a/common/services/all_schemas.py b/common/services/all_schemas.py
index 5f6605517b31bf..e5b60a7ab1a5ca 100644
--- a/common/services/all_schemas.py
+++ b/common/services/all_schemas.py
@@ -246,6 +246,14 @@
         'ResidualsMetadataServiceDataDescription',
         'insights.residuals.crud.ResidualsMetadataServiceDataDescription',
     ),
+    (
+        'AccuracyOverTimeMetadataDataDescription',
+        'common.services.accuracy_over_time.metadata.AccuracyOverTimeMetadataDataDescription',
+    ),
+    (
+        'AccuracyOverTimeMetadataServiceDataDescription',
+        'insights.accuracy_over_time.crud.AccuracyOverTimeMetadataServiceDataDescription',
+    ),
     (
         'FeatureImpactMetadataServiceDataDescription',
         'insights.feature_impact.crud.FeatureImpactMetadataServiceDataDescription',
diff --git a/tests/common/helpers.py b/tests/common/helpers.py
index a073ecf77ea79b..c2b2bf666dd656 100644
--- a/tests/common/helpers.py
+++ b/tests/common/helpers.py
@@ -692,11 +692,8 @@ def are_all_created_files_really_gone(initial_state_files):
     current_state = find_all_files()
     leftover = []
     for f in current_state.difference(initial_state_files):
-        # TODO: TREX-4545, remove when AOT files are deleted
-        if '/accuracyOverTime/' in f:
-            pass
         # TODO: remove once MODEL-13407 is done
-        elif '/hotspots' in f:
+        if '/hotspots' in f:
             pass
         else:
             leftover.append(f)"
Slice sizes validation spins forever with 50% sample window percentage,"When you click buttons to request sliced Feature Impact in the UI, the browser submits a sliceSizes job and waits for the result. It doesn’t work correctly for time-window-sampled models.

Steps to reproduce:

start a project using the dataset tests/testdata/fastiron-train-sample.csv, target SalePrice, OTV partitioning using the saledate feature.

train any model

retrain the model using a time-window sampling of 50%

Create any valid slice. Here is one that should work for the parent model:

slice feature: Enclosure

slice filter: = EROPS

Request feature impact using that slice.

Actual result: spinner spins forever. You can also watch the network console and see the browser polling the sliceSizes endpoint long after the job is (or should be) done.

Expected result: Validation should succeed, Feature Impact jobs should be run, a chart should be displayed.","For OTV and TS feature impact the training sample is selected using a start/end model using the whole range of available training data regardless of the specific training set for the model. This is a deliberate design decision so that changes to feature impact are due to the model being different, not the sample being different. This is not compatible with cached training predictions because those are specific to the training set for the model.
Don't submit the cached predictions job when running sliced feature impact for datetime partitioned models. Use a FeatureImpactLeaderboardPredictor to generate the training predictions in the feature impact finalization job. This is a LeaderboardPredictor modified to return the expected prediction container for feature impact.","diff --git a/public_api/model_insights/controllers.py b/public_api/model_insights/controllers.py
index 33ae9c7881acfa..13d9c42f6b761d 100644
--- a/public_api/model_insights/controllers.py
+++ b/public_api/model_insights/controllers.py
@@ -452,7 +452,9 @@ def _conditionally_launch_predict_job(
             List of qid of prediction jobs launched or in the queue. Otherwise, return an empty list
         """"""
         job_ids = []
-        if self.is_unsupervised_clustering:
+        if self.is_unsupervised_clustering or (
+            self.insight_name == InsightNames.FEATURE_IMPACT and self.is_time_aware
+        ):
             return job_ids
 
         # instantiate necessary services
diff --git a/trust_explainable_ai/model_insights/tests/backend/unit/ModelingMachine/jobs/secure/model_insights/test_insights_feature_impact_jobs.py b/trust_explainable_ai/model_insights/tests/backend/unit/ModelingMachine/jobs/secure/model_insights/test_insights_feature_impact_jobs.py
index c5babba5a37a37..1dc24799023988 100644
--- a/trust_explainable_ai/model_insights/tests/backend/unit/ModelingMachine/jobs/secure/model_insights/test_insights_feature_impact_jobs.py
+++ b/trust_explainable_ai/model_insights/tests/backend/unit/ModelingMachine/jobs/secure/model_insights/test_insights_feature_impact_jobs.py
@@ -34,6 +34,7 @@
 from ModelingMachine.engine.container import Container
 from trusted.jobs.insight_jobs import BuildInfoDatasetReader
 from trusted.jobs.insight_jobs import CachedPredictor
+from trusted.jobs.insight_jobs import FeatureImpactLeaderboardPredictor
 from trusted.jobs.insight_jobs import LeaderboardPredictor
 from trusted.jobs.insights_feature_impact_jobs import FeatureImpactFinalizationInsightJob
 from trusted.jobs.insights_feature_impact_jobs import FeatureImpactSyntheticInsightJob
@@ -408,6 +409,29 @@ def test_uses_cached_predictor_and_build_info_dsreader(self, create_insight_job)
         # ensure we use the desired reader
         assert isinstance(job.data_reader, BuildInfoDatasetReader)
 
+    @pytest.mark.usefixtures(""permissions_check_mock"", ""mock_build_info_reader"")
+    def test_uses_feature_impact_leaderboard_predictor_and_build_info_dsreader_for_OTV(
+        self, create_insight_job
+    ):
+        # construct a fi finalization request and job
+        job_request = make_job_request(
+            command=JobTypeEnum.MODEL_INSIGHTS_FEATURE_IMPACT_FINALIZATION,
+            row_count=10,
+            project_metric=""LogLoss"",
+            special_features=[],
+        )
+        job_request = BaseRequest(job_request)
+        job = create_insight_job(
+            worker_job_request=job_request,
+            insight_job_class=FeatureImpactFinalizationInsightJob,
+            is_otp=True,
+        )
+
+        # must use cached predictor to use stored predictions for slicing
+        assert isinstance(job.predictor, FeatureImpactLeaderboardPredictor)
+        # ensure we use the desired reader
+        assert isinstance(job.data_reader, BuildInfoDatasetReader)
+
     @pytest.mark.usefixtures(
         ""mock_build_info_reader"",
         ""permissions_check_mock"",
diff --git a/trust_explainable_ai/model_insights/tests/backend/unit/public_api/test_insights_api.py b/trust_explainable_ai/model_insights/tests/backend/unit/public_api/test_insights_api.py
index 206effd8d810b4..6136277e0667a1 100644
--- a/trust_explainable_ai/model_insights/tests/backend/unit/public_api/test_insights_api.py
+++ b/trust_explainable_ai/model_insights/tests/backend/unit/public_api/test_insights_api.py
@@ -1236,6 +1236,48 @@ def test_valid_feature_impact__launch_training_predictions(
         assert kwargs[""dependent_jobs_qids""] == []
         assert kwargs[""training_predictions_qid""] == [training_pred_qid]
 
+    @pytest.mark.usefixtures(""mock_validate_request_success"")
+    @mock.patch(""public_api.model_insights.controllers.PredictionStorage.find_record"")
+    @mock.patch(
+        ""public_api.model_insights.controllers.PredictJobService.""
+        ""validate_and_submit_non_stacked_request""
+    )
+    @mock.patch(
+        ""public_api.model_insights.controllers.ComputeFeatureImpactController.is_time_aware"",
+        new=True,
+    )
+    def test_valid_feature_impact__OTV_does_not_launch_training_predictions(
+        self,
+        mock_create_regular_preds,
+        mock_prediction_storage_find_record,
+        client,
+        feature_impact_post_request_dr_model,
+        mock_submit_compute_job,
+    ):
+        # GIVEN, no predictions found on storage, set up just like
+        # test_valid_feature_impact__launch_training_predictions except we mocked is_time_aware=True
+        mock_prediction_storage_find_record.side_effect = ResourceNotFoundError()
+        training_pred_qid = 10
+        mock_create_regular_preds.return_value = training_pred_qid
+
+        request_url = self.compute_sliced_insights_url.format(
+            insightName=InsightNames.FEATURE_IMPACT
+        )
+        request_data = feature_impact_post_request_dr_model
+        response = client.post(request_url, json=request_data)
+
+        assert response.status_code == 202
+
+        # THEN, check we do not call validate and launch regular predict job
+        mock_create_regular_preds.assert_not_called()
+
+        # AND, should submit job has status_id, no dependent_jobs_qid
+        # and has no training predictions qid.
+        _, kwargs = mock_submit_compute_job.call_args
+        assert kwargs[""status_id""] is not None
+        assert kwargs[""dependent_jobs_qids""] == []
+        assert kwargs[""training_predictions_qid""] == []
+
     @pytest.mark.usefixtures(""mock_validate_request_success"")
     @mock.patch(""public_api.model_insights.controllers.PredictionStorage.find_record"")
     @mock.patch(
diff --git a/trusted/jobs/insight_jobs.py b/trusted/jobs/insight_jobs.py
index 1784f24057af5e..bbf37ec6a1ecfd 100644
--- a/trusted/jobs/insight_jobs.py
+++ b/trusted/jobs/insight_jobs.py
@@ -184,6 +184,11 @@ def predictor(self):
                 )
             )
         ):
+            if (
+                command == JobTypeEnum.MODEL_INSIGHTS_FEATURE_IMPACT_FINALIZATION
+                and self.request.is_otp
+            ):
+                return FeatureImpactLeaderboardPredictor(self.request, self.api)
             return CachedPredictor(self.request, self.api)
         return LeaderboardPredictor(self.request, self.api)
 
@@ -563,9 +568,7 @@ def predict_synthetic(self, data):
         raise NotImplementedError
 
 
-class CachedPredictor(InsightPredictor):
-    """"""Used for when predictions have already been cached""""""
-
+class PredictionsContainerMixin:
     def extract_weights_and_exposures_columns(self, data):
         weight_col = self._request[""dr_special_columns_map""].get(""weight"")
         exposure_col = self._request[""dr_special_columns_map""].get(""exposure"")
@@ -593,6 +596,10 @@ def create_predictions_container(partition_tuple, **kwargs):
 
         return out
 
+
+class CachedPredictor(PredictionsContainerMixin, InsightPredictor):
+    """"""Used for when predictions have already been cached""""""
+
     def predict(self, data):
         project_id = self._request[""pid""]
         model_id = self._request[""lid""]
@@ -810,6 +817,32 @@ def config(self):
         return BinaryPredictorConfigs(self._threshold, self._positive_class, self._negative_class)
 
 
+class FeatureImpactLeaderboardPredictor(PredictionsContainerMixin, LeaderboardPredictor):
+    """"""This makes leaderboard predictions and adds the attributes to the output expected
+    for feature impact, similar to CachedPredictor which is used for AutoML feature impact.""""""
+
+    def predict(self, data):
+        container = super().predict(data)
+        r, k = get_first_partition(self._request[""partitions""])
+        partition_tuple = (r, k)
+        # TODO: TREX-3439 .flatten probably doesn't work for multiclass
+        predictions_series = pd.Series(
+            data=container(*partition_tuple).flatten(),
+            index=container.get(""row_index"", *partition_tuple),
+        )
+        weights_and_exposures_map = self.extract_weights_and_exposures_columns(data)
+        target = data[self._request[""target""]]
+        out = self.create_predictions_container(
+            partition_tuple=partition_tuple,
+            predictions=predictions_series.values,
+            row_index=predictions_series.index.values,
+            target=target,
+            sample_weight=data.sample_weight,
+            weights_and_exposures=weights_and_exposures_map,
+        )
+        return out
+
+
 class CustomModelPredictor(ModelPredictor):
     """"""Entity that makes predictions on a custom inference model.""""""
 ",,"Slice sizes validation spins forever with 50% sample window percentage

When you click buttons to request sliced Feature Impact in the UI, the browser submits a sliceSizes job and waits for the result. It doesn’t work correctly for time-window-sampled models.

Steps to reproduce:

start a project using the dataset tests/testdata/fastiron-train-sample.csv, target SalePrice, OTV partitioning using the saledate feature.

train any model

retrain the model using a time-window sampling of 50%

Create any valid slice. Here is one that should work for the parent model:

slice feature: Enclosure

slice filter: = EROPS

Request feature impact using that slice.

Actual result: spinner spins forever. You can also watch the network console and see the browser polling the sliceSizes endpoint long after the job is (or should be) done.

Expected result: Validation should succeed, Feature Impact jobs should be run, a chart should be displayed.","For OTV and TS feature impact the training sample is selected using a start/end model using the whole range of available training data regardless of the specific training set for the model. This is a deliberate design decision so that changes to feature impact are due to the model being different, not the sample being different. This is not compatible with cached training predictions because those are specific to the training set for the model.
Don't submit the cached predictions job when running sliced feature impact for datetime partitioned models. Use a FeatureImpactLeaderboardPredictor to generate the training predictions in the feature impact finalization job. This is a LeaderboardPredictor modified to return the expected prediction container for feature impact.

diff --git a/public_api/model_insights/controllers.py b/public_api/model_insights/controllers.py
index 33ae9c7881acfa..13d9c42f6b761d 100644
--- a/public_api/model_insights/controllers.py
+++ b/public_api/model_insights/controllers.py
@@ -452,7 +452,9 @@ def _conditionally_launch_predict_job(
             List of qid of prediction jobs launched or in the queue. Otherwise, return an empty list
         """"""
         job_ids = []
-        if self.is_unsupervised_clustering:
+        if self.is_unsupervised_clustering or (
+            self.insight_name == InsightNames.FEATURE_IMPACT and self.is_time_aware
+        ):
             return job_ids
 
         # instantiate necessary services
diff --git a/trust_explainable_ai/model_insights/tests/backend/unit/ModelingMachine/jobs/secure/model_insights/test_insights_feature_impact_jobs.py b/trust_explainable_ai/model_insights/tests/backend/unit/ModelingMachine/jobs/secure/model_insights/test_insights_feature_impact_jobs.py
index c5babba5a37a37..1dc24799023988 100644
--- a/trust_explainable_ai/model_insights/tests/backend/unit/ModelingMachine/jobs/secure/model_insights/test_insights_feature_impact_jobs.py
+++ b/trust_explainable_ai/model_insights/tests/backend/unit/ModelingMachine/jobs/secure/model_insights/test_insights_feature_impact_jobs.py
@@ -34,6 +34,7 @@
 from ModelingMachine.engine.container import Container
 from trusted.jobs.insight_jobs import BuildInfoDatasetReader
 from trusted.jobs.insight_jobs import CachedPredictor
+from trusted.jobs.insight_jobs import FeatureImpactLeaderboardPredictor
 from trusted.jobs.insight_jobs import LeaderboardPredictor
 from trusted.jobs.insights_feature_impact_jobs import FeatureImpactFinalizationInsightJob
 from trusted.jobs.insights_feature_impact_jobs import FeatureImpactSyntheticInsightJob
@@ -408,6 +409,29 @@ def test_uses_cached_predictor_and_build_info_dsreader(self, create_insight_job)
         # ensure we use the desired reader
         assert isinstance(job.data_reader, BuildInfoDatasetReader)
 
+    @pytest.mark.usefixtures(""permissions_check_mock"", ""mock_build_info_reader"")
+    def test_uses_feature_impact_leaderboard_predictor_and_build_info_dsreader_for_OTV(
+        self, create_insight_job
+    ):
+        # construct a fi finalization request and job
+        job_request = make_job_request(
+            command=JobTypeEnum.MODEL_INSIGHTS_FEATURE_IMPACT_FINALIZATION,
+            row_count=10,
+            project_metric=""LogLoss"",
+            special_features=[],
+        )
+        job_request = BaseRequest(job_request)
+        job = create_insight_job(
+            worker_job_request=job_request,
+            insight_job_class=FeatureImpactFinalizationInsightJob,
+            is_otp=True,
+        )
+
+        # must use cached predictor to use stored predictions for slicing
+        assert isinstance(job.predictor, FeatureImpactLeaderboardPredictor)
+        # ensure we use the desired reader
+        assert isinstance(job.data_reader, BuildInfoDatasetReader)
+
     @pytest.mark.usefixtures(
         ""mock_build_info_reader"",
         ""permissions_check_mock"",
diff --git a/trust_explainable_ai/model_insights/tests/backend/unit/public_api/test_insights_api.py b/trust_explainable_ai/model_insights/tests/backend/unit/public_api/test_insights_api.py
index 206effd8d810b4..6136277e0667a1 100644
--- a/trust_explainable_ai/model_insights/tests/backend/unit/public_api/test_insights_api.py
+++ b/trust_explainable_ai/model_insights/tests/backend/unit/public_api/test_insights_api.py
@@ -1236,6 +1236,48 @@ def test_valid_feature_impact__launch_training_predictions(
         assert kwargs[""dependent_jobs_qids""] == []
         assert kwargs[""training_predictions_qid""] == [training_pred_qid]
 
+    @pytest.mark.usefixtures(""mock_validate_request_success"")
+    @mock.patch(""public_api.model_insights.controllers.PredictionStorage.find_record"")
+    @mock.patch(
+        ""public_api.model_insights.controllers.PredictJobService.""
+        ""validate_and_submit_non_stacked_request""
+    )
+    @mock.patch(
+        ""public_api.model_insights.controllers.ComputeFeatureImpactController.is_time_aware"",
+        new=True,
+    )
+    def test_valid_feature_impact__OTV_does_not_launch_training_predictions(
+        self,
+        mock_create_regular_preds,
+        mock_prediction_storage_find_record,
+        client,
+        feature_impact_post_request_dr_model,
+        mock_submit_compute_job,
+    ):
+        # GIVEN, no predictions found on storage, set up just like
+        # test_valid_feature_impact__launch_training_predictions except we mocked is_time_aware=True
+        mock_prediction_storage_find_record.side_effect = ResourceNotFoundError()
+        training_pred_qid = 10
+        mock_create_regular_preds.return_value = training_pred_qid
+
+        request_url = self.compute_sliced_insights_url.format(
+            insightName=InsightNames.FEATURE_IMPACT
+        )
+        request_data = feature_impact_post_request_dr_model
+        response = client.post(request_url, json=request_data)
+
+        assert response.status_code == 202
+
+        # THEN, check we do not call validate and launch regular predict job
+        mock_create_regular_preds.assert_not_called()
+
+        # AND, should submit job has status_id, no dependent_jobs_qid
+        # and has no training predictions qid.
+        _, kwargs = mock_submit_compute_job.call_args
+        assert kwargs[""status_id""] is not None
+        assert kwargs[""dependent_jobs_qids""] == []
+        assert kwargs[""training_predictions_qid""] == []
+
     @pytest.mark.usefixtures(""mock_validate_request_success"")
     @mock.patch(""public_api.model_insights.controllers.PredictionStorage.find_record"")
     @mock.patch(
diff --git a/trusted/jobs/insight_jobs.py b/trusted/jobs/insight_jobs.py
index 1784f24057af5e..bbf37ec6a1ecfd 100644
--- a/trusted/jobs/insight_jobs.py
+++ b/trusted/jobs/insight_jobs.py
@@ -184,6 +184,11 @@ def predictor(self):
                 )
             )
         ):
+            if (
+                command == JobTypeEnum.MODEL_INSIGHTS_FEATURE_IMPACT_FINALIZATION
+                and self.request.is_otp
+            ):
+                return FeatureImpactLeaderboardPredictor(self.request, self.api)
             return CachedPredictor(self.request, self.api)
         return LeaderboardPredictor(self.request, self.api)
 
@@ -563,9 +568,7 @@ def predict_synthetic(self, data):
         raise NotImplementedError
 
 
-class CachedPredictor(InsightPredictor):
-    """"""Used for when predictions have already been cached""""""
-
+class PredictionsContainerMixin:
     def extract_weights_and_exposures_columns(self, data):
         weight_col = self._request[""dr_special_columns_map""].get(""weight"")
         exposure_col = self._request[""dr_special_columns_map""].get(""exposure"")
@@ -593,6 +596,10 @@ def create_predictions_container(partition_tuple, **kwargs):
 
         return out
 
+
+class CachedPredictor(PredictionsContainerMixin, InsightPredictor):
+    """"""Used for when predictions have already been cached""""""
+
     def predict(self, data):
         project_id = self._request[""pid""]
         model_id = self._request[""lid""]
@@ -810,6 +817,32 @@ def config(self):
         return BinaryPredictorConfigs(self._threshold, self._positive_class, self._negative_class)
 
 
+class FeatureImpactLeaderboardPredictor(PredictionsContainerMixin, LeaderboardPredictor):
+    """"""This makes leaderboard predictions and adds the attributes to the output expected
+    for feature impact, similar to CachedPredictor which is used for AutoML feature impact.""""""
+
+    def predict(self, data):
+        container = super().predict(data)
+        r, k = get_first_partition(self._request[""partitions""])
+        partition_tuple = (r, k)
+        # TODO: TREX-3439 .flatten probably doesn't work for multiclass
+        predictions_series = pd.Series(
+            data=container(*partition_tuple).flatten(),
+            index=container.get(""row_index"", *partition_tuple),
+        )
+        weights_and_exposures_map = self.extract_weights_and_exposures_columns(data)
+        target = data[self._request[""target""]]
+        out = self.create_predictions_container(
+            partition_tuple=partition_tuple,
+            predictions=predictions_series.values,
+            row_index=predictions_series.index.values,
+            target=target,
+            sample_weight=data.sample_weight,
+            weights_and_exposures=weights_and_exposures_map,
+        )
+        return out
+
+
 class CustomModelPredictor(ModelPredictor):
     """"""Entity that makes predictions on a custom inference model.""""""
 "
"Sliced FI fails during finalization for ""identity"" slice","You can define a slice that does not remove any rows – I call this an “identity slice”. Results for all insights with an identity slice should be the same as when requesting unsliced insights.

This bug depends on some aspect of this particular dataset – I don’t see it with every dataset. For example, tests/testdata/multiseries.csv works fine

Steps to reproduce:

start a project using the dataset tests/testdata/fastiron-train-sample.csv, target SalePrice, OTV partitioning using the saledate feature.

train any model, compute unsliced feature impact

Create an identity slice. Here is one way to do it:

slice feature: Enclosure

slice filter: in and select all available options (==MISSING==, EROPS, EROPS w AC, OROPS)

Request feature impact using that slice, using “quick-compute”.

If you toggle off quick-compute, the job can succeed.

Actual result: Jobs run to the end and then fail suddenly during finalization

Expected result: Successful job yields a chart that is the same as unsliced.","Always use BuildInfoDatasetReader for the sliced feature impact jobs so we get samples with the same row index. Even better might be to store the row index while storing the synthetic feature predictions since that's all we need for the cached predictor, but that is a larger refactor.","diff --git a/trust_explainable_ai/model_insights/tests/backend/unit/ModelingMachine/jobs/secure/model_insights/test_insights_feature_impact_jobs.py b/trust_explainable_ai/model_insights/tests/backend/unit/ModelingMachine/jobs/secure/model_insights/test_insights_feature_impact_jobs.py
index e9e7e0ae17c38e..3a8ca5ebb1e082 100644
--- a/trust_explainable_ai/model_insights/tests/backend/unit/ModelingMachine/jobs/secure/model_insights/test_insights_feature_impact_jobs.py
+++ b/trust_explainable_ai/model_insights/tests/backend/unit/ModelingMachine/jobs/secure/model_insights/test_insights_feature_impact_jobs.py
@@ -34,7 +34,6 @@
 from ModelingMachine.engine.container import Container
 from trusted.jobs.insight_jobs import BuildInfoDatasetReader
 from trusted.jobs.insight_jobs import CachedPredictor
-from trusted.jobs.insight_jobs import InternalDatasetReader
 from trusted.jobs.insight_jobs import LeaderboardPredictor
 from trusted.jobs.insights_feature_impact_jobs import FeatureImpactFinalizationInsightJob
 from trusted.jobs.insights_feature_impact_jobs import FeatureImpactSyntheticInsightJob
@@ -388,8 +387,8 @@ def is_time_series_mock():
 
 
 class TestFeatureImpactFinalizationInsightJob(object):
-    @pytest.mark.usefixtures(""permissions_check_mock"")
-    def test_uses_cached_predictor_and_internal_dsreader(self, create_insight_job):
+    @pytest.mark.usefixtures(""permissions_check_mock"", ""mock_build_info_reader"")
+    def test_uses_cached_predictor_and_build_info_dsreader(self, create_insight_job):
         # construct a fi finalization request and job
         job_request = make_job_request(
             command=JobTypeEnum.MODEL_INSIGHTS_FEATURE_IMPACT_FINALIZATION,
@@ -405,10 +404,10 @@ def test_uses_cached_predictor_and_internal_dsreader(self, create_insight_job):
         # must use cached predictor to use stored predictions for slicing
         assert isinstance(job.predictor, CachedPredictor)
         # ensure we use the desired reader
-        assert isinstance(job.data_reader, InternalDatasetReader)
+        assert isinstance(job.data_reader, BuildInfoDatasetReader)
 
     @pytest.mark.usefixtures(
-        ""internal_dataset_reader_mock"",
+        ""mock_build_info_reader"",
         ""permissions_check_mock"",
         ""cached_predictor_mock"",
         ""mock_insight_calculator_run"",
diff --git a/trusted/jobs/insight_jobs.py b/trusted/jobs/insight_jobs.py
index b3f34c148d8e9b..ca618665980d0b 100644
--- a/trusted/jobs/insight_jobs.py
+++ b/trusted/jobs/insight_jobs.py
@@ -191,10 +191,10 @@ def data_reader(self):
         if self.request.get('training_dataset_id'):
             return CatalogInsightDatasetReader(self.request, self.api)
         command = self.request.get(""command"")
-        if (
-            self.request.get('pid')
-            and command == JobTypeEnum.MODEL_INSIGHTS_FEATURE_IMPACT_SYNTHETIC
-        ):
+        if self.request.get('pid') and command in [
+            JobTypeEnum.MODEL_INSIGHTS_FEATURE_IMPACT_SYNTHETIC,
+            JobTypeEnum.MODEL_INSIGHTS_FEATURE_IMPACT_FINALIZATION,
+        ]:
             return BuildInfoDatasetReader(self.request, self.api)
         if self.request.get(""external_dataset_id""):
             return ExternalTestDatasetReader(self.request, self.api)
diff --git a/trusted/jobs/insights_feature_impact_jobs.py b/trusted/jobs/insights_feature_impact_jobs.py
index 05cb79ebddb951..75ec3a6a0d459b 100644
--- a/trusted/jobs/insights_feature_impact_jobs.py
+++ b/trusted/jobs/insights_feature_impact_jobs.py
@@ -303,7 +303,7 @@ def run_insights(self):
 
     def collect_data(self):
         # Read raw data to then slice it and get indexes
-        data = self.data_reader.read(subset=self.request[""subset""])
+        data = self.data_reader.read()
         # this only applies a data slice and row count slicing
         data = self.permutate_data(data)
 ",,"Sliced FI fails during finalization for ""identity"" slice

You can define a slice that does not remove any rows – I call this an “identity slice”. Results for all insights with an identity slice should be the same as when requesting unsliced insights.

This bug depends on some aspect of this particular dataset – I don’t see it with every dataset. For example, tests/testdata/multiseries.csv works fine

Steps to reproduce:

start a project using the dataset tests/testdata/fastiron-train-sample.csv, target SalePrice, OTV partitioning using the saledate feature.

train any model, compute unsliced feature impact

Create an identity slice. Here is one way to do it:

slice feature: Enclosure

slice filter: in and select all available options (==MISSING==, EROPS, EROPS w AC, OROPS)

Request feature impact using that slice, using “quick-compute”.

If you toggle off quick-compute, the job can succeed.

Actual result: Jobs run to the end and then fail suddenly during finalization

Expected result: Successful job yields a chart that is the same as unsliced.","Always use BuildInfoDatasetReader for the sliced feature impact jobs so we get samples with the same row index. Even better might be to store the row index while storing the synthetic feature predictions since that's all we need for the cached predictor, but that is a larger refactor.

diff --git a/trust_explainable_ai/model_insights/tests/backend/unit/ModelingMachine/jobs/secure/model_insights/test_insights_feature_impact_jobs.py b/trust_explainable_ai/model_insights/tests/backend/unit/ModelingMachine/jobs/secure/model_insights/test_insights_feature_impact_jobs.py
index e9e7e0ae17c38e..3a8ca5ebb1e082 100644
--- a/trust_explainable_ai/model_insights/tests/backend/unit/ModelingMachine/jobs/secure/model_insights/test_insights_feature_impact_jobs.py
+++ b/trust_explainable_ai/model_insights/tests/backend/unit/ModelingMachine/jobs/secure/model_insights/test_insights_feature_impact_jobs.py
@@ -34,7 +34,6 @@
 from ModelingMachine.engine.container import Container
 from trusted.jobs.insight_jobs import BuildInfoDatasetReader
 from trusted.jobs.insight_jobs import CachedPredictor
-from trusted.jobs.insight_jobs import InternalDatasetReader
 from trusted.jobs.insight_jobs import LeaderboardPredictor
 from trusted.jobs.insights_feature_impact_jobs import FeatureImpactFinalizationInsightJob
 from trusted.jobs.insights_feature_impact_jobs import FeatureImpactSyntheticInsightJob
@@ -388,8 +387,8 @@ def is_time_series_mock():
 
 
 class TestFeatureImpactFinalizationInsightJob(object):
-    @pytest.mark.usefixtures(""permissions_check_mock"")
-    def test_uses_cached_predictor_and_internal_dsreader(self, create_insight_job):
+    @pytest.mark.usefixtures(""permissions_check_mock"", ""mock_build_info_reader"")
+    def test_uses_cached_predictor_and_build_info_dsreader(self, create_insight_job):
         # construct a fi finalization request and job
         job_request = make_job_request(
             command=JobTypeEnum.MODEL_INSIGHTS_FEATURE_IMPACT_FINALIZATION,
@@ -405,10 +404,10 @@ def test_uses_cached_predictor_and_internal_dsreader(self, create_insight_job):
         # must use cached predictor to use stored predictions for slicing
         assert isinstance(job.predictor, CachedPredictor)
         # ensure we use the desired reader
-        assert isinstance(job.data_reader, InternalDatasetReader)
+        assert isinstance(job.data_reader, BuildInfoDatasetReader)
 
     @pytest.mark.usefixtures(
-        ""internal_dataset_reader_mock"",
+        ""mock_build_info_reader"",
         ""permissions_check_mock"",
         ""cached_predictor_mock"",
         ""mock_insight_calculator_run"",
diff --git a/trusted/jobs/insight_jobs.py b/trusted/jobs/insight_jobs.py
index b3f34c148d8e9b..ca618665980d0b 100644
--- a/trusted/jobs/insight_jobs.py
+++ b/trusted/jobs/insight_jobs.py
@@ -191,10 +191,10 @@ def data_reader(self):
         if self.request.get('training_dataset_id'):
             return CatalogInsightDatasetReader(self.request, self.api)
         command = self.request.get(""command"")
-        if (
-            self.request.get('pid')
-            and command == JobTypeEnum.MODEL_INSIGHTS_FEATURE_IMPACT_SYNTHETIC
-        ):
+        if self.request.get('pid') and command in [
+            JobTypeEnum.MODEL_INSIGHTS_FEATURE_IMPACT_SYNTHETIC,
+            JobTypeEnum.MODEL_INSIGHTS_FEATURE_IMPACT_FINALIZATION,
+        ]:
             return BuildInfoDatasetReader(self.request, self.api)
         if self.request.get(""external_dataset_id""):
             return ExternalTestDatasetReader(self.request, self.api)
diff --git a/trusted/jobs/insights_feature_impact_jobs.py b/trusted/jobs/insights_feature_impact_jobs.py
index 05cb79ebddb951..75ec3a6a0d459b 100644
--- a/trusted/jobs/insights_feature_impact_jobs.py
+++ b/trusted/jobs/insights_feature_impact_jobs.py
@@ -303,7 +303,7 @@ def run_insights(self):
 
     def collect_data(self):
         # Read raw data to then slice it and get indexes
-        data = self.data_reader.read(subset=self.request[""subset""])
+        data = self.data_reader.read()
         # this only applies a data slice and row count slicing
         data = self.permutate_data(data)
 "
"Cannot install again a deleted via DR-WebUI custom environment, error 'ID already exists'","I did the following actions:

Uploaded our custom environments into DR 9.1.3

Deleted one of them (turn public off, after delete)

Tried to reinstall the custom environment again

Received the following error:



Could not create /opt/datarobot/DataRobot-9.1.3/custom-models1/java_codegen. Response has status 409. Response: {'message': 'An environment with the supplied ID already exists'}
Traceback (most recent call last):
  File ""install_public_environments.py"", line 252, in <module>
    install_images(args.BASE_URL, args.API_TOKEN, args.fail_on_conflict)
  File ""install_public_environments.py"", line 171, in install_images
    raise ValueError(""An error occurred during environment installation"")
ValueError: An error occurred during environment installation
It looks like a custom environment deletion does not clean records in the database properly.

(We ran into a similar problem in the Apple UAT environment before, so I reproduced it in EKS DR 9.1.3","Enable to re-create a drop-in environment after it was deleted by the user. We'll make a best effort to cleanup all associated entities that belonged to the deleted drop-in environment. The new drop-in environment, which has the same MongoDB ID, is considered a totally new drop-in environment.
Allow creation of an already deleted drop-in environment
Integration test","diff --git a/public_api/execution_environment/controllers.py b/public_api/execution_environment/controllers.py
index db89d37eafd77a..18db0ab42e11c7 100644
--- a/public_api/execution_environment/controllers.py
+++ b/public_api/execution_environment/controllers.py
@@ -416,8 +416,18 @@ def validator_valid(self, validator):
                     )
                 )
             try:
-                env_service.get(pre_assigned_id, show_deleted=True)
-                raise Conflict(self.other_status_codes[status.HTTP_409_CONFLICT])
+                execution_environment = env_service.get(pre_assigned_id, show_deleted=True)
+                if execution_environment.is_deleted:
+                    # If the user decided to reinstall an already deleted execution environment,
+                    # we want to make the best effort to destroy all existing associated entities.
+                    # The new execution environment is considered a totally new entity.
+                    permadelete_service = ExecutionEnvironmentPermadeleteService(self.user.uid)
+                    try:
+                        permadelete_service.destroy([pre_assigned_id])
+                    except (ResourceNotFoundError, UnprocessableRequestError):
+                        pass
+                else:
+                    raise Conflict(self.other_status_codes[status.HTTP_409_CONFLICT])
             except ResourceNotFoundError:
                 pass
             validator.data['id'] = validator.data.pop('environment_id')
diff --git a/tests/backend/integration/public_api/execution_environment/test_execution_environment_api.py b/tests/backend/integration/public_api/execution_environment/test_execution_environment_api.py
index 9e9540588a9660..2f8d78caf62a9b 100644
--- a/tests/backend/integration/public_api/execution_environment/test_execution_environment_api.py
+++ b/tests/backend/integration/public_api/execution_environment/test_execution_environment_api.py
@@ -440,28 +440,35 @@ def test__create__pre_assigned_id(self, request, client, ee_user):
             resp = client.post(URL_ENV_LIST, json=payload)
             assert 409 == resp.status_code, str(resp.data)
 
-    def test__create__deleted_pre_assigned_id(
-        self, admin_client, admin_user, execution_environment_factory
+    def test__create__deleted_with_assigned_id(
+        self, unittest_persistent_db, admin_client, admin_user, execution_environment_factory
     ):
         client = admin_client
         ee_user = admin_user
-        ee_service = ExecutionEnvironmentService(user_id=ee_user.uid)
+        ee_service = ExecutionEnvironmentService(
+            user_id=ee_user.uid, persistent=unittest_persistent_db
+        )
 
-        env = execution_environment_factory(user_id=ee_user.uid, is_public=False)
+        env = execution_environment_factory(
+            name='origin_test_name', user_id=ee_user.uid, is_public=False
+        )
         ee_service.delete(env.id)
 
         payload = {
             'environmentId': str(env.id),
-            'name': 'test_name',
-            'description': 'test_description',
+            'name': 'new_test_name',
+            'description': 'new_test_description',
             'programmingLanguage': 'python',
         }
 
-        # trying to create an environment with a predefined
-        # ID that has been deleted should be a conflict
         resp = client.post(URL_ENV_LIST, json=payload)
+        assert 201 == resp.status_code, str(resp.data)
+        assert str(env.id) == resp.json['id']
 
-        assert 409 == resp.status_code, str(resp.data)
+        environment = ee_service.get(env.id)
+        assert environment.name == 'new_test_name'
+        assert environment.description == 'new_test_description'
+        assert environment.programming_language == 'python'
 
     @pytest.mark.parametrize(
         'client, ee_user', [('v2_client', 'v2_user'), ('admin_client', 'admin_user')]",,"Cannot install again a deleted via DR-WebUI custom environment, error 'ID already exists'

I did the following actions:

Uploaded our custom environments into DR 9.1.3

Deleted one of them (turn public off, after delete)

Tried to reinstall the custom environment again

Received the following error:



Could not create /opt/datarobot/DataRobot-9.1.3/custom-models1/java_codegen. Response has status 409. Response: {'message': 'An environment with the supplied ID already exists'}
Traceback (most recent call last):
  File ""install_public_environments.py"", line 252, in <module>
    install_images(args.BASE_URL, args.API_TOKEN, args.fail_on_conflict)
  File ""install_public_environments.py"", line 171, in install_images
    raise ValueError(""An error occurred during environment installation"")
ValueError: An error occurred during environment installation
It looks like a custom environment deletion does not clean records in the database properly.

(We ran into a similar problem in the Apple UAT environment before, so I reproduced it in EKS DR 9.1.3","Enable to re-create a drop-in environment after it was deleted by the user. We'll make a best effort to cleanup all associated entities that belonged to the deleted drop-in environment. The new drop-in environment, which has the same MongoDB ID, is considered a totally new drop-in environment.
Allow creation of an already deleted drop-in environment
Integration test

diff --git a/public_api/execution_environment/controllers.py b/public_api/execution_environment/controllers.py
index db89d37eafd77a..18db0ab42e11c7 100644
--- a/public_api/execution_environment/controllers.py
+++ b/public_api/execution_environment/controllers.py
@@ -416,8 +416,18 @@ def validator_valid(self, validator):
                     )
                 )
             try:
-                env_service.get(pre_assigned_id, show_deleted=True)
-                raise Conflict(self.other_status_codes[status.HTTP_409_CONFLICT])
+                execution_environment = env_service.get(pre_assigned_id, show_deleted=True)
+                if execution_environment.is_deleted:
+                    # If the user decided to reinstall an already deleted execution environment,
+                    # we want to make the best effort to destroy all existing associated entities.
+                    # The new execution environment is considered a totally new entity.
+                    permadelete_service = ExecutionEnvironmentPermadeleteService(self.user.uid)
+                    try:
+                        permadelete_service.destroy([pre_assigned_id])
+                    except (ResourceNotFoundError, UnprocessableRequestError):
+                        pass
+                else:
+                    raise Conflict(self.other_status_codes[status.HTTP_409_CONFLICT])
             except ResourceNotFoundError:
                 pass
             validator.data['id'] = validator.data.pop('environment_id')
diff --git a/tests/backend/integration/public_api/execution_environment/test_execution_environment_api.py b/tests/backend/integration/public_api/execution_environment/test_execution_environment_api.py
index 9e9540588a9660..2f8d78caf62a9b 100644
--- a/tests/backend/integration/public_api/execution_environment/test_execution_environment_api.py
+++ b/tests/backend/integration/public_api/execution_environment/test_execution_environment_api.py
@@ -440,28 +440,35 @@ def test__create__pre_assigned_id(self, request, client, ee_user):
             resp = client.post(URL_ENV_LIST, json=payload)
             assert 409 == resp.status_code, str(resp.data)
 
-    def test__create__deleted_pre_assigned_id(
-        self, admin_client, admin_user, execution_environment_factory
+    def test__create__deleted_with_assigned_id(
+        self, unittest_persistent_db, admin_client, admin_user, execution_environment_factory
     ):
         client = admin_client
         ee_user = admin_user
-        ee_service = ExecutionEnvironmentService(user_id=ee_user.uid)
+        ee_service = ExecutionEnvironmentService(
+            user_id=ee_user.uid, persistent=unittest_persistent_db
+        )
 
-        env = execution_environment_factory(user_id=ee_user.uid, is_public=False)
+        env = execution_environment_factory(
+            name='origin_test_name', user_id=ee_user.uid, is_public=False
+        )
         ee_service.delete(env.id)
 
         payload = {
             'environmentId': str(env.id),
-            'name': 'test_name',
-            'description': 'test_description',
+            'name': 'new_test_name',
+            'description': 'new_test_description',
             'programmingLanguage': 'python',
         }
 
-        # trying to create an environment with a predefined
-        # ID that has been deleted should be a conflict
         resp = client.post(URL_ENV_LIST, json=payload)
+        assert 201 == resp.status_code, str(resp.data)
+        assert str(env.id) == resp.json['id']
 
-        assert 409 == resp.status_code, str(resp.data)
+        environment = ee_service.get(env.id)
+        assert environment.name == 'new_test_name'
+        assert environment.description == 'new_test_description'
+        assert environment.programming_language == 'python'
 
     @pytest.mark.parametrize(
         'client, ee_user', [('v2_client', 'v2_user'), ('admin_client', 'admin_user')]"
Weird behaviour of key value artifact's Description,"Actual Behaviour: 

After Uploading new artifact - Description of that key value is not saved (there will be ""No description"" message for such artefact on model-package-key-values-table)

While Editing existing artifact - there always will be nothing at Description field (even though description was provided and can be seen for such artefact on model-package-key-values-table)

 

Expected Behaviour: 

Any Key value, that user create, can be successfully added, regardless of artifact's accepted format, or key value's Category/Value/etc.

Image key values artifacts can be viewed in the expanded modal pop-up window (by clicking ""View image"" button)

Archive-type key values artifacts can be viewed in AI Catalog (by clicking ""View in AI Catalog"" button)

While adding or editing any valid key-value pair - Description is an Optional field, but since it was set - it can be appropriately reflected on UI at artefact on model-package-key-values-table, ""Add new key value"" modal window, and ""Edit existing artifact"" modal window.

Steps to reproduce


1. Create a new (or use existing) Binary Classification project
2. Go to Leaderboard and pick top-model → Predict → Deploy → Add to Model Registry
3. Click on created Model Package to get redirected to ""Model Registry → Model Packages → [top model package]"" page
4. Switch to ""Key values"" tab → + Add key values → Upload an artifact
5. Choose file → Local file
- (upload any image)
- Description Optional: ""test description""
- OK
6. Observe appearance of newly added key value artefact
7. OPTIONAL: Try to Edit existing artifact → Check ""Description Optional"" field","We were not passing in all params (description) when creating a model package key value in the UI.

Pass through description when creating model package key value
Unit tests for adding description and ensuring description appears from selected model packages when editing key values.","diff --git a/client/js/model-registry/model-packages/model-packages-table/model-package-key-values/model-package-key-values.js b/client/js/model-registry/model-packages/model-packages-table/model-package-key-values/model-package-key-values.js
index 0139b1d61865ba..9a9dc42a2a0ce5 100644
--- a/client/js/model-registry/model-packages/model-packages-table/model-package-key-values/model-package-key-values.js
+++ b/client/js/model-registry/model-packages/model-packages-table/model-package-key-values/model-package-key-values.js
@@ -132,7 +132,7 @@ const ModelPackageKeyValues = ({ modelPackage }) => {
     }
   };
 
-  const handleUploadFile = async ({ file, valueType, name }) => {
+  const handleUploadFile = async (params) => {
     const saveFunc = isEditing
       ? editPackageKeyValueFromFile
       : createPackageKeyValueFromFile;
@@ -142,10 +142,8 @@ const ModelPackageKeyValues = ({ modelPackage }) => {
     try {
       await saveFunc({
         ...(isEditing ? { id: selectedKeyValue.id } : {}),
-        file,
-        valueType,
+        ...params,
         category: CATEGORIES.ARTIFACT,
-        name,
       });
       setIsLoadingRequest(false);
       await onRequestComplete({ category: CATEGORIES.ARTIFACT });
diff --git a/client/js/model-registry/model-packages/model-packages-table/model-package-key-values/model-package-key-values.test.js b/client/js/model-registry/model-packages/model-packages-table/model-package-key-values/model-package-key-values.test.js
index f2bbf199083389..cdc36c823a44f9 100644
--- a/client/js/model-registry/model-packages/model-packages-table/model-package-key-values/model-package-key-values.test.js
+++ b/client/js/model-registry/model-packages/model-packages-table/model-package-key-values/model-package-key-values.test.js
@@ -494,6 +494,9 @@ describe('<ModelPackageKeyValues />', function () {
       fireEvent.change(artifactNameInput, {
         target: { value: 'new_name.csv' },
       });
+      fireEvent.change(screen.getByTestId('artifact-description'), {
+        target: { value: 'simple_description' },
+      });
       fireEvent.click(screen.getByTestId('modal-close'));
 
       await waitFor(() =>
@@ -506,6 +509,7 @@ describe('<ModelPackageKeyValues />', function () {
             file,
             name: 'new_name.csv',
             valueType: 'dataset',
+            description: 'simple_description',
           },
           expect.anything()
         )
@@ -616,6 +620,9 @@ describe('<ModelPackageKeyValues />', function () {
       expect(await screen.findByTestId('artifact-name')).toHaveValue(
         KEY_VALUE_DATASET.name
       );
+      expect(await screen.findByTestId('artifact-description')).toHaveValue(
+        KEY_VALUE_DATASET.description
+      );
 
       // Remove selected dataset.
       fireEvent.click(
@@ -626,6 +633,10 @@ describe('<ModelPackageKeyValues />', function () {
 
       const file = await dragNDropFile();
 
+      fireEvent.change(screen.getByTestId('artifact-description'), {
+        target: { value: 'simple_description' },
+      });
+
       fireEvent.click(screen.getByTestId('modal-close'));
 
       await waitFor(() =>
@@ -639,6 +650,7 @@ describe('<ModelPackageKeyValues />', function () {
             valueType: 'dataset',
             file,
             name: DEFAULT_FILE_NAME,
+            description: 'simple_description',
           },
           expect.anything()
         )
diff --git a/client/js/model-registry/model-packages/model-packages-table/model-package-key-values/upload-key-value-file-modal.js b/client/js/model-registry/model-packages/model-packages-table/model-package-key-values/upload-key-value-file-modal.js
index 09af403778361a..47aaa8ae0999f5 100644
--- a/client/js/model-registry/model-packages/model-packages-table/model-package-key-values/upload-key-value-file-modal.js
+++ b/client/js/model-registry/model-packages/model-packages-table/model-package-key-values/upload-key-value-file-modal.js
@@ -77,6 +77,7 @@ const UploadKeyValueFileModal = ({
       originalFileName: file.name,
       artifactName: file.artifactName || file.name,
       valueType: getValueType(file.name.split('.').pop()),
+      description: file.description,
     }));
 
     return Promise.resolve(file);
@@ -106,6 +107,7 @@ const UploadKeyValueFileModal = ({
           name: selectedKeyValue.originalFileName || selectedKeyValue.name,
           size: selectedKeyValue.artifactSize,
           artifactName: selectedKeyValue.name,
+          description: selectedKeyValue.description,
         },
       ]
     : [];
@@ -148,7 +150,7 @@ const UploadKeyValueFileModal = ({
         fieldTestId=""artifact-name""
         label={_t('Artifact name')}
         name=""artifactName""
-        value={state.artifactName}
+        value={state.artifactName || ''}
         onChange={onChange}
         isDisabled={isDisabled}
       />
@@ -158,7 +160,7 @@ const UploadKeyValueFileModal = ({
         fieldTestId=""artifact-description""
         label={_t('Description')}
         name=""description""
-        value={state.description}
+        value={state.description || ''}
         onChange={onChange}
         isDisabled={isDisabled}
         isOptional
diff --git a/tests/js/unit/mocks/model-package-mocks.js b/tests/js/unit/mocks/model-package-mocks.js
index 8935e936d3af3c..c9fe7a61ee26f9 100644
--- a/tests/js/unit/mocks/model-package-mocks.js
+++ b/tests/js/unit/mocks/model-package-mocks.js
@@ -536,7 +536,7 @@ export const KEY_VALUE_DATASET = {
   value: '123123123123',
   valueType: 'dataset',
   category: 'artifact',
-  description: '',
+  description: 'dataset_description',
   id: '638928bd77e6bdc5cf57d871',
 };
 ",,"Weird behaviour of key value artifact's Description

Actual Behaviour: 

After Uploading new artifact - Description of that key value is not saved (there will be ""No description"" message for such artefact on model-package-key-values-table)

While Editing existing artifact - there always will be nothing at Description field (even though description was provided and can be seen for such artefact on model-package-key-values-table)

 

Expected Behaviour: 

Any Key value, that user create, can be successfully added, regardless of artifact's accepted format, or key value's Category/Value/etc.

Image key values artifacts can be viewed in the expanded modal pop-up window (by clicking ""View image"" button)

Archive-type key values artifacts can be viewed in AI Catalog (by clicking ""View in AI Catalog"" button)

While adding or editing any valid key-value pair - Description is an Optional field, but since it was set - it can be appropriately reflected on UI at artefact on model-package-key-values-table, ""Add new key value"" modal window, and ""Edit existing artifact"" modal window.

Steps to reproduce


1. Create a new (or use existing) Binary Classification project
2. Go to Leaderboard and pick top-model → Predict → Deploy → Add to Model Registry
3. Click on created Model Package to get redirected to ""Model Registry → Model Packages → [top model package]"" page
4. Switch to ""Key values"" tab → + Add key values → Upload an artifact
5. Choose file → Local file
- (upload any image)
- Description Optional: ""test description""
- OK
6. Observe appearance of newly added key value artefact
7. OPTIONAL: Try to Edit existing artifact → Check ""Description Optional"" field","We were not passing in all params (description) when creating a model package key value in the UI.

Pass through description when creating model package key value
Unit tests for adding description and ensuring description appears from selected model packages when editing key values.

diff --git a/client/js/model-registry/model-packages/model-packages-table/model-package-key-values/model-package-key-values.js b/client/js/model-registry/model-packages/model-packages-table/model-package-key-values/model-package-key-values.js
index 0139b1d61865ba..9a9dc42a2a0ce5 100644
--- a/client/js/model-registry/model-packages/model-packages-table/model-package-key-values/model-package-key-values.js
+++ b/client/js/model-registry/model-packages/model-packages-table/model-package-key-values/model-package-key-values.js
@@ -132,7 +132,7 @@ const ModelPackageKeyValues = ({ modelPackage }) => {
     }
   };
 
-  const handleUploadFile = async ({ file, valueType, name }) => {
+  const handleUploadFile = async (params) => {
     const saveFunc = isEditing
       ? editPackageKeyValueFromFile
       : createPackageKeyValueFromFile;
@@ -142,10 +142,8 @@ const ModelPackageKeyValues = ({ modelPackage }) => {
     try {
       await saveFunc({
         ...(isEditing ? { id: selectedKeyValue.id } : {}),
-        file,
-        valueType,
+        ...params,
         category: CATEGORIES.ARTIFACT,
-        name,
       });
       setIsLoadingRequest(false);
       await onRequestComplete({ category: CATEGORIES.ARTIFACT });
diff --git a/client/js/model-registry/model-packages/model-packages-table/model-package-key-values/model-package-key-values.test.js b/client/js/model-registry/model-packages/model-packages-table/model-package-key-values/model-package-key-values.test.js
index f2bbf199083389..cdc36c823a44f9 100644
--- a/client/js/model-registry/model-packages/model-packages-table/model-package-key-values/model-package-key-values.test.js
+++ b/client/js/model-registry/model-packages/model-packages-table/model-package-key-values/model-package-key-values.test.js
@@ -494,6 +494,9 @@ describe('<ModelPackageKeyValues />', function () {
       fireEvent.change(artifactNameInput, {
         target: { value: 'new_name.csv' },
       });
+      fireEvent.change(screen.getByTestId('artifact-description'), {
+        target: { value: 'simple_description' },
+      });
       fireEvent.click(screen.getByTestId('modal-close'));
 
       await waitFor(() =>
@@ -506,6 +509,7 @@ describe('<ModelPackageKeyValues />', function () {
             file,
             name: 'new_name.csv',
             valueType: 'dataset',
+            description: 'simple_description',
           },
           expect.anything()
         )
@@ -616,6 +620,9 @@ describe('<ModelPackageKeyValues />', function () {
       expect(await screen.findByTestId('artifact-name')).toHaveValue(
         KEY_VALUE_DATASET.name
       );
+      expect(await screen.findByTestId('artifact-description')).toHaveValue(
+        KEY_VALUE_DATASET.description
+      );
 
       // Remove selected dataset.
       fireEvent.click(
@@ -626,6 +633,10 @@ describe('<ModelPackageKeyValues />', function () {
 
       const file = await dragNDropFile();
 
+      fireEvent.change(screen.getByTestId('artifact-description'), {
+        target: { value: 'simple_description' },
+      });
+
       fireEvent.click(screen.getByTestId('modal-close'));
 
       await waitFor(() =>
@@ -639,6 +650,7 @@ describe('<ModelPackageKeyValues />', function () {
             valueType: 'dataset',
             file,
             name: DEFAULT_FILE_NAME,
+            description: 'simple_description',
           },
           expect.anything()
         )
diff --git a/client/js/model-registry/model-packages/model-packages-table/model-package-key-values/upload-key-value-file-modal.js b/client/js/model-registry/model-packages/model-packages-table/model-package-key-values/upload-key-value-file-modal.js
index 09af403778361a..47aaa8ae0999f5 100644
--- a/client/js/model-registry/model-packages/model-packages-table/model-package-key-values/upload-key-value-file-modal.js
+++ b/client/js/model-registry/model-packages/model-packages-table/model-package-key-values/upload-key-value-file-modal.js
@@ -77,6 +77,7 @@ const UploadKeyValueFileModal = ({
       originalFileName: file.name,
       artifactName: file.artifactName || file.name,
       valueType: getValueType(file.name.split('.').pop()),
+      description: file.description,
     }));
 
     return Promise.resolve(file);
@@ -106,6 +107,7 @@ const UploadKeyValueFileModal = ({
           name: selectedKeyValue.originalFileName || selectedKeyValue.name,
           size: selectedKeyValue.artifactSize,
           artifactName: selectedKeyValue.name,
+          description: selectedKeyValue.description,
         },
       ]
     : [];
@@ -148,7 +150,7 @@ const UploadKeyValueFileModal = ({
         fieldTestId=""artifact-name""
         label={_t('Artifact name')}
         name=""artifactName""
-        value={state.artifactName}
+        value={state.artifactName || ''}
         onChange={onChange}
         isDisabled={isDisabled}
       />
@@ -158,7 +160,7 @@ const UploadKeyValueFileModal = ({
         fieldTestId=""artifact-description""
         label={_t('Description')}
         name=""description""
-        value={state.description}
+        value={state.description || ''}
         onChange={onChange}
         isDisabled={isDisabled}
         isOptional
diff --git a/tests/js/unit/mocks/model-package-mocks.js b/tests/js/unit/mocks/model-package-mocks.js
index 8935e936d3af3c..c9fe7a61ee26f9 100644
--- a/tests/js/unit/mocks/model-package-mocks.js
+++ b/tests/js/unit/mocks/model-package-mocks.js
@@ -536,7 +536,7 @@ export const KEY_VALUE_DATASET = {
   value: '123123123123',
   valueType: 'dataset',
   category: 'artifact',
-  description: '',
+  description: 'dataset_description',
   id: '638928bd77e6bdc5cf57d871',
 };
 "
Error retrieving cluster insight for column with 100% missing values,"When column in dataset has all missing values, and when such column is used in clustering project for computation of clustering insights. Than when retrieved via Api all metrics have null values except for metric missingRowsPercent which has value 100. This situation is not handled correctly on python client api when computing and retrieveing insights using method: insights = model.compute_insights()","Fixed bug for unsupervised clustering project type.
Scenario: When dataset has empty column (100% rows missing) and clustering insights are calculated and retrieved for all features (including empty column). Method throws error due to validation constraints in trafaret.
Optionality added to validation is limited to fields needed to retrieve such insights when invoking: model.compute_insights() for clustering models.
- added unittest reproducing this scenario
- added fix","diff --git a/datarobot/models/cluster_insight.py b/datarobot/models/cluster_insight.py
index c1db1c0ebd..c9651329bb 100644
--- a/datarobot/models/cluster_insight.py
+++ b/datarobot/models/cluster_insight.py
@@ -18,12 +18,13 @@
 from datarobot.utils.waiters import wait_for_async_resolution
 
 # Numeric features.
-
-numeric_insight = t.Dict({t.Key(""statistic""): t.Float(), t.Key(""cluster_name""): String()})
+numeric_insight = t.Dict(
+    {t.Key(""statistic"", optional=True): t.Float(), t.Key(""cluster_name""): String()}
+)
 
 numeric_insight_per_cluster = t.Dict(
     {
-        t.Key(""all_data""): t.Float(),
+        t.Key(""all_data"", optional=True): t.Float(),
         t.Key(""per_cluster""): t.List(numeric_insight),
         t.Key(""insight_name""): String(),
     }
diff --git a/tests/cluster/conftest.py b/tests/cluster/conftest.py
index 96c7facd6d..e6710bdb18 100644
--- a/tests/cluster/conftest.py
+++ b/tests/cluster/conftest.py
@@ -181,6 +181,80 @@ def cluster_insights_numeric_feature__no_feature_impact(cluster_insights_numeric
     return feature_impact_missing(cluster_insights_numeric_feature)
 
 
+@pytest.fixture
+def cluster_insight_numeric_feature__all_rows_missing():
+    return {
+        ""featureName"": ""term (Numeric)"",
+        ""featureType"": ""numeric"",
+        ""insights"": [
+            {
+                ""allData"": None,
+                ""perCluster"": [
+                    {""statistic"": None, ""clusterName"": ""Cluster 1""},
+                    {""statistic"": None, ""clusterName"": ""Cluster 2""},
+                    {""statistic"": None, ""clusterName"": ""Cluster 3""},
+                ],
+                ""insightName"": ""avg"",
+            },
+            {
+                ""allData"": None,
+                ""perCluster"": [
+                    {""statistic"": None, ""clusterName"": ""Cluster 1""},
+                    {""statistic"": None, ""clusterName"": ""Cluster 2""},
+                    {""statistic"": None, ""clusterName"": ""Cluster 3""},
+                ],
+                ""insightName"": ""firstQuartile"",
+            },
+            {
+                ""allData"": None,
+                ""perCluster"": [
+                    {""statistic"": None, ""clusterName"": ""Cluster 1""},
+                    {""statistic"": None, ""clusterName"": ""Cluster 2""},
+                    {""statistic"": None, ""clusterName"": ""Cluster 3""},
+                ],
+                ""insightName"": ""max"",
+            },
+            {
+                ""allData"": None,
+                ""perCluster"": [
+                    {""statistic"": None, ""clusterName"": ""Cluster 1""},
+                    {""statistic"": None, ""clusterName"": ""Cluster 2""},
+                    {""statistic"": None, ""clusterName"": ""Cluster 3""},
+                ],
+                ""insightName"": ""median"",
+            },
+            {
+                ""allData"": None,
+                ""perCluster"": [
+                    {""statistic"": None, ""clusterName"": ""Cluster 1""},
+                    {""statistic"": None, ""clusterName"": ""Cluster 2""},
+                    {""statistic"": None, ""clusterName"": ""Cluster 3""},
+                ],
+                ""insightName"": ""min"",
+            },
+            {
+                ""allData"": 100.0,
+                ""perCluster"": [
+                    {""statistic"": 100.0, ""clusterName"": ""Cluster 1""},
+                    {""statistic"": 100.0, ""clusterName"": ""Cluster 2""},
+                    {""statistic"": 100.0, ""clusterName"": ""Cluster 3""},
+                ],
+                ""insightName"": ""missingRowsPercent"",
+            },
+            {
+                ""allData"": None,
+                ""perCluster"": [
+                    {""statistic"": None, ""clusterName"": ""Cluster 1""},
+                    {""statistic"": None, ""clusterName"": ""Cluster 2""},
+                    {""statistic"": None, ""clusterName"": ""Cluster 3""},
+                ],
+                ""insightName"": ""thirdQuartile"",
+            },
+        ],
+        ""featureImpact"": None,
+    }
+
+
 @pytest.fixture
 def cluster_insights_categorical_feature():
     return {
diff --git a/tests/cluster/test_cluster_insight.py b/tests/cluster/test_cluster_insight.py
index fdf40b7c63..dcb8b03f30 100644
--- a/tests/cluster/test_cluster_insight.py
+++ b/tests/cluster/test_cluster_insight.py
@@ -33,7 +33,11 @@ def assert_equal(self, cluster_insight, dict_data):
 
     @pytest.mark.parametrize(
         ""insight_fixture"",
-        [""cluster_insights_numeric_feature"", ""cluster_insights_numeric_feature__no_feature_impact""],
+        [
+            ""cluster_insights_numeric_feature"",
+            ""cluster_insights_numeric_feature__no_feature_impact"",
+            ""cluster_insight_numeric_feature__all_rows_missing"",
+        ],
     )
     def test_numeric_instantiation(self, insight_fixture, request):
         """"""",,"Error retrieving cluster insight for column with 100% missing values

When column in dataset has all missing values, and when such column is used in clustering project for computation of clustering insights. Than when retrieved via Api all metrics have null values except for metric missingRowsPercent which has value 100. This situation is not handled correctly on python client api when computing and retrieveing insights using method: insights = model.compute_insights()","Fixed bug for unsupervised clustering project type.
Scenario: When dataset has empty column (100% rows missing) and clustering insights are calculated and retrieved for all features (including empty column). Method throws error due to validation constraints in trafaret.
Optionality added to validation is limited to fields needed to retrieve such insights when invoking: model.compute_insights() for clustering models.
- added unittest reproducing this scenario
- added fix

diff --git a/datarobot/models/cluster_insight.py b/datarobot/models/cluster_insight.py
index c1db1c0ebd..c9651329bb 100644
--- a/datarobot/models/cluster_insight.py
+++ b/datarobot/models/cluster_insight.py
@@ -18,12 +18,13 @@
 from datarobot.utils.waiters import wait_for_async_resolution
 
 # Numeric features.
-
-numeric_insight = t.Dict({t.Key(""statistic""): t.Float(), t.Key(""cluster_name""): String()})
+numeric_insight = t.Dict(
+    {t.Key(""statistic"", optional=True): t.Float(), t.Key(""cluster_name""): String()}
+)
 
 numeric_insight_per_cluster = t.Dict(
     {
-        t.Key(""all_data""): t.Float(),
+        t.Key(""all_data"", optional=True): t.Float(),
         t.Key(""per_cluster""): t.List(numeric_insight),
         t.Key(""insight_name""): String(),
     }
diff --git a/tests/cluster/conftest.py b/tests/cluster/conftest.py
index 96c7facd6d..e6710bdb18 100644
--- a/tests/cluster/conftest.py
+++ b/tests/cluster/conftest.py
@@ -181,6 +181,80 @@ def cluster_insights_numeric_feature__no_feature_impact(cluster_insights_numeric
     return feature_impact_missing(cluster_insights_numeric_feature)
 
 
+@pytest.fixture
+def cluster_insight_numeric_feature__all_rows_missing():
+    return {
+        ""featureName"": ""term (Numeric)"",
+        ""featureType"": ""numeric"",
+        ""insights"": [
+            {
+                ""allData"": None,
+                ""perCluster"": [
+                    {""statistic"": None, ""clusterName"": ""Cluster 1""},
+                    {""statistic"": None, ""clusterName"": ""Cluster 2""},
+                    {""statistic"": None, ""clusterName"": ""Cluster 3""},
+                ],
+                ""insightName"": ""avg"",
+            },
+            {
+                ""allData"": None,
+                ""perCluster"": [
+                    {""statistic"": None, ""clusterName"": ""Cluster 1""},
+                    {""statistic"": None, ""clusterName"": ""Cluster 2""},
+                    {""statistic"": None, ""clusterName"": ""Cluster 3""},
+                ],
+                ""insightName"": ""firstQuartile"",
+            },
+            {
+                ""allData"": None,
+                ""perCluster"": [
+                    {""statistic"": None, ""clusterName"": ""Cluster 1""},
+                    {""statistic"": None, ""clusterName"": ""Cluster 2""},
+                    {""statistic"": None, ""clusterName"": ""Cluster 3""},
+                ],
+                ""insightName"": ""max"",
+            },
+            {
+                ""allData"": None,
+                ""perCluster"": [
+                    {""statistic"": None, ""clusterName"": ""Cluster 1""},
+                    {""statistic"": None, ""clusterName"": ""Cluster 2""},
+                    {""statistic"": None, ""clusterName"": ""Cluster 3""},
+                ],
+                ""insightName"": ""median"",
+            },
+            {
+                ""allData"": None,
+                ""perCluster"": [
+                    {""statistic"": None, ""clusterName"": ""Cluster 1""},
+                    {""statistic"": None, ""clusterName"": ""Cluster 2""},
+                    {""statistic"": None, ""clusterName"": ""Cluster 3""},
+                ],
+                ""insightName"": ""min"",
+            },
+            {
+                ""allData"": 100.0,
+                ""perCluster"": [
+                    {""statistic"": 100.0, ""clusterName"": ""Cluster 1""},
+                    {""statistic"": 100.0, ""clusterName"": ""Cluster 2""},
+                    {""statistic"": 100.0, ""clusterName"": ""Cluster 3""},
+                ],
+                ""insightName"": ""missingRowsPercent"",
+            },
+            {
+                ""allData"": None,
+                ""perCluster"": [
+                    {""statistic"": None, ""clusterName"": ""Cluster 1""},
+                    {""statistic"": None, ""clusterName"": ""Cluster 2""},
+                    {""statistic"": None, ""clusterName"": ""Cluster 3""},
+                ],
+                ""insightName"": ""thirdQuartile"",
+            },
+        ],
+        ""featureImpact"": None,
+    }
+
+
 @pytest.fixture
 def cluster_insights_categorical_feature():
     return {
diff --git a/tests/cluster/test_cluster_insight.py b/tests/cluster/test_cluster_insight.py
index fdf40b7c63..dcb8b03f30 100644
--- a/tests/cluster/test_cluster_insight.py
+++ b/tests/cluster/test_cluster_insight.py
@@ -33,7 +33,11 @@ def assert_equal(self, cluster_insight, dict_data):
 
     @pytest.mark.parametrize(
         ""insight_fixture"",
-        [""cluster_insights_numeric_feature"", ""cluster_insights_numeric_feature__no_feature_impact""],
+        [
+            ""cluster_insights_numeric_feature"",
+            ""cluster_insights_numeric_feature__no_feature_impact"",
+            ""cluster_insight_numeric_feature__all_rows_missing"",
+        ],
     )
     def test_numeric_instantiation(self, insight_fixture, request):
         """""""